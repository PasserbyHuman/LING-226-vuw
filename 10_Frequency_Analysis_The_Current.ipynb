{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPkONPzxCwv2VGn+s164DH7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/10_Frequency_Analysis_The_Current.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Frequency of words in comments for The Current**\n",
        "\n",
        "What are the most frequent words in the comments people leave as comment in The Current data? Are certain words more frequent in some questions than others, or are words used in a relatively equal manner among the questions? What about the messyness of the data - how much cleaning and preprocessing must be done in order to make sense of the data?\n",
        "\n",
        "These are all valid questions to ask as a way to start thinking about research questions and how they might be answered using computational linguistic approaches. In this notebook, we will create and compare frequency distributions of words in the different comment data for The Current.\n",
        "\n",
        "First, load in the nltk resources."
      ],
      "metadata": {
        "id": "DdUCwQ7v2ivL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the main nltk module\n",
        "import nltk\n",
        "\n",
        "# download the nltk.book resources\n",
        "nltk.download('book')\n",
        "\n",
        "# import the resources\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "bzTBZDjc3NX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to load in some of the data from The Current. I will load in data for two questions: whether petrol cars should be banned, and whether freedom camping should be illegal.\n",
        "\n"
      ],
      "metadata": {
        "id": "khB0Txh93vuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# petrol car data\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp001.txt'\n",
        "\n",
        "# freedom camping data\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp017.txt'"
      ],
      "metadata": {
        "id": "sck-R7Mh3tz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the list of each text to a variable, stripping trailing newlines and then splitting on newlines."
      ],
      "metadata": {
        "id": "BACTI2i-4gWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petrol = open('tp001.txt').read().rstrip().split('\\n')\n",
        "camping = open('tp017.txt').read().rstrip().split('\\n')"
      ],
      "metadata": {
        "id": "vh2X0Afb4fjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently the data is a set of separate comments, but we might want to represent all the comments in one container, so each text is a single string of all the comments. We can do this by using `''.join()` to glue together the results of splitting out the comments from the ratings in each text. And, we will actually put a space in the call to `' '.join()`, so that a space is placed between each comment."
      ],
      "metadata": {
        "id": "bgxVcRvL46p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# glue the comments together, note that the call to .join() has a space between the delimiters\n",
        "petrol_text = ' '.join([comment.split('\\t')[1] for comment in petrol])"
      ],
      "metadata": {
        "id": "0_aJiH3g8UD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look - a single string\n",
        "petrol_text"
      ],
      "metadata": {
        "id": "4I7y5VXF8mnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now to create a frequency distribution of the words, we need to tokenize the text into tokens. Let's use `nltk.word_tokenize()` and `FreqDist()` to do this."
      ],
      "metadata": {
        "id": "bjoH4_d-9C6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first create the tokens\n",
        "petrol_tokens = nltk.word_tokenize(petrol_text)"
      ],
      "metadata": {
        "id": "QTJyhwiW9CQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now create the Frequency Distribution\n",
        "petrol_fdist = nltk.FreqDist(petrol_tokens)"
      ],
      "metadata": {
        "id": "wWGFesyS9PmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now that there is a frequency distribution of the petrol text, we can look at what the most frequent words in those comments are! Let's look at the top 20 most frequent words using `.most_common()`.\n",
        "\n",
        "What do you think of the results?"
      ],
      "metadata": {
        "id": "pAeLQpMq9U0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "petrol_fdist.most_common(20)"
      ],
      "metadata": {
        "id": "87vfMxWy9Z5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Can you repeat this to make a frequency distribution for the camping text? You just need to repeat the above code but with the camping text instead of the petrol text! This is the output you should see if you ask for the 20 most frequent words:\n",
        "\n",
        "```\n",
        "[('.', 1591),\n",
        " ('to', 1562),\n",
        " ('the', 1419),\n",
        " ('and', 1120),\n",
        " ('it', 899),\n",
        " ('be', 863),\n",
        " ('is', 792),\n",
        " ('people', 770),\n",
        " ('camping', 674),\n",
        " ('a', 663),\n",
        " ('i', 631),\n",
        " ('should', 628),\n",
        " ('of', 573),\n",
        " ('freedom', 563),\n",
        " ('for', 482),\n",
        " ('we', 478),\n",
        " ('that', 428),\n",
        " ('nature', 401),\n",
        " ('in', 392),\n",
        " ('not', 388)]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "jvDLWeHV-IhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code cells for making fdist for camping"
      ],
      "metadata": {
        "id": "A65cFELL-qQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Discuss**\n",
        "\n",
        "- What words are repeated between the two texts?\n",
        "- What words are unique to the texts?\n",
        "- Does this analysis tell us anything about the ability for computational measures to identify features of different texts?"
      ],
      "metadata": {
        "id": "TJx-7mWu-r2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What about them stopwords?**\n",
        "\n",
        "The words that are repeated among the texts are so-called function words, determiners such as `the` or `an`, as well as prepositions such as `in`, `on`, etc. These words *are* important in English for making meaning, but maybe we don't want them in this analysis?\n",
        "\n",
        "Your challenge is to create a frequency distribution for each text which only considers words of certain lengths, 4 characters or more. What will this do to the results?"
      ],
      "metadata": {
        "id": "MLFXnoS1-9s_"
      }
    }
  ]
}