{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1XELW8Zoxr4cfsM7vx4EAYbe56V56RnKh",
      "authorship_tag": "ABX9TyNobzt67xHNJSBbYWPgNuaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/34_Word_Vector_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Vectors\n",
        "\n",
        "You might not have any idea what someone means by the term word vectors or word embeddings, but such technology can no longer be escaped. What we will cover in this notebook is a method to represent word relationships in a new way, one which has eventually formed the backbone of generative AI technology such as ChatGPT.\n",
        "\n",
        "Now, we are not going to be training ChatGPT models, but we are going to look at how to model the distributional patterns of words using algorithms which generate word vectors. One notable algorithm is `word2vec`. What is so great about it? Well, rather than calculating raw probabilities of word occurences, word2vec considers how words influence the probabilities of other words, and then groups similar words together based on those probabilities. It does so by assigning words not just one score, but tens or hundreds of scores which represent how the words function in a bunch of different contexts. These scores are the vectors, which can be mapped to a multidimension space. Then, the distance among these scores can be compared in the space to find similar / different words.\n",
        "\n",
        "There are ton of attempts to explain word2vec. One article called [The Illustrated word2vec](https://jalammar.github.io/illustrated-word2vec/). might be of interest to you, as well as the [author's YouTube video explaining the article](https://www.youtube.com/watch?v=ISPId9Lhc1g).\n",
        "\n",
        "Maybe you don't care too much about this, or maybe my explanation sucks. The point is, that we can learn how to use this algorithm to train our very own vector models on our texts. Then we can find which words are most similar to other words in texts. To do so, we will use a library called [`gensim`](https://radimrehurek.com/gensim/).\n",
        "\n",
        "Let's use the `state_union` corpus in `nltk` to learn how to train and use a model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Knj3xrpxIALg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to tokenize plus download state_union corpus\n",
        "import nltk\n",
        "nltk.download(['punkt', 'state_union'])\n",
        "from nltk.corpus import state_union"
      ],
      "metadata": {
        "id": "uTf7DgAJzQ1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to convert our data into a list of tokenized \"sentences\", although the term \"sentence\" is a big vague here. A sentence could be a paragraph or an entire document. Below I will convert each State of the Union text into a single tokenized list, resulting in a list of lists (which is what the gensim Word2Vec will expect as input).\n",
        "\n",
        "I will also lower case the words and remove any numbers and punctuation."
      ],
      "metadata": {
        "id": "abTX2FKMAN2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output containers, plus corpus size for info\n",
        "state_union_combined = []\n",
        "corpus_size = 0\n",
        "\n",
        "# loop the the file ids then tokenize and add to list\n",
        "for fileid in state_union.fileids():\n",
        "  text = state_union.raw(fileid)\n",
        "  words = nltk.word_tokenize(text)\n",
        "  words = [word.lower() for word in words if word.isalpha()]\n",
        "  state_union_combined.append(words)\n",
        "  corpus_size += len(words)\n",
        "\n",
        "# how many words in the corpus?\n",
        "print(corpus_size)"
      ],
      "metadata": {
        "id": "FvfcmxSUz2yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **state union word2vec model**\n",
        "\n",
        "To fit a `word2vec` model, we choose the text, plus the `vector_size`, `epohcs`, `window`, and `min_count`.\n",
        "\n",
        "The default vector size is 100, which means each word in the text will have 100 values or vectors (these are the values used to represent the word in the multidimensional space). I will lower this to 50 because of our relatively small corpus.  `window` represents the size of the text spans the algorithm will use (i.e., it looks at each sequential 5-word contents). `min_count` means a word must occur at least that many times, 5 is the default but I will increase that to ten.\n",
        "\n",
        "`epochs` represents the number of retrainings to do, which can help when you have small data. I ask for 10 epochs here as a means to obtain a more stable model.\n",
        "\n",
        "These parameters are not set in stone, and usually require some tweaking or adjustment based on the size of the corpus. So, feel free to play around with them\n",
        "\n",
        "\n",
        "Our state union corpus is pretty small (only about 344k words), but we will still be able to find some interesting association among words."
      ],
      "metadata": {
        "id": "cKt_CxZwAuKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in gensim and the word2vec class\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "rIdMadAc14bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model using our texts and specific parameters\n",
        "su_w2v = Word2Vec(sentences = state_union_combined, vector_size = 50, epochs = 10, window = 5, min_count = 10)"
      ],
      "metadata": {
        "id": "IWfxwQM71kiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trains relatively fast in even in Colab. Now that we have a trained model, what can we do with it? With these vectors, we can:\n",
        "\n",
        "\n",
        "1. query the model to find the most similar words to an input word\n",
        "2. find the difference between different words\n",
        "3. find words that don't match in a list of other words\n"
      ],
      "metadata": {
        "id": "nnP9eREtBERs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`.most_similar()`**\n",
        "\n",
        "The most useful information we can obtain from a model like this is word similarity metrics. We can ask the model to output a list of the most similar words to an input word. By default, this will produce the top twn most similar words, alone with the cosine distance value. The cosine distance value will range from -1 to 1. Negative values mean the words mean something very different, positive values mean the words mean something similar, and neutral values indicate no relationship.\n",
        "\n",
        "> By the way, this is exactly what `spaCy` does when using the `.similar()` function to find the similarity values between two words.\n",
        "\n",
        "To use `.most_similar()`, we need to access the `.wv` attribute from the model, which stands for word vectors. Below I look up the similarity metrics for several words. What do you think about the output?\n",
        "\n"
      ],
      "metadata": {
        "id": "bHl3X1MKBmMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar('america')"
      ],
      "metadata": {
        "id": "CCQd00xm2HE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar('conflict')"
      ],
      "metadata": {
        "id": "gwD9-ar82d__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar('peace')"
      ],
      "metadata": {
        "id": "12x17FNt2ukr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar('solution')"
      ],
      "metadata": {
        "id": "ecU72BwH2wsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`.most_similar(positive, negative)`**\n",
        "\n",
        "You can be more specific in your approach to using `.most_similar()`, by supplying not just one word, but several words. You can also specify whether the similarity should be positively or negatively related to some words.\n",
        "\n",
        "Compare the difference between asking for only the most similar words to `\"money\"` against asking for words similar to `\"money\"` but not similar to `\"war\"`.\n",
        "\n"
      ],
      "metadata": {
        "id": "qlZno8nEDEVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar(positive = 'money')"
      ],
      "metadata": {
        "id": "CLW6JOeX-pfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar(positive = 'money', negative = 'war')"
      ],
      "metadata": {
        "id": "KdR8JeMF2ydG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can enter as many words as you like as a list."
      ],
      "metadata": {
        "id": "FZ3w_q8Q-5pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar(positive = ['citizen', 'taxpayer'])"
      ],
      "metadata": {
        "id": "SSxzwOxj_Ara"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slight differences here.\n",
        "su_w2v.wv.most_similar(positive = ['citizen', 'taxpayer'], negative = ['terrorist'])"
      ],
      "metadata": {
        "id": "an5lpzF929_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar(positive = ['jobs'])"
      ],
      "metadata": {
        "id": "fvJHbyX7_GTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar(positive = ['jobs'], negative = ['military'])"
      ],
      "metadata": {
        "id": "vm_H_HTn8iDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **.doesnt_match()**\n",
        "\n",
        "This function takes a list of words and tells you which word is the least similar to the other words. I am not sure how useful this really is, but it is at least fun to play with."
      ],
      "metadata": {
        "id": "2b-q-q_98v09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.doesnt_match(['war', 'jobs', 'military'])"
      ],
      "metadata": {
        "id": "BVhP1i5A8632"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# why do you think \"america\" is the outlier here?\n",
        "su_w2v.wv.doesnt_match(['america', 'france', 'germany', 'china'])"
      ],
      "metadata": {
        "id": "3nTMFjqR9ZDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.doesnt_match(['rights', 'constitution', 'freedom', 'taxes'])"
      ],
      "metadata": {
        "id": "b-Z-0Mqu9g5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **fastext**\n",
        "\n",
        "Word2Vec is not the only way to train word vectors / embeddings. Another method is called FastText. [This algorithm works a bit differently by breaking down words and considering morphological differences.](https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py) It is also apparently better at dealing with mispelled words.\n",
        "\n",
        "Let's try it out. First we import the required class. Then we create the model with a vector size of 50.\n",
        "\n",
        "Then, we build the model vocabulary using the same `state_union_combined` object we fed to word2vec above.\n",
        "\n"
      ],
      "metadata": {
        "id": "zD_fa2ZYxi81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "su_ft = FastText(vector_size = 50)\n",
        "\n",
        "# build the vocabulary\n",
        "su_ft.build_vocab(corpus_iterable = state_union_combined)"
      ],
      "metadata": {
        "id": "-xmGyu_BcKgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to train the model. There are a number of parameters, which we will leave as defaults."
      ],
      "metadata": {
        "id": "zh-lcwGpeSYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "su_ft.train(corpus_iterable = state_union_combined, epochs = su_ft.epochs,\n",
        "                  total_examples = su_ft.corpus_count, total_words = su_ft.corpus_total_words)"
      ],
      "metadata": {
        "id": "Vbecw5HkdEB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the model just the same as the word2vec model to explore similarities among words. Compare the output here versus the output for word2vec. The most similar words here are more similar in terms of morphological form. This is because of differences in the algorithms."
      ],
      "metadata": {
        "id": "ylIbDDvNeogA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fastText vectors\n",
        "su_ft.wv.most_similar(\"america\")"
      ],
      "metadata": {
        "id": "YCeN-1mYdlG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec vectors\n",
        "su_w2v.wv.most_similar(\"america\")"
      ],
      "metadata": {
        "id": "ZacJwjs6AYTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **using pretrained vectors**\n",
        "\n",
        "You can also use sets of pretrained vectors which have been run on much larger data sets. The language models in `spaCy` for example have larger vectors than the data we have here. Keep in mind those vectors are intending to show similarity at a generalizable level. One advantage of training on our own data is that we can explore similarity in a specific context.\n",
        "\n",
        "One interesting experiment then is to see whether your corpus or texts have different similarity relationships when compared to a larger set of reference vectors.\n",
        "\n",
        "`gensim` has [a number of pretrained models you can use.](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models).\n",
        "\n"
      ],
      "metadata": {
        "id": "mgEe82OVAuor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all gensim models:\n",
        "import gensim.downloader\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "id": "Qgr2-S8dBhhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, I ask for gensim to download a pretrained set of vectors trained on a corpus of 3 million words from google news. The dimensions of this set are 300, meaning there are 300 vectors for each word. The model is 1.6 GB large, so it takes awhile to download. If you want to reuse resources such as this, it would be best to download it to your Google Drive and then reload it as you need. But if you want to click Play below, go grab a drink while you want for it to download.\n",
        "\n",
        "You would need to repeat this download each time you run the notebook."
      ],
      "metadata": {
        "id": "62TLFNgvB2LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download\n",
        "\n",
        "google_w2v = glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "qgJFFM9UBrmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a loaded model, we can use it just the same. Because we have downloaded the vectors directly and not the model, we do not need to use the `.wv.` part.\n",
        "\n",
        "You can see the differences using `type`"
      ],
      "metadata": {
        "id": "ib_xnmZVEunO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a model\n",
        "type(su_w2v)"
      ],
      "metadata": {
        "id": "g24g4HcEE6_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keyed vectors\n",
        "type(su_w2v.wv)"
      ],
      "metadata": {
        "id": "7QXVW_hQFAyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keyed vectors\n",
        "type(google_w2v)"
      ],
      "metadata": {
        "id": "PUzlmxwVFDYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the most similar words to 'america' in the Google New corpus vectors. I will show the state union vectors again for reference. We can see that we obtain vastly different results. This is a direct result of the different training data use. The Google News dataset is larger and has more topics. The State Union corpus is smaller and more homogenous (i.e., it is all speeches from US Presidents).\n"
      ],
      "metadata": {
        "id": "DeMnibCWFLxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_w2v.most_similar('america')"
      ],
      "metadata": {
        "id": "Q-0ZEAZDEwfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "su_w2v.wv.most_similar('america')"
      ],
      "metadata": {
        "id": "PzBGwVWQFUKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "Training word vectors is a bit more complicated than the other NLP tasks covered so far. But, libraries like `gensim` make it relatively painless to do so. It is very important to explore the output of the model and potentially tune the different parameters until you get something that makes sense.\n",
        "\n",
        "Take this time to either train your own model or explore the ones I've loaded / created here, or to look into the other models provided by `gensim`.\n",
        "\n",
        "There are also other sources of pretrained vectors besides `gensim`, and you can use `gensim` to load in these vectors and use them as you like.\n",
        "\n",
        "You might still be a bit fuzzy about what vectors actually *are*, and that's pretty normal. [There are several blogs and other explanations](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) out there which may help. Vectors are now fundamental tools for modern NLP and AI, so gaining an awareness of how they work and how to train them is important."
      ],
      "metadata": {
        "id": "Vt-lGi0hFP9X"
      }
    }
  ]
}