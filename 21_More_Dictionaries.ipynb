{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/21_More_Dictionaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nujejxMHIXY"
      },
      "source": [
        "# Doing more with Dictionaries\n",
        "\n",
        "If you've inspected the `nltk.FreqDist()` and `nltk.ConditionalFreqDist()` functions, you'll see they store data as dictionaries. We have already covered dictionaries in general. By now you should see that dictionaries are quite useful for storing linguistic information, which NLTK refers to as mapping from one thing to another (e.g., mapping a POS tag to a word). In this notebook we will return to dictionaries as a refresher, and also learn about extensions to dictionaries used by NLTK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6IBVGAg-fWQT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download necessary resources for this notebook\n",
        "import nltk\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets', 'treebank', 'universal_tagset', 'book'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46w0qoxWq94B"
      },
      "source": [
        "As a reminder, we can manually create a dictionary using a pair of curly braces `{}`. Below, I define a dictionary where words are keys and POS tags are the values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-uqRa48LUMI"
      },
      "outputs": [],
      "source": [
        "# manually create a dictionary\n",
        "pos = {}\n",
        "pos['colorless'] = 'ADJ'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU-7GaS-LaGM"
      },
      "outputs": [],
      "source": [
        "# add entries which are word:pos_tag key/value pairs\n",
        "pos['ideas'] = 'N'\n",
        "pos['sleep'] = 'V'\n",
        "pos['furiously'] = 'ADV'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LAihy4jLaIq"
      },
      "outputs": [],
      "source": [
        "# inspect our dictionary\n",
        "pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r22Tgz6UL_rm"
      },
      "outputs": [],
      "source": [
        "# Using list() will give all the keys, handy as a shortcut\n",
        "list(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvzluQt6Mo67"
      },
      "source": [
        "## DefaultDict\n",
        "\n",
        "A `defaultdict` is an extension of Python dictionaries from the `collections` module (the same module which gave us `namedtuple`). Learning how to use `defaultdict` is helpful if you are performing operations where you want automatic values set to keys which do not yet exist in a dictionary.\n",
        "\n",
        "First of all, consider what happens when we ask a dictionary for a key that does not exist. I'll create a dictionary with fictional frequency values for two words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OKiAV2Ny0oOe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 1337, 'dog': 42}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make a small dictionary with word:pos_tag key/values\n",
        "fake_frequencies = {'the': 1337, 'dog': 42} \n",
        "fake_frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "516E9IQ406o9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1337"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# everything is fine when I ask for a key already in the dictionary.\n",
        "fake_frequencies['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1N2Mx_8J07fF"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'ran'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32md:\\OneDrive\\OneDrive - Victoria University of Wellington - STUDENT\\200 level docs\\LING226\\Jupyter Notebook\\21_More_Dictionaries.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/OneDrive%20-%20Victoria%20University%20of%20Wellington%20-%20STUDENT/200%20level%20docs/LING226/Jupyter%20Notebook/21_More_Dictionaries.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# but if I ask for a key not in the dictionary, we get a key error\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive/OneDrive%20-%20Victoria%20University%20of%20Wellington%20-%20STUDENT/200%20level%20docs/LING226/Jupyter%20Notebook/21_More_Dictionaries.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fake_frequencies[\u001b[39m'\u001b[39;49m\u001b[39mran\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
            "\u001b[1;31mKeyError\u001b[0m: 'ran'"
          ]
        }
      ],
      "source": [
        "# but if I ask for a key not in the dictionary, we get a key error\n",
        "fake_frequencies['ran']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CflZ2VDR1AyG"
      },
      "source": [
        "Using a `defaultdict` allows you to be a bit more defensive and avoid errors when looking up values that might not exist. And, by extension, allows us to think about how this might be useful for things like frequency dictionaries — if a word does not exist in our frequency dictionary, we could assume that it has a frequency of zero. This is exactly what the NLTK `FreqDist` dictionary provides you with.\n",
        "\n",
        "Same thing with POS tags or other lexical information — rather than finding out that something is not in the dictionary, it would be preferable to update the dictionary with a value indicating the lack of such information.\n",
        "\n",
        "When you create a default dictionary, you can specify the default value (or function) which will trigger if a value does not exist in the dictionary. In the cell below, I import `defaultdict` and then specify the default value for a key not in the dictionary will be an `int`. Because I do not supply any other information about the `int`, the default value will be `0`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YBivI72hMrCc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import defaultdict\n",
        "from collections import defaultdict\n",
        "\n",
        "# create a defaultdictionary where default values are `int` of 0.\n",
        "frequency = defaultdict(int)\n",
        "\n",
        "# add a key/value to the dictionary\n",
        "frequency['colorless'] = 4\n",
        "\n",
        "# inspect key/value\n",
        "frequency['colorless']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Chskn44KQrT-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now look up something not in the dictionary\n",
        "# the key was not there, so was added with the DEFAULT value of int = 0\n",
        "frequency['ideas']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7HajDfcgAdha"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to understand why `int` gives us zero:\n",
        "int()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AphIEEmL_sgD"
      },
      "source": [
        "If you wanted to set the default value to be a specific number, you can use (`lambda: value`) as the default argument:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QlV7VwSe_uSG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1337"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# I use an anonymous function (lambda) to set default value to a number that is not zero.\n",
        "any_integer = defaultdict(lambda: 1337)\n",
        "\n",
        "any_integer['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ27hHC33HLj"
      },
      "source": [
        "We can extend this to other information such as parts of speech. Below, I indicate the `defaultdict` should have an empty `list` as the default value, and the same process occurs when we look up words not in the dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fp1sRUpBQjbw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NOUN', 'VERB']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# default is an empty list for tag sets\n",
        "pos = defaultdict(list)\n",
        "# add a key value\n",
        "pos['sleep'] = ['NOUN', 'VERB']\n",
        "# works as intended\n",
        "pos['sleep']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_LFVME6L0W-_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a word not in the dictionary is updated to default value (an empty list)\n",
        "pos['green']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzweb02BRIMM"
      },
      "source": [
        "For something like parts of speech, we might actually want to have a default POS tag instead of an empty list. Again we can use the `lambda` function to supply a default POS tag.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vXvLaS-PRN9v"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ADJ'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tell defaultdict that the defaul POS tag is \"NOUN\"\n",
        "pos = defaultdict(lambda: 'NOUN')\n",
        "\n",
        "pos['colorless'] = 'ADJ'\n",
        "pos['colorless']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c1_S1V1qRUjK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NOUN'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# if an entry doesn't exist it is added to the dictionary\n",
        "# there are other ways to do this using the basic dictionary type as well\n",
        "pos['blog']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fSyb1-1lRR_4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('colorless', 'ADJ'), ('blog', 'NOUN')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remember that list() can be used to inspect\n",
        "list(pos.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpbSh8W8WKTJ"
      },
      "source": [
        "Recall that `nltk.pos_tag()` will tag things as a noun by default, and that `FreqDist()` will give frequencies of zero for unknown words. Now you can understand a little bit more about how they go about doing this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVmApnNLc2NF"
      },
      "source": [
        "## **Tagging unknown words**\n",
        "\n",
        "Now, instead of supplying a default tag such as \"NOUN\" (which could be dangerous), NLTK shows how labels such as \"unknown\" can be used to tag words which are \"out of vocabulary.\"\n",
        "\n",
        "Below, we read in *Alice in Wonderland* from the Gutenberg data built into NLTK. We access the `.words` of the book to get the tokenized list, and then create a frequency distribution of those words using `nltk.FreqDist()`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L3CuMs1Mc4Mk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Mock', 56),\n",
              " ('by', 55),\n",
              " ('my', 55),\n",
              " ('Hatter', 55),\n",
              " ('Gryphon', 55),\n",
              " ('quite', 53),\n",
              " ('your', 53),\n",
              " ('an', 52),\n",
              " ('much', 51),\n",
              " ('say', 51)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a frequency distribution of all the words from alice in wonderland\n",
        "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
        "alice_vocab = nltk.FreqDist(alice)\n",
        "\n",
        "# take a peek at some of the most common words\n",
        "alice_vocab.most_common(100)[90:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI5UdqPjCqdS"
      },
      "source": [
        "In the next code cell, we use a `list comprehension` to extract the most frequent 1000 words, and we also toss away the frequency values (because we just want the words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E2T_5kqrfN3t"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Mock', 'by', 'my', 'Hatter', 'Gryphon', 'quite', 'your', 'an', 'much', 'say']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# gather the most frequent 1000 tokens\n",
        "alice_top_1000 = [word for (word, frequency) in alice_vocab.most_common(1000)]\n",
        "alice_top_1000[90:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOKo9ymqDfww"
      },
      "source": [
        "Now we will create a default dictionary in which the default tag will be 'UNK' for unknown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rAYvA3z9dcTj"
      },
      "outputs": [],
      "source": [
        "# create default dict with a default string of \"UNK\" if not in the dict.\n",
        "from collections import defaultdict\n",
        "alice_known = defaultdict(lambda: 'UNK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxOR3gqVD2KW"
      },
      "source": [
        "We will then add our top 1000 words to the dictionary, and in doing so, will make both the key and the value the word. We do so using a for loop which loops over each word and sets that word as they key and the value.\n",
        "\n",
        "Why are we doing this? We are simulating a list of \"known\" words. Any word which has itself as the value is considered \"known.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "je8tLdTKdrBJ"
      },
      "outputs": [],
      "source": [
        "# add each word from top 1000 as itself to the dictionary\n",
        "for v in alice_top_1000:\n",
        "  alice_known[v] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KyDzEAHxgVNo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Mock', 'Mock'),\n",
              " ('by', 'by'),\n",
              " ('my', 'my'),\n",
              " ('Hatter', 'Hatter'),\n",
              " ('Gryphon', 'Gryphon'),\n",
              " ('quite', 'quite'),\n",
              " ('your', 'your'),\n",
              " ('an', 'an'),\n",
              " ('much', 'much'),\n",
              " ('say', 'say')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure you understand what is going on here.\n",
        "list(alice_known.items())[90:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LziKbUEEMt1"
      },
      "source": [
        "Now, we can perform a `list comprehension` which loops over all of the words in the book (contained in the variable `alice`). This will simulate reading the book one word at a time.\n",
        "\n",
        "The list comprehension is asking for the value of each word from our default dictionary `alice_known`. Remember, we've already added the top 1000 words to this dictionary, so if they are checked, they will return themselves.\n",
        "\n",
        "For the words in `alice` which are *not* in `alice_known`, the default dictionary will return our unknown tag \"UNK\".\n",
        "\n",
        "The end result is a list of tokens the same length as `alice`, but any word not in the top 1000 more frequent words will be replaced with \"UNK\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VSvtZiYqdIeW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Alice',\n",
              " \"'\",\n",
              " 's',\n",
              " 'Adventures',\n",
              " 'in',\n",
              " 'Wonderland',\n",
              " 'by',\n",
              " 'UNK',\n",
              " 'UNK',\n",
              " 'UNK',\n",
              " 'UNK',\n",
              " 'CHAPTER',\n",
              " 'I',\n",
              " '.',\n",
              " 'Down',\n",
              " 'the',\n",
              " 'Rabbit',\n",
              " '-',\n",
              " 'UNK',\n",
              " 'Alice',\n",
              " 'was',\n",
              " 'beginning',\n",
              " 'to',\n",
              " 'get',\n",
              " 'very',\n",
              " 'tired',\n",
              " 'of',\n",
              " 'sitting',\n",
              " 'by',\n",
              " 'her',\n",
              " 'sister',\n",
              " 'on',\n",
              " 'the',\n",
              " 'bank',\n",
              " ',',\n",
              " 'and',\n",
              " 'of',\n",
              " 'having',\n",
              " 'nothing',\n",
              " 'to',\n",
              " 'do',\n",
              " ':',\n",
              " 'once',\n",
              " 'or',\n",
              " 'twice',\n",
              " 'she',\n",
              " 'had',\n",
              " 'peeped',\n",
              " 'into',\n",
              " 'the',\n",
              " 'book',\n",
              " 'her',\n",
              " 'sister',\n",
              " 'was',\n",
              " 'reading',\n",
              " ',',\n",
              " 'but',\n",
              " 'it',\n",
              " 'had',\n",
              " 'no',\n",
              " 'pictures',\n",
              " 'or',\n",
              " 'UNK',\n",
              " 'in',\n",
              " 'it',\n",
              " ',',\n",
              " \"'\",\n",
              " 'and',\n",
              " 'what',\n",
              " 'is',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'a',\n",
              " 'book',\n",
              " \",'\",\n",
              " 'thought',\n",
              " 'Alice',\n",
              " \"'\",\n",
              " 'without',\n",
              " 'pictures',\n",
              " 'or',\n",
              " 'conversation',\n",
              " \"?'\",\n",
              " 'So',\n",
              " 'she',\n",
              " 'was',\n",
              " 'considering',\n",
              " 'in',\n",
              " 'her',\n",
              " 'own',\n",
              " 'mind',\n",
              " '(',\n",
              " 'as',\n",
              " 'well',\n",
              " 'as',\n",
              " 'she',\n",
              " 'could',\n",
              " ',']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a new vocab which then looks at all the words in alice (not just the top 1000)\n",
        "alice_complete = [alice_known[v] for v in alice]\n",
        "\n",
        "# anything not already in the top 1000 words gets assigned to UNK\n",
        "alice_complete[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkDwIKM6IrZO"
      },
      "source": [
        "We can compare the two versions - because the words `Lewis` and `Carroll` were not in the top 1000 most frequent words, they are replaced with `UNK`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ooxO7NajGzEc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original version:\n",
            "Alice ' s Adventures in Wonderland by Lewis Carroll \n",
            "\n",
            "Processed version:\n",
            "Alice ' s Adventures in Wonderland by UNK UNK\n"
          ]
        }
      ],
      "source": [
        "# extract just the first 10 real words from each version\n",
        "first_10_raw = ' '.join([v for v in alice[1:10]])\n",
        "first_10_processed = ' '.join([v for v in alice_complete[1:10]])\n",
        "\n",
        "# print them for comparison\n",
        "print(f'Original version:\\n{first_10_raw} \\n\\nProcessed version:\\n{first_10_processed}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5grnPmJDIZcn"
      },
      "source": [
        "\n",
        "\n",
        "We can confirm the two lists of tokens are the same lengths(i.e., have the same number of words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UxBYwgobIUhx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34110"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(alice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "r80kP4FFIVaj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34110"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(alice_complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tHV4UDTIomZ"
      },
      "source": [
        "So, in the end, this was a really convoluted method which replaced all of the words in the book that are *not* in the top 1000 most frequent words with the tag \"UNK\".\n",
        "\n",
        "\n",
        "You are probably wondering, why might we ever want to do something like this? It of course depends on the goals of your analysis. Extracting the most-frequent 1000 words from a text may in turn include a good portion of the text. In fact, if we ask that question about our text here, we can see that the 1000 most-frequent words account for over 90% of all the words in the book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wVDID9_9Ku_H"
      },
      "outputs": [],
      "source": [
        "# all words that have a value in our known values default dict\n",
        "in_vocab = [w for w in alice if w in alice_known.values()]\n",
        "\n",
        "# all those that do not have values in the default dict\n",
        "out_vocab = [w for w in alice if w not in alice_known.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MCLut85vL37w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "91.5713866901202"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate proportion based on total words for top 1000\n",
        "len(in_vocab) / len(alice) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "e-VtS-9MMOS2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8.4286133098798"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# same for those without\n",
        "len(out_vocab) / len(alice) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0AsVgPRMiGJ"
      },
      "source": [
        "What we have done by looking at the top 1000 most frequent words is reduced the search space for any task or procedure we might want to perform on this text. The NLTK book points out that a procedure such as this can help with Part of Speech tagging, because it means a tagger would not need to consider any word tagged with \"UNK\" and thus increase accuracy and performance.\n",
        "\n",
        "We can also use such information and compare the top 1000-most frequent words in this text to other texts and/or word lists as a way to assess topics or similarity between two texts, or assess the overall difficulty of the vocabulary by comparing this list to lists of word frequency built on even larger corpora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5VaS0x7eO3i"
      },
      "source": [
        "## Incrementally updating a dictionary\n",
        "\n",
        "The NLTK book shows us how to use a very handy operator, the `+=` which iterates a value by a set value. Most commonly we see this used as `+= 1`, which means increase something by 1. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qkv3sez1f4MN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set the value 1 to the variable a\n",
        "a = 1\n",
        "\n",
        "# increment the value by 1\n",
        "a += 1\n",
        "\n",
        "# what is 1 + 1?\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQk1Q88wOScv"
      },
      "source": [
        "We can choose any number we like as the units of increment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZbusAXnqf5qR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# same as above but increment by 1000 instead of 1\n",
        "a = 1\n",
        "a += 1000\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxkq7JtaOyNp"
      },
      "source": [
        "This method for incrementing can be used to update information each time something is encountered, such as increasing the frequency counts of words, pos tags, or anything else we are interested in counting during a loop procedure.\n",
        "\n",
        "For example, the NLTK book shows us that we might be interested in counting the total incidence of different Part of Speech tags in the Brown corpus, and shows us how to do it.\n",
        "\n",
        "We create a `defaultdict` named `counts` and set the default to `int` (which means 0).\n",
        "\n",
        "We then import `brown`, and loop through the tagged version of the corpus. The loop checks the tag in `counts` and increments the value by 1. Because the default value of `counts` will be 0, the very first time a tag is found, that value will increment to 1, and so on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KQg2xgxUftcE"
      },
      "outputs": [],
      "source": [
        "# create the default dictionary with default of 0\n",
        "counts = defaultdict(int)\n",
        "\n",
        "# read in brown corpus\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# NLTK authors use the += operator to increment the tag count by 1 each time it is seen.\n",
        "for (word, tag) in brown.tagged_words(categories = 'news', tagset = 'universal'):\n",
        "  counts[tag] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "04TOZTc78X3o"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([('DET', 11389), ('NOUN', 30654), ('ADJ', 6706), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ('ADV', 3349), ('CONJ', 2717), ('PRT', 2264), ('PRON', 2535), ('NUM', 2166), ('X', 92)])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the dictionary is frequency count of POS tags\n",
        "counts.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FXwhsUXLPohY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11389"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# do you have any ideas why this might be the most frequent tag?\n",
        "counts['DET']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-tNpr_-Ty_"
      },
      "source": [
        "## **Anagram dictionary**\n",
        "\n",
        "Although it is very brief the NLTK book talks about finding anagrams using a `defaultdict`. I've tried to add some more detail here to make it clear how this function works and what it is doing to the words.\n",
        "\n",
        "Basically, more than one word might include all of the same letters, albeit in a different order. By alphabetically sorting the letters of all words, any word with the same letters will be associated with the same alphabetic pattern, which can then be used as a common key for different words.\n",
        "\n",
        "For example, the words `heart` and `earth` all use the same letters but in a different sequence. If we used `sorted()` on both strings, they give us the same result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FS3Xyw9gQsDl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aehrt\n",
            "aehrt\n"
          ]
        }
      ],
      "source": [
        "# both words are the same when you sort them!\n",
        "print(''.join(sorted('earth')))\n",
        "print(''.join(sorted('heart')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uXDdKriM-c8p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['neophyte', 'neophytic', 'neophytish', 'neophytism']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import NLTK's giant list of English words\n",
        "words = nltk.corpus.words.words('en')\n",
        "words[123456:123460]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dlHLXjVZ-nC7"
      },
      "outputs": [],
      "source": [
        "# create anagrams as a default dict with list as the default\n",
        "anagrams = defaultdict(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HCh1RK7k-1hu"
      },
      "outputs": [],
      "source": [
        "# loop over each word (e.g., 'earth')\n",
        "for word in words:\n",
        "  # the KEY will be the sorted version (e.g., 'aehrt')\n",
        "  key = ''.join(sorted(word))\n",
        "  # append the VALUE to that key (e.g., 'earth')\n",
        "  anagrams[key].append(word) # each new word which has the same key will be added to this value (using list.append())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Zz89265zAJ9H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agnor : ['angor', 'argon', 'goran', 'grano', 'groan', 'nagor', 'orang', 'organ', 'rogan']\n",
            "acert : ['caret', 'carte', 'cater', 'crate', 'creat', 'creta', 'react', 'recta', 'trace']\n",
            "eerst : ['ester', 'estre', 'reest', 'reset', 'steer', 'stere', 'stree', 'terse', 'tsere']\n"
          ]
        }
      ],
      "source": [
        "# this might help you see what's going on here\n",
        "# we can find any pattern which has more than a certain number of words - here I choose more than 6\n",
        "# although, some of these words sure seem odd to me!\n",
        "for key in anagrams:\n",
        "  if len(anagrams[key]) > 8: # play around with this value to get different results.\n",
        "    print(key, ':', anagrams[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7oggLDzBL2P"
      },
      "source": [
        "# `nltk.Index`\n",
        "\n",
        "As the authors of NLTK seem fond of doing, they briefly introduce a custom way for doing the same thing using a special NLTK version. Honestly, don't worry too much about this, basically they are just providing their own version of `defaultdict`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8iycKa_KBWQh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class Index in module nltk.util:\n",
            "\n",
            "class Index(collections.defaultdict)\n",
            " |  Index(pairs)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Index\n",
            " |      collections.defaultdict\n",
            " |      builtins.dict\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, pairs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from collections.defaultdict:\n",
            " |  \n",
            " |  __copy__(...)\n",
            " |      D.copy() -> a shallow copy of D.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __missing__(...)\n",
            " |      __missing__(key) # Called by __getitem__ for missing key; pseudo-code:\n",
            " |      if self.default_factory is None: raise KeyError((key,))\n",
            " |      self[key] = value = self.default_factory()\n",
            " |      return value\n",
            " |  \n",
            " |  __or__(self, value, /)\n",
            " |      Return self|value.\n",
            " |  \n",
            " |  __reduce__(...)\n",
            " |      Return state information for pickling.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __ror__(self, value, /)\n",
            " |      Return value|self.\n",
            " |  \n",
            " |  copy(...)\n",
            " |      D.copy() -> a shallow copy of D.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from collections.defaultdict:\n",
            " |  \n",
            " |  __class_getitem__(...) from builtins.type\n",
            " |      See PEP 585\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from collections.defaultdict:\n",
            " |  \n",
            " |  default_factory\n",
            " |      Factory for default value called by __missing__().\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      True if the dictionary has the specified key, else False.\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getitem__(...)\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __ior__(self, value, /)\n",
            " |      Return self|=value.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __reversed__(self, /)\n",
            " |      Return a reverse iterator over the dict keys.\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sizeof__(...)\n",
            " |      D.__sizeof__() -> size of D in memory, in bytes\n",
            " |  \n",
            " |  clear(...)\n",
            " |      D.clear() -> None.  Remove all items from D.\n",
            " |  \n",
            " |  get(self, key, default=None, /)\n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  items(...)\n",
            " |      D.items() -> a set-like object providing a view on D's items\n",
            " |  \n",
            " |  keys(...)\n",
            " |      D.keys() -> a set-like object providing a view on D's keys\n",
            " |  \n",
            " |  pop(...)\n",
            " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
            " |      \n",
            " |      If the key is not found, return the default if given; otherwise,\n",
            " |      raise a KeyError.\n",
            " |  \n",
            " |  popitem(self, /)\n",
            " |      Remove and return a (key, value) pair as a 2-tuple.\n",
            " |      \n",
            " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
            " |      Raises KeyError if the dict is empty.\n",
            " |  \n",
            " |  setdefault(self, key, default=None, /)\n",
            " |      Insert key with a value of default if key is not in the dictionary.\n",
            " |      \n",
            " |      Return the value for key if key is in the dictionary, else default.\n",
            " |  \n",
            " |  update(...)\n",
            " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
            " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
            " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
            " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
            " |  \n",
            " |  values(...)\n",
            " |      D.values() -> an object providing a view on D's values\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from builtins.dict:\n",
            " |  \n",
            " |  fromkeys(iterable, value=None, /) from builtins.type\n",
            " |      Create a new dictionary with keys from iterable and values set to value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from builtins.dict:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from builtins.dict:\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(nltk.Index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wM7V2XLnBSKN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)\n",
        "# choose one that's funny\n",
        "anagrams['aeilnrt']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-iE-aQPY6yj"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook we considered an extension to dictionaries which allows for default values. The main benefits of using such dictionaries for our purposes is to count various properties of words, such as word frequency and parts of speech. The convenience of using `defaultdict` is that we don't have to account for missing keys when constructing such objects.\n",
        "\n",
        "There are a number of other demonstrations in this chapter which can be interesting to study but are not necessary to understand for our purposes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOeQDVVlAld4zXMi11m5dzr",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
