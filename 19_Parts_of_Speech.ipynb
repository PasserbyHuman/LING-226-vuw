{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/19_Parts_of_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7XKg53_5d6C"
      },
      "source": [
        "# **Parts of Speech**\n",
        "\n",
        "This notebook discusses a very important lexical property: parts of speech. We have thus far dealt with searching and manipulating text based on the orthographic features of words (i.e., their written forms). That means we have remained focused on the forms of the words (i.e., types and tokens). This has given us the ability to measure basic yet important aspects of texts: length, word frequency, and also some considerations about lexical diversity.\n",
        "\n",
        "You probably have some familiarity with how words are classified into different lexical and syntactic categories, such as nouns, verbs, adjectives, pronouns, etc. These categories are called *parts of speech* (POS), and can be used as another source of information which can be exploited during linguistic analysis of texts.\n",
        "\n",
        "For NLP and Computational Linguistics, it is common to see reference made to POS **Tags**, which are essentially labels or annotations associated with a word to represent more information about that word. These tags can be counted and compared, and also provide critical information for building and understanding grammars of languages.\n",
        "\n",
        "Let's explore how to tag texts and then use these tags to query information about texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqS5Ui7-3PZl"
      },
      "source": [
        "## Using `nltk.pos_tag()` to automatically tag texts.\n",
        "\n",
        "Fortunately for us, NLTK includes a function which will automatically assign part of speech tags to a text. To use this function, we need to import NLTK and download some additional resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zrbjuen42OPj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import NLTK and download the necessary resources\n",
        "import nltk\n",
        "# import resources for tokenizing and tagging\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets', 'treebank', 'universal_tagset', 'book'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz2iIYaz3K4_"
      },
      "source": [
        "\n",
        "The NLTK function expects a list of tokens and is used like this:\n",
        "\n",
        "> `nltk.pos_tag(tokens)`\n",
        "\n",
        "The results will be a list of `(word,tag)` pairs (which are in the form of a tuple.)\n",
        "\n",
        "The next three cells demonstrate an example of how to POS tag a text using nltk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vv4eYY2vtiTn"
      },
      "outputs": [],
      "source": [
        "# step 1: have some text\n",
        "rant = \"You know, we're living in a society! We're supposed to act in a civilized way.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lW8_DVm7tr6G"
      },
      "outputs": [],
      "source": [
        "# step 2: tokenize\n",
        "rant_tokens = nltk.word_tokenize(rant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jgRt_jykt3bS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('You', 'PRP'),\n",
              " ('know', 'VBP'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " (\"'re\", 'VBP'),\n",
              " ('living', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('society', 'NN'),\n",
              " ('!', '.'),\n",
              " ('We', 'PRP'),\n",
              " (\"'re\", 'VBP'),\n",
              " ('supposed', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('act', 'VB'),\n",
              " ('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('civilized', 'JJ'),\n",
              " ('way', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# step 3: tag\n",
        "rant_pos = nltk.pos_tag(rant_tokens)\n",
        "\n",
        "# look at the resulting (word, tag) pairs\n",
        "[tagged for tagged in rant_pos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MQhZJCeuSYO"
      },
      "source": [
        "POS tagging has created a list of tuples of our words, with `(word, tag)` format. The tags are informative and indeed go beyond broader word categories such as NOUN and VERB. For example,\n",
        "\n",
        "```\n",
        "VBP = verb, present tense, not 3rd person singular\n",
        "```\n",
        "\n",
        "while\n",
        "\n",
        "```\n",
        "VBG = verb, present participle or gerund.\n",
        "```\n",
        "\n",
        "These tags are from the Penn tagset which is a very commonly used set of POS tags. You can run the following cell to see the full list or by [going to this link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
        "\n",
        "Take a moment to scroll throught these tags and explore their explanations and examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eQusTQjMudIS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        }
      ],
      "source": [
        " # full list of tags, with definitions and examples\n",
        " nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9kuNUXci2R"
      },
      "source": [
        "You can also look up one specific tag by supplying the tag as a string to the `help` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BpqE3Yb6cnkA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n"
          ]
        }
      ],
      "source": [
        "# what is the NNP tag?\n",
        "nltk.help.upenn_tagset('NNP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLzeBkKRIHx-"
      },
      "source": [
        "### **Discussion**\n",
        "\n",
        "Part of speech tags help make sense of words in the context of other words. Consider this example from the NLTK book - what is the difference in use for the instances of *refuse* and *permit*?\n",
        "\n",
        "Think back to what we know about bigrams. Are there any clues  provided by the words which come *before* refuse/permit that might facilitate tagging of the proper part of speech?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5YAYJBKBdTh9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('refuse', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('permit', 'VB'),\n",
              " ('us', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('obtain', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('refuse', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzIaCK5sdc17"
      },
      "source": [
        "We can supply our own examples as well — let us compare two uses of the same word \"comb\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l-zzcRuIIG_s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Quick', 'NNP'),\n",
              " (',', ','),\n",
              " ('comb', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('desert', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('droids', 'NNS'),\n",
              " ('!', '.')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# what pos tag does the word \"comb\" have in this example?\n",
        "nltk.pos_tag(nltk.word_tokenize('Quick, comb the desert for droids!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8sZL8rR0IcOZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Where', 'WRB'),\n",
              " ('is', 'VBZ'),\n",
              " ('my', 'PRP$'),\n",
              " ('comb', 'NN'),\n",
              " ('?', '.'),\n",
              " ('hlg会更加', 'NN')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# and what pos tag does the word \"comb\" have in this example?\n",
        "nltk.pos_tag(nltk.word_tokenize('Where is my comb? hlg会更加'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOnAeZTIslo"
      },
      "source": [
        "So, adding POS tag information provides more information about a text, which becomes useful for more advanced NLP applications such as information extraction, text prediction, and so on. Because the tags are stores as strings, you can use knowledge of Python to search or filter through the list in order to find specific words associated with specific tags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHxQsM27FUA"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "- Explore using `nltk.pos_tag()` on some texts.\n",
        "- See if you can understand the different POS tags and what they mean about the words.\n",
        "- Can you \"break\" the tagger or have it produce innaccurate results?\n",
        "- The tagger has a rule that if it does not know the tag for a word, it will automatically assign a default POS tag. Can you figure out what this default tag is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwV8MINvygA"
      },
      "source": [
        "# Bigrams and Parts of Speech\n",
        "\n",
        "NLTK reviews major parts of speech, with examples. Regardless of what you think you might know (or not know) about grammar/language, you should carefully read these sections. Look at the patterns associated with different parts of speech – these patterns are crucial for training taggers. This is evidenced in the example showing how bigrams of POS tags show typically English word order. We can try the same with our own example.\n",
        "\n",
        "Let's use `nltk.bigrams()` on a set of tagged tokens — this will create a set of ((word, tag) , (word, tag)) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mIT7JrbmwWIX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('You', 'PRP'), ('know', 'VBP')),\n",
              " (('know', 'VBP'), (',', ',')),\n",
              " ((',', ','), ('we', 'PRP')),\n",
              " (('we', 'PRP'), (\"'re\", 'VBP')),\n",
              " ((\"'re\", 'VBP'), ('living', 'VBG')),\n",
              " (('living', 'VBG'), ('in', 'IN')),\n",
              " (('in', 'IN'), ('a', 'DT')),\n",
              " (('a', 'DT'), ('society', 'NN')),\n",
              " (('society', 'NN'), ('!', '.')),\n",
              " (('!', '.'), ('We', 'PRP')),\n",
              " (('We', 'PRP'), (\"'re\", 'VBP')),\n",
              " ((\"'re\", 'VBP'), ('supposed', 'VBN')),\n",
              " (('supposed', 'VBN'), ('to', 'TO')),\n",
              " (('to', 'TO'), ('act', 'VB')),\n",
              " (('act', 'VB'), ('in', 'IN')),\n",
              " (('in', 'IN'), ('a', 'DT')),\n",
              " (('a', 'DT'), ('civilized', 'JJ')),\n",
              " (('civilized', 'JJ'), ('way', 'NN')),\n",
              " (('way', 'NN'), ('.', '.'))]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create bigrams of our pos tagged example\n",
        "rant_bigrams = [bigram for bigram in nltk.bigrams(rant_pos)]\n",
        "\n",
        "# inspect the bigrams\n",
        "rant_bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjOJpTooHuvu"
      },
      "source": [
        "Now that we have the part of speech information included with our words, we can shift our search patterns away from the orthographic forms of words to instead the part of speech of words. This allows us to find more abstract patterns in language associated with word *categories* rather than with the forms of words themselves.\n",
        "\n",
        "For instance, let's look for all words in our example which come before nouns. This requires a bit of slicing, because we are looping through pairs set within a single tuple\n",
        "\n",
        "```\n",
        "((word, tag), (word, tag))\n",
        "```\n",
        "\n",
        "So to access the word in the first pair, we would first index the larger tuple using `[0]` to get the first `(word, tag)` pair, then then index that pair using `[0]` to get the first part of `(word, tag)`, which would be the word. This is demonstrated in the next code cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rvHllia4SEka"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You\n",
            "know\n",
            ",\n",
            "we\n",
            "'re\n",
            "living\n",
            "in\n",
            "a\n",
            "society\n",
            "!\n",
            "We\n",
            "'re\n",
            "supposed\n",
            "to\n",
            "act\n",
            "in\n",
            "a\n",
            "civilized\n",
            "way\n"
          ]
        }
      ],
      "source": [
        "for i in rant_bigrams:\n",
        "  print(i[0][0]) #index the first nested tuple, then index that tuples first value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrIOqdE6k3ZH"
      },
      "source": [
        "Another strategy would be to follow the NLTK book's guide and set the tuple pair as the iterator, allowing you to index the tuple in a more transparent way.\n",
        "\n",
        "```\n",
        "[a for (a,b) in bigrams]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVybDY4nlnvf"
      },
      "outputs": [],
      "source": [
        "# you can select the first pair of each pair\n",
        "[a for (a, b) in rant_bigrams]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DClyNDoil3tK"
      },
      "outputs": [],
      "source": [
        "# or the second\n",
        "[b for (a,b) in rant_bigrams]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1EehWXkl7aB"
      },
      "source": [
        "Let's steal the example from NLTK and find all the words which precede nouns in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PF4iEr5kQcgl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 'DT'), ('civilized', 'JJ')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "noun_preceders = [a for (a, b) in rant_bigrams if b[1] == 'NN']\n",
        "noun_preceders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf6qwQCRmCyS"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "What do you notice about the words which come before nouns?\n",
        "\n",
        "- Apply the strategy above to some longer texts of your choice (e.g., you could load in Brown?)\n",
        "- Do you find that same words appearing in front of nouns? What patterns are you noticing?\n",
        "- Do the findings make sense in terms of what you know about nouns?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23J_L0Rc64_p"
      },
      "source": [
        "# Frequency distributions and POS tags\n",
        "\n",
        "The NLTK book demonstrates that we can use frequency distributions to find the most common words associated with a particular part of speech.\n",
        "\n",
        "They do so by using a [tagged corpus](https://catalog.ldc.upenn.edu/LDC99T42) comprised of articles from the *Wall Street Journal*, using the treebank tag format. This tag format is different from the Penn tags we've been looking at thus far, and is known as the [universal tag set](https://universaldependencies.org/u/pos/).\n",
        "\n",
        "We can access this corpus through NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-UaxjLYtxMxu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ...]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load in Penn Treebank corpus using universal pos tags (they are simpler)\n",
        "wsj = nltk.corpus.treebank.tagged_words(tagset = 'universal')\n",
        "wsj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVU9XAgzpuOV"
      },
      "source": [
        "The corpus is tagged, so if we use `nltk.FreqDist()`, we will get a frequency distribution of `(word, tag)` pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UYRUcwfKx57l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[((',', '.'), 4885),\n",
              " (('the', 'DET'), 4038),\n",
              " (('.', '.'), 3828),\n",
              " (('of', 'ADP'), 2319),\n",
              " (('to', 'PRT'), 2161),\n",
              " (('a', 'DET'), 1874),\n",
              " (('in', 'ADP'), 1554),\n",
              " (('and', 'CONJ'), 1505),\n",
              " (('*-1', 'X'), 1123),\n",
              " (('0', 'X'), 1099)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a frequency distribution of the pairs\n",
        "word_tag_fd = nltk.FreqDist(wsj)\n",
        "\n",
        "# not surprisingly, the most common pairs are punctuation and function words.\n",
        "word_tag_fd.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-qtFVNp2nY"
      },
      "source": [
        "We can then run a conditional test on the freqdist to find most common words of a certain category, such as finding the most common verbs.\n",
        "\n",
        "To do so, we run a list comprehension with a conditional test over the results of the frequency distribution. Note that in the loop we specify the `((word, tag),freq)` nature of each item being iterated over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0biiWpnvqL0D"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is',\n",
              " 'said',\n",
              " 'was',\n",
              " 'are',\n",
              " 'be',\n",
              " 'has',\n",
              " 'have',\n",
              " 'will',\n",
              " 'says',\n",
              " 'would',\n",
              " 'were',\n",
              " 'had',\n",
              " 'been',\n",
              " 'could',\n",
              " \"'s\",\n",
              " 'can',\n",
              " 'do',\n",
              " 'say',\n",
              " 'make',\n",
              " 'may',\n",
              " 'did',\n",
              " 'rose',\n",
              " 'made',\n",
              " 'does',\n",
              " 'expected']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ask for just verbs (leaving most_common empty means it prints all of the words)\n",
        "# I included a slice to 25 just so it doesn't spam the screen\n",
        "[word for ((word, tag), freq) in word_tag_fd.most_common() if tag == 'VERB'][:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNWYHN_yrN5P"
      },
      "source": [
        "## Conditional Freq Dist and POS Tags\n",
        "\n",
        "Finding the most common verbs is interesting, but we can also use a conditional frequency distribution to find the frequency of specific words among different POS Tags. This allows us to find the frequency to which certain words might appear under different parts of speech.\n",
        "\n",
        "The FreqDist will be constructed in a way where each word is a dictionary key, and the values for that key will be each Part of Speech the word occurs under, followed the the frequency:\n",
        "\n",
        "```\n",
        "- Word 1\n",
        "  - POS Tag 1: Frequency\n",
        "  - POS Tag 2: Frequency\n",
        "  - etc..\n",
        "Word 2\n",
        "  - POS Tag 1: Frequency\n",
        "\n",
        "Etc..\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wUVtQ_XRypFq"
      },
      "outputs": [],
      "source": [
        "# Create a conditional frequency distribution\n",
        "wsj_cfd = nltk.ConditionalFreqDist(wsj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwKtXc0Yy4KK"
      },
      "source": [
        "Because words are the keys of the dictionary, we can query the conditional frequency distribution using the words as keys.\n",
        "\n",
        "This part is really cool – you can see that words are not always used with just one part of speech tag. When the CFD has the words as the conditions (i.e., the first part of the pair), we can see how often different POS tags occur, as the examples below show."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-3I3-Cs5yuSJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'VERB': 28, 'NOUN': 20})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wsj_cfd['yield']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uLe3Yt0PsLIW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('VERB', 28), ('NOUN', 20)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# you can use .most_common() to access the values directly\n",
        "wsj_cfd['yield'].most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nsUChX0pyubo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('VERB', 25), ('NOUN', 3)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wsj_cfd['cut'].most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5GjfZCzHdt"
      },
      "source": [
        "Some words are more restricted to specific parts of speech, these are the so-called function words. What do you think happened with these tag for `the` that are not `det`?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WhOPk4KPzJY9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('DET', 4038), ('ADJ', 5), ('NOUN', 1), ('NUM', 1)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wsj_cfd['the'].most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zc_nauEzZrK"
      },
      "source": [
        "You can obtain the initial treebank pos tagset by loading in the corpus and not specifying that you need the \"universal\" tagset. This tagset is more detailed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzdGfwpssbad"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Spend some time querying the `wsj_cfd` for different words.\n",
        "\n",
        "- Which words are more likely to appear in different POS?\n",
        "- Which ones are not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWLgA1tJNZbD"
      },
      "outputs": [],
      "source": [
        "# Look at different POS tags here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQ72fAJaLX6"
      },
      "source": [
        "# Tagging less clean data\n",
        "\n",
        "Let's briefly look at what it might be like to tag some of the data from The Current. We will load the data in and clean out the punctuation, but otherwise leave the comments as a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWX5nd7ybKf5"
      },
      "outputs": [],
      "source": [
        "# supermarkets should only sell sustainably caught fish\n",
        "# load the TP002 data to the notebook environment\n",
        "#!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp002.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BpbHkijEcCkF"
      },
      "outputs": [],
      "source": [
        "# read in the entire file\n",
        "tp002 = open('./the-current/tp002.txt', encoding=\"utf8\").read().rstrip()\n",
        "\n",
        "# remove any punctuation\n",
        "import re\n",
        "punctuation = '[#.,!\\'\"-]'\n",
        "tp002 = re.sub(pattern = punctuation, repl = '', string = tp002)\n",
        "\n",
        "# extract the comments\n",
        "tp002_comments = ' '.join([comment.split('\\t')[1] for comment in tp002.split('\\n')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UGX-ip45chwx"
      },
      "outputs": [],
      "source": [
        "# create tokens from entire set of comments\n",
        "tp002_tokens = nltk.word_tokenize(tp002_comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4EKKsXohdSkr"
      },
      "outputs": [],
      "source": [
        "# tag the tokens\n",
        "tp002_pos = nltk.pos_tag(tp002_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qWfbRqOJdcDe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('agree', 'VBP'),\n",
              " ('with', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('idea', 'NN'),\n",
              " ('but', 'CC'),\n",
              " ('there', 'EX'),\n",
              " ('also', 'RB'),\n",
              " ('needs', 'VBZ'),\n",
              " ('to', 'TO')]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# look at first ten token/tag pairs\n",
        "tp002_pos[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUkIyN8EdspG"
      },
      "source": [
        "Let's create a FreqDist of the token/tag pairs to locate some frequent and infrequent combinations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2shkbAeud11T"
      },
      "outputs": [],
      "source": [
        "tp002_fdist = nltk.FreqDist(tp002_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEaIt4y-eT2W"
      },
      "source": [
        "Look at the top ten most frequent word/tag pairs. What do you notice here - are there any commonalities among these words/parts of speech?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7yT2WCtad4HH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('the', 'DT'), 5460),\n",
              " (('to', 'TO'), 4922),\n",
              " (('we', 'PRP'), 3587),\n",
              " (('and', 'CC'), 3283),\n",
              " (('it', 'PRP'), 2829),\n",
              " (('is', 'VBZ'), 2700),\n",
              " (('be', 'VB'), 2366),\n",
              " (('a', 'DT'), 2243),\n",
              " (('for', 'IN'), 2047),\n",
              " (('our', 'PRP$'), 1909)]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# top ten most frequent word/tag pairs.\n",
        "tp002_fdist.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56rwWQhneF94"
      },
      "source": [
        "The `.hapaxes()` function returns all items which occur only one time. Look at the first ten hapaxes. What does this tell us about the data, and/or any additional preprocessing we might need to do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mKF1JbTCd4Me"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('targeting', 'VBG'),\n",
              " ('ecourages', 'VBZ'),\n",
              " ('Provide', 'VBP'),\n",
              " ('sthe', 'NN'),\n",
              " ('cheap/unethical', 'JJ'),\n",
              " ('pork/chicken', 'NN'),\n",
              " ('guidelines', 'NNS'),\n",
              " ('Therell', 'NNP'),\n",
              " ('flowon', 'JJ'),\n",
              " ('Encouraging', 'NNP')]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# first ten hapaxes in the data\n",
        "tp002_fdist.hapaxes()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbYRCrjumbON"
      },
      "source": [
        "# **Adding your own tags**\n",
        "\n",
        "You can manually tag your own text using a built-in NLTK function, `nltk.tag.str2tuple`. You supply a word and tag in the form of `word/tag', and the result is a (word, tag) pair. For example, below I add the POS tag \"NN\" to the word \"fly\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vNpCIXbnmgju"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fly', 'NN')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
        "tagged_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds8kPB9qeznd"
      },
      "source": [
        "You can then quickly add a number of tags to a text in one go by writing raw strings with word/tag pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QWKSijzinB5I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['We/PRP', 'live/VB', 'in/AT', 'a/DET', 'society/NN']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add tags to a longer text, then tokenize\n",
        "raw_text = 'We/PRP live/VB in/AT a/DET society/NN'\n",
        "\n",
        "tokenized_text = nltk.word_tokenize(raw_text)\n",
        "tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vx4Ulmn0noml"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('We', 'PRP'), ('live', 'VB'), ('in', 'AT'), ('a', 'DET'), ('society', 'NN')]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now split the tags\n",
        "[nltk.tag.str2tuple(w) for w in tokenized_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx47H0rkfEyh"
      },
      "source": [
        "## Why would you want to add your own tags?\n",
        "\n",
        "You might wonder what the point of adding your own tags is, especially if you find you are not super confident about which tags should be used for a particular word! One of the reasons NLTK shows this to you is to provide a glimpse into how some corpora come tagged, and how NLTK reads those tags and provides them to you.\n",
        "\n",
        "Another potential use is that you could in theory supply any tags you wanted to your text. So, instead of tagging each word with part of speech, you could devise your own coding scheme for other properties of words. For example, if you had an example of speech which mixed two languages, you could tag the language of each word:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FvaZdBrvfjZV"
      },
      "outputs": [],
      "source": [
        "# tag words in English/Mandarin\n",
        "code_switch = 'Soda/EN is/EN a/EN 很/ZH 乖/ZH 的/ZH 狗/ZH'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "D25mnIUCf1ib"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Soda', 'EN'),\n",
              " ('is', 'EN'),\n",
              " ('a', 'EN'),\n",
              " ('很', 'ZH'),\n",
              " ('乖', 'ZH'),\n",
              " ('的', 'ZH'),\n",
              " ('狗', 'ZH')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now my text can be searched for \"EN\" or \"ZH\"\n",
        "[nltk.tag.str2tuple(w) for w in nltk.word_tokenize(code_switch)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x3LtYxixgLP"
      },
      "source": [
        "## **Your Turn**\n",
        "\n",
        "- It might be useful to think about some other categories you might be interested in applying to words\n",
        "- At the least, play around with `str2tuple` and make sure you have a handle on it"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNp+RQ+jvHHq3XFIm3PrW1V",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
