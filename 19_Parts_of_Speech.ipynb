{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOe7ZtASbcKPNqOzAARUrcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/19_Parts_of_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7XKg53_5d6C"
      },
      "source": [
        "# **Parts of Speech**\n",
        "\n",
        "This notebook discusses a very important lexical property: parts of speech. We have thus far dealt with searching and manipulating text based on the orthographic features of words (i.e., their written forms). That means we have remained focused on the forms of the words (i.e., types and tokens). This has given us the ability to measure basic yet important aspects of texts: length, word frequency, and also some considerations about lexical diversity.\n",
        "\n",
        "You probably have some familiarity with how words are classified into different lexical and syntactic categories, such as nouns, verbs, adjectives, pronouns, etc. These categories are called *parts of speech* (POS), and can be used as another source of information which can be exploited during linguistic analysis of texts.\n",
        "\n",
        "For NLP and Computational Linguistics, it is common to see reference made to POS **Tags**, which are essentially labels or annotations associated with a word to represent more information about that word. These tags can be counted and compared, and also provide critical information for building and understanding grammars of languages.\n",
        "\n",
        "Let's explore how to tag texts and then use these tags to query information about texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqS5Ui7-3PZl"
      },
      "source": [
        "## Using `nltk.pos_tag()` to automatically tag texts.\n",
        "\n",
        "Fortunately for us, NLTK includes a function which will automatically assign part of speech tags to a text. To use this function, we need to import NLTK and download some additional resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrbjuen42OPj"
      },
      "outputs": [],
      "source": [
        "# import NLTK and download the necessary resources\n",
        "import nltk\n",
        "# import resources for tokenizing and tagging\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets', 'treebank', 'universal_tagset', 'book'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz2iIYaz3K4_"
      },
      "source": [
        "\n",
        "The NLTK function expects a list of tokens and is used like this:\n",
        "\n",
        "> `nltk.pos_tag(tokens)`\n",
        "\n",
        "The results will be a list of `(word,tag)` pairs (which are in the form of a tuple.)\n",
        "\n",
        "The next three cells demonstrate an example of how to POS tag a text using nltk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv4eYY2vtiTn"
      },
      "outputs": [],
      "source": [
        "# step 1: have some text\n",
        "rant = \"You know, we're living in a society! We're supposed to act in a civilized way.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW8_DVm7tr6G"
      },
      "outputs": [],
      "source": [
        "# step 2: tokenize\n",
        "rant_tokens = nltk.word_tokenize(rant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgRt_jykt3bS"
      },
      "outputs": [],
      "source": [
        "# step 3: tag\n",
        "rant_pos = nltk.pos_tag(rant_tokens)\n",
        "\n",
        "# look at the resulting (word, tag) pairs\n",
        "[tagged for tagged in rant_pos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MQhZJCeuSYO"
      },
      "source": [
        "POS tagging has created a list of tuples of our words, with `(word, tag)` format. The tags are informative and indeed go beyond broader word categories such as NOUN and VERB. For example,\n",
        "\n",
        "```\n",
        "VBP = verb, present tense, not 3rd person singular\n",
        "```\n",
        "\n",
        "while\n",
        "\n",
        "```\n",
        "VBG = verb, present participle or gerund.\n",
        "```\n",
        "\n",
        "These tags are from the Penn tagset which is a very commonly used set of POS tags. You can run the following cell to see the full list or by [going to this link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
        "\n",
        "Take a moment to scroll throught these tags and explore their explanations and examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQusTQjMudIS"
      },
      "outputs": [],
      "source": [
        " # full list of tags, with definitions and examples\n",
        " nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9kuNUXci2R"
      },
      "source": [
        "You can also look up one specific tag by supplying the tag as a string to the `help` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpqE3Yb6cnkA"
      },
      "outputs": [],
      "source": [
        "# what is the NNP tag?\n",
        "nltk.help.upenn_tagset('NNP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLzeBkKRIHx-"
      },
      "source": [
        "### **Discussion**\n",
        "\n",
        "Part of speech tags help make sense of words in the context of other words. Consider this example from the NLTK book - what is the difference in use for the instances of *refuse* and *permit*?\n",
        "\n",
        "Think back to what we know about bigrams. Are there any clues  provided by the words which come *before* refuse/permit that might facilitate tagging of the proper part of speech?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YAYJBKBdTh9"
      },
      "outputs": [],
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzIaCK5sdc17"
      },
      "source": [
        "We can supply our own examples as well — let us compare two uses of the same word \"comb\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-zzcRuIIG_s"
      },
      "outputs": [],
      "source": [
        "# what pos tag does the word \"comb\" have in this example?\n",
        "nltk.pos_tag(nltk.word_tokenize('Quick, comb the desert for droids!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sZL8rR0IcOZ"
      },
      "outputs": [],
      "source": [
        "# and what pos tag does the word \"comb\" have in this example?\n",
        "nltk.pos_tag(nltk.word_tokenize('Where is my comb?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOnAeZTIslo"
      },
      "source": [
        "So, adding POS tag information provides more information about a text, which becomes useful for more advanced NLP applications such as information extraction, text prediction, and so on. Because the tags are stores as strings, you can use knowledge of Python to search or filter through the list in order to find specific words associated with specific tags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHxQsM27FUA"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "- Explore using `nltk.pos_tag()` on some texts.\n",
        "- See if you can understand the different POS tags and what they mean about the words.\n",
        "- Can you \"break\" the tagger or have it produce innaccurate results?\n",
        "- The tagger has a rule that if it does not know the tag for a word, it will automatically assign a default POS tag. Can you figure out what this default tag is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwV8MINvygA"
      },
      "source": [
        "# Bigrams and Parts of Speech\n",
        "\n",
        "NLTK reviews major parts of speech, with examples. Regardless of what you think you might know (or not know) about grammar/language, you should carefully read these sections. Look at the patterns associated with different parts of speech – these patterns are crucial for training taggers. This is evidenced in the example showing how bigrams of POS tags show typically English word order. We can try the same with our own example.\n",
        "\n",
        "Let's use `nltk.bigrams()` on a set of tagged tokens — this will create a set of ((word, tag) , (word, tag)) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIT7JrbmwWIX"
      },
      "outputs": [],
      "source": [
        "# create bigrams of our pos tagged example\n",
        "rant_bigrams = [bigram for bigram in nltk.bigrams(rant_pos)]\n",
        "\n",
        "# inspect the bigrams\n",
        "rant_bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjOJpTooHuvu"
      },
      "source": [
        "Now that we have the part of speech information included with our words, we can shift our search patterns away from the orthographic forms of words to instead the part of speech of words. This allows us to find more abstract patterns in language associated with word *categories* rather than with the forms of words themselves.\n",
        "\n",
        "For instance, let's look for all words in our example which come before nouns. This requires a bit of slicing, because we are looping through pairs set within a single tuple\n",
        "\n",
        "```\n",
        "((word, tag), (word, tag))\n",
        "```\n",
        "\n",
        "So to access the word in the first pair, we would first index the larger tuple using `[0]` to get the first `(word, tag)` pair, then then index that pair using `[0]` to get the first part of `(word, tag)`, which would be the word. This is demonstrated in the next code cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvHllia4SEka"
      },
      "outputs": [],
      "source": [
        "for i in rant_bigrams:\n",
        "  print(i[0][0]) #index the first nested tuple, then index that tuples first value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrIOqdE6k3ZH"
      },
      "source": [
        "Another strategy would be to follow the NLTK book's guide and set the tuple pair as the iterator, allowing you to index the tuple in a more transparent way.\n",
        "\n",
        "```\n",
        "[a for (a,b) in bigrams]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVybDY4nlnvf"
      },
      "outputs": [],
      "source": [
        "# you can select the first pair of each pair\n",
        "[a for (a, b) in rant_bigrams]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DClyNDoil3tK"
      },
      "outputs": [],
      "source": [
        "# or the second\n",
        "[b for (a,b) in rant_bigrams]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1EehWXkl7aB"
      },
      "source": [
        "Let's steal the example from NLTK and find all the words which precede nouns in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF4iEr5kQcgl"
      },
      "outputs": [],
      "source": [
        "noun_preceders = [a for (a, b) in rant_bigrams if b[1] == 'NN']\n",
        "noun_preceders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf6qwQCRmCyS"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "What do you notice about the words which come before nouns?\n",
        "\n",
        "- Apply the strategy above to some longer texts of your choice (e.g., you could load in Brown?)\n",
        "- Do you find that same words appearing in front of nouns? What patterns are you noticing?\n",
        "- Do the findings make sense in terms of what you know about nouns?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23J_L0Rc64_p"
      },
      "source": [
        "# Frequency distributions and POS tags\n",
        "\n",
        "The NLTK book demonstrates that we can use frequency distributions to find the most common words associated with a particular part of speech.\n",
        "\n",
        "They do so by using a [tagged corpus](https://catalog.ldc.upenn.edu/LDC99T42) comprised of articles from the *Wall Street Journal*, using the treebank tag format. This tag format is different from the Penn tags we've been looking at thus far, and is known as the [universal tag set](https://universaldependencies.org/u/pos/).\n",
        "\n",
        "We can access this corpus through NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UaxjLYtxMxu"
      },
      "outputs": [],
      "source": [
        "# load in Penn Treebank corpus using universal pos tags (they are simpler)\n",
        "wsj = nltk.corpus.treebank.tagged_words(tagset = 'universal')\n",
        "wsj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVU9XAgzpuOV"
      },
      "source": [
        "The corpus is tagged, so if we use `nltk.FreqDist()`, we will get a frequency distribution of `(word, tag)` pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYRUcwfKx57l"
      },
      "outputs": [],
      "source": [
        "# create a frequency distribution of the pairs\n",
        "word_tag_fd = nltk.FreqDist(wsj)\n",
        "\n",
        "# not surprisingly, the most common pairs are punctuation and function words.\n",
        "word_tag_fd.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-qtFVNp2nY"
      },
      "source": [
        "We can then run a conditional test on the freqdist to find most common words of a certain category, such as finding the most common verbs.\n",
        "\n",
        "To do so, we run a list comprehension with a conditional test over the results of the frequency distribution. Note that in the loop we specify the `((word, tag),freq)` nature of each item being iterated over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0biiWpnvqL0D"
      },
      "outputs": [],
      "source": [
        "# ask for just verbs (leaving most_common empty means it prints all of the words)\n",
        "# I included a slice to 25 just so it doesn't spam the screen\n",
        "[word for ((word, tag), freq) in word_tag_fd.most_common() if tag == 'VERB'][:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNWYHN_yrN5P"
      },
      "source": [
        "## Conditional Freq Dist and POS Tags\n",
        "\n",
        "Finding the most common verbs is interesting, but we can also use a conditional frequency distribution to find the frequency of specific words among different POS Tags. This allows us to find the frequency to which certain words might appear under different parts of speech.\n",
        "\n",
        "The FreqDist will be constructed in a way where each word is a dictionary key, and the values for that key will be each Part of Speech the word occurs under, followed the the frequency:\n",
        "\n",
        "```\n",
        "- Word 1\n",
        "  - POS Tag 1: Frequency\n",
        "  - POS Tag 2: Frequency\n",
        "  - etc..\n",
        "Word 2\n",
        "  - POS Tag 1: Frequency\n",
        "\n",
        "Etc..\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUVtQ_XRypFq"
      },
      "outputs": [],
      "source": [
        "# Create a conditional frequency distribution\n",
        "wsj_cfd = nltk.ConditionalFreqDist(wsj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwKtXc0Yy4KK"
      },
      "source": [
        "Because words are the keys of the dictionary, we can query the conditional frequency distribution using the words as keys.\n",
        "\n",
        "This part is really cool – you can see that words are not always used with just one part of speech tag. When the CFD has the words as the conditions (i.e., the first part of the pair), we can see how often different POS tags occur, as the examples below show."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3I3-Cs5yuSJ"
      },
      "outputs": [],
      "source": [
        "wsj_cfd['yield']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLe3Yt0PsLIW"
      },
      "outputs": [],
      "source": [
        "# you can use .most_common() to access the values directly\n",
        "wsj_cfd['yield'].most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsUChX0pyubo"
      },
      "outputs": [],
      "source": [
        "wsj_cfd['cut'].most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5GjfZCzHdt"
      },
      "source": [
        "Some words are more restricted to specific parts of speech, these are the so-called function words. What do you think happened with these tag for `the` that are not `det`?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhOPk4KPzJY9"
      },
      "outputs": [],
      "source": [
        "wsj_cfd['the'].most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zc_nauEzZrK"
      },
      "source": [
        "You can obtain the initial treebank pos tagset by loading in the corpus and not specifying that you need the \"universal\" tagset. This tagset is more detailed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzdGfwpssbad"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Spend some time querying the `wsj_cfd` for different words.\n",
        "\n",
        "- Which words are more likely to appear in different POS?\n",
        "- Which ones are not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWLgA1tJNZbD"
      },
      "outputs": [],
      "source": [
        "# Look at different POS tags here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbX7juAMJ995"
      },
      "outputs": [],
      "source": [
        "nltk.FreqDist(nltk.pos_tag(nltk.word_tokenize('Where is my comb? Please comb the desert for droids!')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tagging less clean data\n",
        "\n",
        "Let's briefly look at what it might be like to tag some of the data from The Current. We will load the data in and clean out the punctuation, but otherwise leave the comments as a single string."
      ],
      "metadata": {
        "id": "SJQ72fAJaLX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supermarkets should only sell sustainably caught fish\n",
        "# load the TP002 data to the notebook environment\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp002.txt'"
      ],
      "metadata": {
        "id": "EWX5nd7ybKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the entire file\n",
        "tp002 = open('tp002.txt').read().rstrip()\n",
        "\n",
        "# remove any punctuation\n",
        "import re\n",
        "punctuation = '[#.,!\\'\"-]'\n",
        "tp002 = re.sub(pattern = punctuation, repl = '', string = tp002)\n",
        "\n",
        "# extract the comments\n",
        "tp002_comments = ' '.join([comment.split('\\t')[1] for comment in tp002.split('\\n')])"
      ],
      "metadata": {
        "id": "BpbHkijEcCkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokens from entire set of comments\n",
        "tp002_tokens = nltk.word_tokenize(tp002_comments)"
      ],
      "metadata": {
        "id": "UGX-ip45chwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tag the tokens\n",
        "tp002_pos = nltk.pos_tag(tp002_tokens)"
      ],
      "metadata": {
        "id": "4EKKsXohdSkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at first ten token/tag pairs\n",
        "tp002_pos[:10]"
      ],
      "metadata": {
        "id": "qWfbRqOJdcDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a FreqDist of the token/tag pairs to locate some frequent and infrequent combinations:\n"
      ],
      "metadata": {
        "id": "xUkIyN8EdspG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp002_fdist = nltk.FreqDist(tp002_pos)"
      ],
      "metadata": {
        "id": "2shkbAeud11T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the top ten most frequent word/tag pairs. What do you notice here - are there any commonalities among these words/parts of speech?"
      ],
      "metadata": {
        "id": "AEaIt4y-eT2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top ten most frequent word/tag pairs.\n",
        "tp002_fdist.most_common(10)"
      ],
      "metadata": {
        "id": "7yT2WCtad4HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `.hapaxes()` function returns all items which occur only one time. Look at the first ten hapaxes. What does this tell us about the data, and/or any additional preprocessing we might need to do?"
      ],
      "metadata": {
        "id": "56rwWQhneF94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first ten hapaxes in the data\n",
        "tp002_fdist.hapaxes()[:10]"
      ],
      "metadata": {
        "id": "mKF1JbTCd4Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbYRCrjumbON"
      },
      "source": [
        "# **Adding your own tags**\n",
        "\n",
        "You can manually tag your own text using a built-in NLTK function, `nltk.tag.str2tuple`. You supply a word and tag in the form of `word/tag', and the result is a (word, tag) pair. For example, below I add the POS tag \"NN\" to the word \"fly\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNpCIXbnmgju"
      },
      "outputs": [],
      "source": [
        "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
        "tagged_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds8kPB9qeznd"
      },
      "source": [
        "You can then quickly add a number of tags to a text in one go by writing raw strings with word/tag pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWKSijzinB5I"
      },
      "outputs": [],
      "source": [
        "# add tags to a longer text, then tokenize\n",
        "raw_text = 'We/PRP live/VB in/AT a/DET society/NN'\n",
        "\n",
        "tokenized_text = nltk.word_tokenize(raw_text)\n",
        "tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx4Ulmn0noml"
      },
      "outputs": [],
      "source": [
        "# now split the tags\n",
        "[nltk.tag.str2tuple(w) for w in tokenized_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx47H0rkfEyh"
      },
      "source": [
        "## Why would you want to add your own tags?\n",
        "\n",
        "You might wonder what the point of adding your own tags is, especially if you find you are not super confident about which tags should be used for a particular word! One of the reasons NLTK shows this to you is to provide a glimpse into how some corpora come tagged, and how NLTK reads those tags and provides them to you.\n",
        "\n",
        "Another potential use is that you could in theory supply any tags you wanted to your text. So, instead of tagging each word with part of speech, you could devise your own coding scheme for other properties of words. For example, if you had an example of speech which mixed two languages, you could tag the language of each word:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvaZdBrvfjZV"
      },
      "outputs": [],
      "source": [
        "# tag words in English/Mandarin\n",
        "code_switch = 'Soda/EN is/EN a/EN 很/ZH 乖/ZH 的/ZH 狗/ZH'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D25mnIUCf1ib"
      },
      "outputs": [],
      "source": [
        "# now my text can be searched for \"EN\" or \"ZH\"\n",
        "[nltk.tag.str2tuple(w) for w in nltk.word_tokenize(code_switch)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x3LtYxixgLP"
      },
      "source": [
        "## **Your Turn**\n",
        "\n",
        "- It might be useful to think about some other categories you might be interested in applying to words\n",
        "- At the least, play around with `str2tuple` and make sure you have a handle on it"
      ]
    }
  ]
}