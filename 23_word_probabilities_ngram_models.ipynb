{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHLFZOLPBMg6XXupNr4Xbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/23_word_probabilities_ngram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvNLWVTCYCy4"
      },
      "source": [
        "# **Word Probabilities and ngram models**\n",
        "The application of word probabilities we are concerned about here has to do with *predicting* which word will come next given any one particular sequence of words – predicting the future! (well, not really...)\n",
        "\n",
        "As you have hopefully come to learn, language is largely patterned, and corpora are very useful for finding and analysing these patterns. The word probabilities discussed in this notebook will also exploit the patterned nature of language. However, the word probabilities are going to be a bit different in that they use frequency statistics and chained probabilities (eek, math!) to predict words. \n",
        "\n",
        "\n",
        "## **N-grams**\n",
        "You have seen reference made to the term `n-grams`. I'm probably repeating myself from an earlier notebook, but as a reminder, to understand what this means, consider that a single word can also be called a `unigram`, a pair of words can be called a `bigram`, three is a `trigram`, and so on. So the \"n\" in `n-gram` can be any sequence of words of any length long (e.g., you could have a 100-gram). However, think about our work with corpus patterns – most of them are not this large! As NLTK explained a long time ago, bi-grams and tri-grams end up being very helpful for predicting future words.\n",
        "\n",
        "There are two other books which I thought about using for this course and have informed this notebook. One is called [*Language and Computers*](https://www.wiley.com/en-us/Language+and+Computers-p-9781405183055), and the other is called [*Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/).\n",
        "\n",
        "When discussing word probabilities, both texts use a similar approach to describing how ngram models are used in NLP (*L&C*, pp. 26-28; *SLP*, pp. 29-35). Each uses an uncompleted sentence to make the same point – you probably have a really good guess for which words come after other certain words. (*L&C* uses the example \"I dreamed I saw the knights in...\", whereas *SLP* uses \"Please turn your homework...\"). \n",
        "\n",
        "But, why do **you** have that knowledge, as a human? What does this say about the brain and language – is our language knowledge a series of statistical connections? Is the brain a computer? Keep this question in the back of your mind while you read on and complete this notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZGVBs1egqee"
      },
      "source": [
        "## **Word probabilities for single tokens**\n",
        "\n",
        "Let's start small with making sure we understand how to calculate probabilities. We just need to learn some new terms to unpack probablities. You might have taken a statistics course that covered this concept, or this might be your first time. It should be just a light touch!\n",
        "\n",
        "`P(A)` is a shorthand for representing the probability (or likelihood) that something (`A`) will occur. For word probabilities, we can think of this as telling us how likely a single token will occur in some larger context, such as the likelihood of randomly drawing a particular token from a document. Because in this scenario we assume all tokens would have an equally likely chance of being selected, we can rely on a fairly simple method for calculating our probability: counting.\n",
        "\n",
        "If we want to predict the probability of randomly selecting a single token from a document we only have two possible outcomes: the randomly selected token either *is* or *is not* our target token. So, the probability of choosing any one token is based on the total number of tokens in the document/space we are searching. If I had a sentence of four different tokens, each token would have a 25% probability of being randomly chosen. Once we start repeating the same token in a text, the chances of randomly selecting tokens of that type increase. \n",
        "\n",
        "We can apply this calculation to an entire text, keeping in mind the distinction between types and tokens. To calculate this, we count how many tokens for our target occur in the document/space, and divide that by the total space we are sampling from (i.e., the total number of tokens in a document). \n",
        "\n",
        "Therefore, the probability of choosing a word (`P(A)`) is directly linked to the overall frequency of the tokens representing that word in a document. \n",
        "\n",
        "For example, if the word `pretzels` occurs 100 times in a 1000 word document, the probability of choosing a random token from the document and that token being `pretzels` would be be 100/1000, which is 0.1, or 10%.\n",
        "\n",
        "So the `P(A)` for this example says there is a 10% likelihood to encounter the token `pretzels` if we randomly sampled one token from this hypothetical document.\n",
        "\n",
        "Of course, that's not very useful yet. We want to make predictions about the presence of one token in light of *other* tokens. So, we need to modify the approach to take this information into account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uye0vAmaCLs5"
      },
      "source": [
        "## **Changing the search space to other words**\n",
        "\n",
        "We can calculate the probability for a word occuring after *other* words by changing a word's search space to be specific patterns of tokens (rather than an entire document). In the example above, our search space was every possible word in the document, which does not take into account any other information about where the word occurs.\n",
        "\n",
        "However if we predefine a smaller context for a search space, such as a smaller sequence of tokens, we can then divide by only the number of times that sequence appears (rather than the size of an entire document).\n",
        "\n",
        "Say for instance we wanted to know the probability of the word `\"thirsty\"` appearing after the words `\"these pretzels are making me`\". To get this prediction, we need to count the total instances of the complete sentence `\"these pretzels are making me thirsty\"` (instead of counting the the total number of times `thirsty` appears). \n",
        "\n",
        "If I search for the phrase \"`these pretzels are making me thirsty`\" in Google, I get 174,000 hits. Let's pretend that's the total frequency of the phrase in a corpus. \n",
        "\n",
        "We could then calculate the probability of `\"thirsty\"` appearing in this context by dividing the total number of times the sentence occurs with *any* word, not just `\"thirsty\"`. In Google, I get 11,200,000 results for the phrase `\"these pretzels are making me\"` follwed by any word (including \"thirsty\").\n",
        "\n",
        "We then divide the frequency of our target phrase by the frequency of the target phrase *with* our target word. In this case it would be `59,000/64,800`, which is 1.5% That might not seem like a lot, but a 1.5% predictive chance is likely a lot higher than any other word which can fit into that slot (and is aided by the frequency of this phrase being associated with a Seinfeld episode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_RTPuqCRqOv"
      },
      "outputs": [],
      "source": [
        "174000/11200000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OSxrIk8Rmgj"
      },
      "source": [
        "## **Issues with using full sentences as contexts**\n",
        "\n",
        "As is pointed out by the authors of *SLP*, this method brings with it a few challenges. Language is creative and even similar sentences might be phrased in slightly different ways, leading to a lot of different possible sentences that need to be counted. Language corpora cannot be guaranteed to contain all of these creative examples, and thus prediction will suffer. (Moreover, using Google like I did is not really a good method because we are not really sampling from a corpus of language – I mainly used it to represent the concept).   \n",
        "\n",
        "The only reason my sentence above was so frequent is because it is a well known quote from *Seinfeld* and has apparently been repeated many, many times on the internet! \n",
        "\n",
        "The solution to address this challenge is to avoid trying to calculate the probability of a word based on the entire sentence context, but instead calculate the individual probabilities of `n-grams` in a sentence, and then multiply these probabilities together as they appear in sentences. This means we want to calculate the combined probability of `\"thirsty\"` coming after `\"me\"` and of `\"me\"` coming after the word `\"making\"`, and so on. This smaller combined probability approximates using the entire sentence context.\n",
        "\n",
        "In other words, this approach makes an assumption that the probabilities associated with the prior 1-2 words of a word are predictive enough to stand in for longer sentence contexts - which *SLP* explains is a **Markov assumption**. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6X6pn66jKFJ"
      },
      "source": [
        "## **Conditional probabilities and n-grams**\n",
        "So, how can we go about doing this? Now we are more interested in how to predict a word *given a prior word context* that is limited to just one or two words. Let's just stick with one word for now. \n",
        "\n",
        "We need to expand our probability notation to include the likelihood of something occuring when something *else* also occurs. This is the exact same as what we have done above, except we are changing our contexts of counting. Let's also add to our notation for probabilities.\n",
        "\n",
        "`P(A|B)` is a shorthand for representing the probability (or likelihood) that something (A) will occur **if** something else occurs (B). This is called the [`conditional probability`](http://www.stat.yale.edu/Courses/1997-98/101/condprob.htm). If you clicked that link and read the examples (or remember from prior courses / reading), you'll see that conditional probability is just more counting and then some division. \n",
        "\n",
        "In this approach, our *condition* is the appearance of a prior word, and we want to see how strongly that condition is associated with the occurence of our target word. \n",
        "\n",
        "For predicting a probability on a conditional event (such as word A occuring after word B), we divide the total count of the event occuring (such as word A occuring after word B) by the total number of times the event *could* occur (such as the word B occuring before another word) – this is what I did above with the pretzels example, except now we are looking at individual words rather than full sentences. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLp-Cqu7GtRb"
      },
      "source": [
        "## **Let's calculate a conditional probability**\n",
        "\n",
        "*SLP* gives the following formula to calculate the conditional probability of word cooccurence:\n",
        "\n",
        "`P(Wn|Wn-1) = c(Wn-1Wn)/c(Wn-1)`\n",
        "\n",
        "Here is what the symbols mean:\n",
        "\n",
        "`W = word`\n",
        "\n",
        "`n = the index or identity of a word (think of it like a Python list index)`\n",
        "\n",
        "`Wn-1 = the word occuring one index *before* the word we want to predict`\n",
        "\n",
        "`c = the frequency count`\n",
        "\n",
        "Let's break the formula down using the example of `\"these pretzels\"`. If we wanted to predict the likelihood of `\"pretzels\"` coming after the word `\"these\"`, the notation is thus:\n",
        "\n",
        "```\n",
        "# divide the total frequency of \"these pretzels\" by the total frequency of \"these\"\n",
        "P(\"pretzels\"|\"these\") = c(\"these pretzels\")/c(\"these\")\n",
        "```\n",
        "\n",
        "Hypothetically, if in a particular search space the bigram `\"these pretzels\"` occured 20 times and `\"these\"` occured 128 times, then:\n",
        "\n",
        "`P(\"pretzels\" | \"these\") = 20/128 = .156`\n",
        "\n",
        "Our definition above says we should divide the total count of our target bigram (`these pretzels`) by the total number of times the the prior word (`these`) could occur before *any word*, including our target word. So technically this value should be the number of bigrams involving the first word. As is pointed out by *SLP*, however, we can use the simple frequency of the word before our target word as the denominator. \n",
        "\n",
        " **[SLP](https://web.stanford.edu/~jurafsky/slp3/3.pdf) says \"the reader should take a moment to be convinced by this\" (p. 32)** \n",
        "\n",
        "Are you? Think...can the total frequency of the number of times word B (`these`) potentially occurs before any other word ever be different than the total number of times word B occurs on its own?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2v5W2Fqp0XN"
      },
      "source": [
        "## **A longer example**\n",
        "Let's work with more data and start writing some functions to calculate these probabilities for an input text. \n",
        "\n",
        "Consider the following sentence: \n",
        "\n",
        ">`\"these pretzels are making me thirsty! They are also making me tired! I love preztels because they make me thirsty!\"`\n",
        "\n",
        "Let's now consider the  probabilities of the word `\"thirsty\"`. In our sentence, `\"thirsty\"` occurs two times, and both times after the word `\"me\"`. We also see that `\"me\"` occurs three times, two times before `\"thirsty\"` but also one time before `\"tired\"`. Let's calculate some basic probabilities for these words.\n",
        "\n",
        "### **Calculate single word probabilities**\n",
        "\n",
        "We can write some quick code to:\n",
        "\n",
        "1. Count the total number of words\n",
        "2. Count the frequency of the words we are interested in (`\"thirsty,\" \"tired,\"` and `\"me\"`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MFwkMRf9Xa1"
      },
      "outputs": [],
      "source": [
        "# Create the data\n",
        "# triple quote to break lines\n",
        "text_input = \"\"\"these pretzels are making me thirsty\n",
        "They are also making me tired\n",
        "I love pretzels because they make me thirsty\"\"\"\n",
        "\n",
        "# split data into separate words\n",
        "text_input_words = str.split(text_input)\n",
        "print(text_input_words)\n",
        "\n",
        "# total search space\n",
        "len(text_input_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZAkbeUx-nX6"
      },
      "outputs": [],
      "source": [
        "# Get frequency of each word and save to a dictionary\n",
        "word_count = dict()\n",
        "\n",
        "# here are our target words\n",
        "target_words = ['thirsty', 'tired', 'me']\n",
        "\n",
        "# loop and add the freq to dict\n",
        "for word in target_words:\n",
        "  # using .count to get frequency. \n",
        "  word_count[word] = text_input.count(word)\n",
        "\n",
        "word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7aS2jM8Npiv"
      },
      "outputs": [],
      "source": [
        "# what is the probability of each word, on its own?\n",
        "for word in word_count:\n",
        "  print(f'word: {word} | total frequency: {word_count[word]} | probability: {word_count[word]/len(text_input_words)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RHple9gck1x"
      },
      "source": [
        "Since we know that `thirsty` occurs twice, both times before `me`, and we also know that there are two instances of our target bigram, `me thirsty`. We know that `me` occurs three times total, twice with `thirsty` and once with `tired`\n",
        "\n",
        "So `P(thirsty|me) = c(\"me thirsty\")/c(\"me\") = 2/3 = .66`\n",
        "\n",
        "Within this search space, the word `thirsty` has a 10% chance of occuring overall, but a 66% chance of coming after `me.` With such a small sentence, this isn't very useful. We want to start increasing the search space and looking directly at bigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ObxCwgPQ_V"
      },
      "source": [
        "### **Calculate bigram probabilities**\n",
        "\n",
        "The next step is to expand this program to calculate the bigram probabilities of a text. Our program will take an input text and an input word, and then calculate all of the bigrams and their probabilities for that word given the input text. \n",
        "\n",
        "Below, I define a function the takes in a raw string and tokenizes it using the NLTK tokenizer. \n",
        "\n",
        "I then initialize empty dictionaries to store frequency values of each bigram including our target word (`bigram_counts`), as well as the frequency of the word which appears *before* our target word (`headword_totals`). I also create a third dictionary to store the calculated probabilities (`bigram_probabilities`).\n",
        "\n",
        "I then loop through each word in the text, searching for our target word. If I find that word, I then check to see if there is a token before that word, and also whether that prior token is not punctuation. If so, I then add that prior token as the `headword`. I calculate the overall frequency of the `headword` and store it in a dictionary, but only if the headword is not already in the dictionary. \n",
        "\n",
        "I then use the `.get()` method to update the frequency of that particular bigram in the `bigram_counts` dictionary (see the comments in the function for how this works). \n",
        "\n",
        "Once the loop is complete, I then loop over each bigram and calculate the bigram probability by dividing the frequency of the bigram by the total frequency of the headword. I save this to a dictionary although that's probably not necessary (well, I'm sure most of this function is not necessary :))\n",
        "\n",
        "I then use a set of print statements to output the information about the bigram and headword, so that you can see how the calculations are being made. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suN2YgJJD8A2"
      },
      "outputs": [],
      "source": [
        "# want to use nltk.tokenize()\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWvkilOPQSX9"
      },
      "outputs": [],
      "source": [
        "# calculate bigrams in a text for target words\n",
        "def bigram_counter(text, target):\n",
        "  \"\"\"return probability of a target word for every bigram headword it is associated with\"\"\"\n",
        "  \n",
        "  # tokenize the lowered text. \n",
        "  text_lower = nltk.word_tokenize(text.lower())\n",
        "\n",
        "  #### initialize empty dictionaries to store frequencies/probabilities ####\n",
        "\n",
        "  # frequency of target bigrams\n",
        "  bigram_counts = dict()\n",
        "  # frequency of the bigram headword\n",
        "  headword_frequencies = dict()\n",
        "  # store probabilities here\n",
        "  bigram_probabilities = dict()\n",
        "\n",
        "  ### calculate bigrams ###\n",
        "\n",
        "  # loop through input text using enumerate, which provides an index\n",
        "  for word_index, word in enumerate(text_lower):\n",
        "\n",
        "    # find instances of target word, then extract headword\n",
        "    if word == target and word_index != 0: # for all instances that are not sentence initial\n",
        "      if text_lower[word_index-1].isalpha(): # this will avoid treating punctuation as headwords\n",
        "        headword = text_lower[word_index-1]\n",
        "      \n",
        "      # add the total frequency of the headword to the dictionary if it doesn't yet exist.\n",
        "        if headword not in headword_frequencies.keys():\n",
        "\n",
        "          headword_frequencies[headword] = text_lower.count(headword)\n",
        "      \n",
        "      # update the bigram dictionary\n",
        "      # .get() will return the value for a key in a dictionary, \n",
        "      # if the key does not exist, it will return the default instead\n",
        "      # so this says make an entry in the dictionary using the headword\n",
        "      # if the entry does not exist, set it to the default, which is 0\n",
        "      # if the entry does exist, grab the value\n",
        "      # then add 1 to either the default or the value \n",
        "      # (now you can see why defaultdict might be better)\n",
        "        bigram_counts[(headword, word)] = bigram_counts.get((headword, word), 0) + 1\n",
        "\n",
        "  # once the loop for the target word is complete, \n",
        "  # calculate probability of word for each headword\n",
        "  # print out the information\n",
        "  print(f'total frequency of {target} is {text_lower.count(target)} \\n')\n",
        "  print(f'probabilities for bigrams with {target} are:\\n')\n",
        "  for bigram in bigram_counts:\n",
        "    bigram_probabilities[bigram] = bigram_counts[bigram] / headword_frequencies[bigram[0]] # slicing just the headword\n",
        "    print(f'total frequency of bigram {bigram}: {bigram_counts[bigram]}')\n",
        "    print(f'total frequency of headword {bigram[0]} is {headword_frequencies[bigram[0]]}') # slicing just the headword\n",
        "    # I use .upper() to make the words stand out\n",
        "    print(f'probability of {target.upper()} after {bigram[0].upper()}: {bigram_probabilities[bigram]} \\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuA3JtXJITsy"
      },
      "source": [
        "Let's practice our function on Bill Clinton's 2000 State of the Union Address. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8ygSyObVx1K"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('state_union')\n",
        "from nltk.corpus import state_union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-8ASakJnBO_"
      },
      "outputs": [],
      "source": [
        "clinton = state_union.raw('2000-Clinton.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZupMhTbDdeq"
      },
      "outputs": [],
      "source": [
        "clinton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZhWSsi_nF_q"
      },
      "outputs": [],
      "source": [
        "# let's pick three words to search for. what do you think about their bigram probabilities?\n",
        "clinton_targets = ['americans', 'america', 'fellow']\n",
        "\n",
        "for target in clinton_targets:\n",
        "  bigram_counter(clinton, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUgi4I2ZFVN1"
      },
      "source": [
        "## **Chaining probabilities**\n",
        "Our function thus computes target bigrams for any one word and then gives us the probability of that word's occurance after a particular headword. That's kinda cool, right? We probably should calculate sentence boundaries and maybe do this for each sentence rather than a whole text, but hopefully you get the idea of how this function works now. We could next write a function which does the reverse – takes in a target word and asks for the probability of all words that comes *after* it, sort of a forward bigram counter.\n",
        "\n",
        "However, instead of doing that, let's now think about how we can use this information to predict the probability of longer string of words using bigram probabilities. We want to be able to predict the overall probability of a word after more than one word, say, to finish a complete sentence (such as the ones given at the start of this notebook). \n",
        "\n",
        "As was teased above, we can do this by chaining together the individual bigram probabilities of all the words in a target string that come before a word. This means if we wanted to compute the likelihood of the sentence `'these pretzels are making me thirsty`', we would calculate : \n",
        "\n",
        "`P(pretzels|these) * P(are|pretzels) * P(making|are) * P(me|making) * P(thirsy|me)`\n",
        "\n",
        "Can we create a function which will do this? We need to: \n",
        "\n",
        "- read in a text and tokenize it\n",
        "- accept a target phrase \n",
        "- for each bigram in the phrase, find the probability of that bigram\n",
        "- multiply the bigram probabilities to find the overall probability of the phrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di9lKW1LPT-x"
      },
      "source": [
        "# **phrase likelihood**\n",
        "\n",
        "We can create a final function to do so. We did it the hard way above (or, at least I did...). This time, we'll use the NLTK bigrams function to much more easily count bigrams and calculate their probabilities. We *could* also use the `nltk.ConditionalFreqDist`, similar to [Chapter 2, Section 2.4 in NLTK](https://www.nltk.org/book/ch02.html) but maybe this function will make things a bit more transparent because we'll manually create the distribution. And, building functions which turn out to be redundant/inferior to built-in methods is one of the joys of programming ;)\n",
        "\n",
        "\n",
        "- I'll start off by reading in a text and creating a tokenized version of the lowercased text. \n",
        "- I'll then use the `nltk.bigrams()` function to create sets of bigrams for the entire text. \n",
        "- I'll then create a dictionary containing *every* bigram in the text, and then return it (so you can see what's going on). \n",
        "\n",
        "If you read through the bigrams, you'll see we have one entry for each bigram with the bigram probability. Notice that when a bigram repeats in the text, it's absent. For example, there is no repetition of the bigram `(of, the)` between `(state, of)` and (`the, union)`, because the bigram appeared in line 8. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQWN3dlSPPPf"
      },
      "outputs": [],
      "source": [
        "def phrase_likelihood(text):\n",
        "\n",
        "  # create tokenized, lower case version of text and targets\n",
        "  text_tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "  # create bigrams for text (could combine this with above for a one-liner)\n",
        "  text_bigrams = [text_bigram for text_bigram in nltk.bigrams(text_tokens)]\n",
        " \n",
        "  # dict to store the bigram probabilities\n",
        "  bigram_pbs = dict()\n",
        "\n",
        "  # use .count() to divide bigram count by headword cout (note the slice to get just headword)\n",
        "  for bigram in text_bigrams:\n",
        "    if bigram[0].isalpha() and bigram[1].isalpha(): # let's avoid bigrams containing punctuation\n",
        "      bigram_pbs[bigram] = text_bigrams.count(bigram)/text_tokens.count(bigram[0])\n",
        "\n",
        "  return bigram_pbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6yKiBTeP-KX"
      },
      "outputs": [],
      "source": [
        "phrase_likelihood(clinton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETux7VDcwMQJ"
      },
      "source": [
        "## **chained probabilities**\n",
        "Now it is relatively straightforward to query our probability dictionary to create chained bigram probabilities from our reference corpus. I add a second argument to the function, `target`, which allows for a target phrase to be queried. \n",
        "\n",
        "I then lowercase, tokenize, and create bigrams from the target phrase. I loop through these bigrams and multiply their values to get the overall chained probability of the phrase. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJpO39DsvczR"
      },
      "outputs": [],
      "source": [
        "def phrase_likelihood(text, target):\n",
        "\n",
        "  # create tokenized, lower case version of text and targets\n",
        "  text_tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "  # create bigrams for text (could combine this with above for a one-liner)\n",
        "  text_bigrams = [text_bigram for text_bigram in nltk.bigrams(text_tokens)]\n",
        " \n",
        "  # dict to store the bigram probabilities\n",
        "  bigram_pbs = dict()\n",
        "\n",
        "  # use .count() to divide bigram count by headword cout (note the slice to get just headword)\n",
        "  for bigram in text_bigrams:\n",
        "    if bigram[0].isalpha() and bigram[1].isalpha(): # let's avoid bigrams containing punctuation\n",
        "      bigram_pbs[bigram] = text_bigrams.count(bigram)/text_tokens.count(bigram[0])\n",
        "\n",
        "  # now create bigrams of our target phrase. \n",
        "  target_tokens = nltk.word_tokenize(target.lower())\n",
        "  target_bigrams = [target_bigram for target_bigram in nltk.bigrams(target_tokens)]\n",
        "\n",
        "  # safety first\n",
        "  if target_bigrams:\n",
        "    # start the multiplication with the first probability\n",
        "    pb = bigram_pbs[target_bigrams[0]]\n",
        "    # start the loop with the second probability\n",
        "    for bigram in target_bigrams[1:]:\n",
        "      pb = pb * bigram_pbs[bigram]\n",
        "    \n",
        "    print(f'probability of phrase {target} is {pb}')\n",
        "\n",
        "  else:\n",
        "    print('ain\\'t nothing here')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xh8hL-zycnX"
      },
      "outputs": [],
      "source": [
        "phrase_likelihood(clinton, 'my fellow americans')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJbf57ELysA3"
      },
      "source": [
        "Note that the function takes a few seconds to run, since we're redoing the bigram probabilities every time. Let's separate this out and do this process *once*, then query the result with our phrase to speed up the time for eaching new phrases. \n",
        "\n",
        "I split the functions into two. `bigram_model` creates our probabilities and returns a dict.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neBIV61Ky1FK"
      },
      "outputs": [],
      "source": [
        "# make our reference model\n",
        "def bigram_model(text):\n",
        "  \"\"\"calculate probabilities for bigrams in a text\"\"\"\n",
        "  # create tokenized, lower case version of text and targets\n",
        "  text_tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "  # create bigrams for text (could combine this with above for a one-liner)\n",
        "  text_bigrams = [text_bigram for text_bigram in nltk.bigrams(text_tokens)]\n",
        " \n",
        "  # dict to store the bigram probabilities\n",
        "  bigram_pbs = dict()\n",
        "\n",
        "  # use .count() to divide bigram count by headword cout (note the slice to get just headword)\n",
        "  for bigram in text_bigrams:\n",
        "    if bigram[0].isalpha() and bigram[1].isalpha(): # let's avoid bigrams containing punctuation\n",
        "      bigram_pbs[bigram] = text_bigrams.count(bigram)/text_tokens.count(bigram[0])\n",
        "\n",
        "  return bigram_pbs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI_DvlDEzG_-"
      },
      "source": [
        "`phrase_probability` takes a new argument, `model`, which should be an ngram dict. I've also changed the dictionary call to match the argument (i.e., `model`). \n",
        "\n",
        "I've also tweaked the bigram loop to account for keys not in the model, using 0 instead. Remember what happens if we multiply by 0? We will get a total value of zero, which makes sense and would reflect that a phrase is *not* in the reference model. This of course means a model needs to be very large to be effective, much larger than a single state of the union speech. Are there other ways we might want to deal with phrases that have some missing bigrams?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTDbPoLxy64w"
      },
      "outputs": [],
      "source": [
        "def phrase_probability(model, target):\n",
        "  \"\"\"calculate chained probabilities of target phrase from reference model\"\"\"\n",
        "\n",
        "  # create bigrams of our target phrase. \n",
        "  target_tokens = nltk.word_tokenize(target.lower())\n",
        "  target_bigrams = [target_bigram for target_bigram in nltk.bigrams(target_tokens)]\n",
        "\n",
        "\n",
        "  if target_bigrams:\n",
        "    print(target_bigrams)\n",
        "    # start the multiplication with the first probability\n",
        "    pb = model.get(target_bigrams[0], 0)\n",
        "    # start the loop with the probability of the second bigram (if it exists)\n",
        "    for bigram in target_bigrams[1:]:\n",
        "      print(bigram)\n",
        "      if bigram in model:\n",
        "        pb = pb * model[bigram]\n",
        "      else:\n",
        "        pb = pb * 0\n",
        "    \n",
        "    print(f'probability of phrase {target} is {pb}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUTe6S_CzuTY"
      },
      "outputs": [],
      "source": [
        "# test our new functions out \n",
        "clinton_bigram_probs = bigram_model(clinton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8mb1qtlz5fT"
      },
      "source": [
        "Now that we've trained our model, we can query it. Holy crap..we've made an n-gram language model!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1XD57qgz32N"
      },
      "outputs": [],
      "source": [
        "phrase_probability(clinton_bigram_probs, 'my fellow americans')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ykCg89P0KXt"
      },
      "outputs": [],
      "source": [
        "phrase_probability(clinton_bigram_probs, 'my fellow romulans')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqIGbyv36qE5"
      },
      "source": [
        "I don't really recommend trying to train a larger model using brown or something like that. I tried and got bored of waiting after like 5 minutes. NPS chat is small enough to work though. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA0zJuEp6wC2"
      },
      "outputs": [],
      "source": [
        "nltk.download('book')\n",
        "from nltk.book import nps_chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M16H6Weu68O5"
      },
      "outputs": [],
      "source": [
        "nps_words = ' '.join([w for w in nps_chat.words()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdBBWppj7Do4"
      },
      "outputs": [],
      "source": [
        "# this corpus has some rather interesting language in it...\n",
        "nps_words[205:230]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhF0Ho8S65iC"
      },
      "outputs": [],
      "source": [
        "# takes about a minute to run. \n",
        "# making larger models requires more optimization and faster computers than the virtual server colab gives us.\n",
        "chat_ngrams = bigram_model(nps_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiVt86xm7w3y"
      },
      "source": [
        "**longer phrases**\n",
        "\n",
        "All right. Now, check out what happens to the probability as we expand our ngram into larger and larger phrases. The probability will decrease. \n",
        "\n",
        "You're witnessing a method used to predict upcoming text. We could now take two arguments, a phrase stem (such as 'please hand your homework...') and then find words with the highest overall phrase probability, eventually locating 'please hand your homework **in**' as the most likely combination, depending on which model we use. In this way, we could use this as a simplistic text prediction algorithm. \n",
        "\n",
        "Thinking back to some of the earlier questions I opened the notebook with about how ***you*** can predict upcoming words. One argument is that your brain contains some form of probabilistic distribution of language patterns, which is why we are so good at predicting upcoming words. Whether this computational approximation using n-grams reflects the same process can't be fully proven (yet), but as language models get larger, and the methods for computing probailities becomes more sophisticated, the performance of NLP models and predictive text applications continues to increase and match the way humans can predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNs-h4F97iTn"
      },
      "outputs": [],
      "source": [
        "phrase_probability(chat_ngrams, 'wanna chat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6RGPUS87pn7"
      },
      "outputs": [],
      "source": [
        "phrase_probability(chat_ngrams, 'wanna chat with')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA24VfiX7sX6"
      },
      "outputs": [],
      "source": [
        "phrase_probability(chat_ngrams, 'wanna chat with me')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXlJ6PpR79D7"
      },
      "outputs": [],
      "source": [
        "phrase_probability(chat_ngrams, 'go to')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YryFZpHj7_uM"
      },
      "outputs": [],
      "source": [
        "phrase_probability(chat_ngrams, 'go to the')"
      ]
    }
  ]
}