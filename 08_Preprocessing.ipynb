{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/08_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8n1A6AlehM"
      },
      "source": [
        "## The importance of preprocessing\n",
        "\n",
        "\n",
        "\n",
        "It's time to return to something we've already covered â€” tokenizing a text and defining what counts as a word. So far we've already been doing this with the `.split()` function, which has worked relatively well for us. But, there is one issue, which is that splitting on white space means that sometimes punctuation is included with our words.\n",
        "\n",
        "For example, running `.split()` on the example below will retain commas and exclamation marks as part of the words:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j2pTkAurmO82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['teenage',\n",
              " 'mutant',\n",
              " 'ninja',\n",
              " 'turtles,',\n",
              " 'teenage',\n",
              " 'mutant',\n",
              " 'ninja',\n",
              " 'turtles,',\n",
              " 'teenage',\n",
              " 'mutant',\n",
              " 'ninja',\n",
              " 'turtles,',\n",
              " 'heroes',\n",
              " 'in',\n",
              " 'a',\n",
              " 'halfshell,',\n",
              " 'turtle',\n",
              " 'power!']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "turtles = \"\"\"teenage mutant ninja turtles,\n",
        "            teenage mutant ninja turtles,\n",
        "            teenage mutant ninja turtles,\n",
        "            heroes in a halfshell, turtle power!\"\"\"\n",
        "\n",
        "turtles.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKrsYJGPYZP"
      },
      "source": [
        "Therefore, we might want to perform some operations on this text *before* we start processing it for linguistic information. These operations will work to normalize and standardize the text so that noise is removed. This is called preprocessing. Preprocessing comes in many options - you could remove just punctuation, or convert everything to lowercase, or remove very frequent words, or remove words that are not in the dictionary, or remove words that only occur one time, and so on. Different algorithms and approaches to NLP will all include their own methods and steps for preprocessing, which are tied to the goals of the analysis.\n",
        "\n",
        "For now, let's focus on the issue of punctuation in the turtles text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg5I3GW8mvk6"
      },
      "source": [
        "### Frequency and preprocessing\n",
        "\n",
        "What will happen if we run `.split()` and create a `FreqDist` from the turtles text without any preprocessing?\n",
        "\n",
        "\n",
        "Let's import the NLTK resources first...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e2vGV9dWQNlB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ],
      "source": [
        "# import the main nltk module\n",
        "import nltk\n",
        "\n",
        "# download the nltk.book resources\n",
        "nltk.download('book')\n",
        "\n",
        "# import the resources\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VLDFTXlymzSy"
      },
      "outputs": [],
      "source": [
        "# make a frequency distro of our turtles\n",
        "tfdist = nltk.FreqDist(turtles.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Khw_Q026nA4U"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we know that the world \"turtles\" occurs in the song, so why don't we see it?\n",
        "tfdist['turtles']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2ysD5hQUnHOv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# because the commas has been saved as part of the word! uhg!\n",
        "tfdist['turtles,']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qM_3Z9RnZzh"
      },
      "source": [
        "Using `.split()` clearly needs some help and might benefit from some preprocessing.\n",
        "\n",
        "Why is this important? Well, if we want to calculate the frequency of a word in a corpus / text *properly*, we have to make sure all words are on an even playing ground. Before we even get into punctuation, consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlhqdYiyoYBv"
      },
      "outputs": [],
      "source": [
        "nltk.FreqDist('Victoria University of WELLINGTON is in Wellington'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgP_6Z4poxko"
      },
      "source": [
        "Although the word \"Wellington\" occured twice in the string above, one version was in all capitals and one was not. The `FreqDist` function treated these as two separate words. Why? The answer reminds us about the way these strings are being compared by Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rBbx_g_o7n0"
      },
      "outputs": [],
      "source": [
        "# These are two different values!\n",
        "'WELLINGTON' == 'Wellington'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEjgKkd_qclu"
      },
      "source": [
        "While we know that these are basically the same word, Python doesn't care because they are *not* the same word in terms of being 100% identical values. So, we want to consider performing some initial processing (i.e., *preprocessing*) on a text before counting the words as a means to normalize or control for these properties of words we might not care about. For example, we could solve the problem above by converting all of our words to lower case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmsC7sPtqb1r"
      },
      "outputs": [],
      "source": [
        "# Hey we're the same now!\n",
        "'WELLINGTON'.lower() == 'Wellington'.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhRcumclJek"
      },
      "source": [
        "### Lexical diversity and preprocessing\n",
        "\n",
        "As another example, let's consider how pre-processing influences the effects of a measure we've already explored: lexical diversity. Compare what capitalization will do to measures of lexical diversity on these two texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dkN1UbNQlpiN"
      },
      "outputs": [],
      "source": [
        "# create two texts that only differ based on capitalization\n",
        "version1 = ['Soda', 'soda', 'Onion', 'onion']\n",
        "version2 = ['soda', 'soda', 'onion', 'onion']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GceP7bhgjqFQ"
      },
      "source": [
        "Create a lexical diversity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QOH6FT_ElzZC"
      },
      "outputs": [],
      "source": [
        "# remember how to measure ttr?\n",
        "def lexical_diversity(text):\n",
        "  ld = len(set(text))/len(text)\n",
        "  return ld"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjsP1NVRQlBr"
      },
      "source": [
        "Preprocessing leads to very different TTRs values for the \"same\" texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-fOuhioNl6wF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lexical_diversity(version1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UPwh7W-Nl8gq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lexical_diversity(version2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecwgkFxmFwi"
      },
      "source": [
        "We clearly would not want to think that `version1` is more lexically diverse than `version2`, unless we have strong reason to believe the capitalization results in a fundamentally different word.\n",
        "\n",
        "Hence, normalization is needed to address these issues.\n",
        "\n",
        "You might question this approach and wonder whether normalizing serves to remove important information about a text - perhaps capitalization matters? What if Soda is a proper name and soda is just the noun?\n",
        "\n",
        "These are important things to take into consideration when doing any sort of NLP - the scope of your research questions and the nature of the linguistic features you are interested in (and how you measure them) should drive these decisions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSFw1qyDnMn9"
      },
      "source": [
        "### Cleaning punctuation\n",
        "\n",
        "But our problem above with `turtles` was also caused by the use of punctuation and `.split()`. What could we do? Well, we *could* remove all of the punctuation before splitting the text, and this would provide a satisfactory solution (for now).\n",
        "\n",
        "Based on what we know now about Python, how could we remove all of the punctuation from a text? We can actually do this quite simply and quickly using a list comprehension.\n",
        "\n",
        "We would want to set a condition that inspects each character in a string, and as long as that character is *not* a punctuation mark, keep it.\n",
        "\n",
        "Here is some pseudocode that expresses our goal:\n",
        "\n",
        "\n",
        "```\n",
        "[character for character in string if character not punctuation]\n",
        "```\n",
        "\n",
        "To exectute this code, we'd need to tell Python what we mean by \"punctuation\". One way is to define a string containing all the puncuation marks we don't want.\n",
        "\n",
        "At the same time, we can make sure to lower case everything in the same expression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2UnRKy0vsIXH"
      },
      "outputs": [],
      "source": [
        "# define a string containing punctuation we don't like, in this case just commas and exclamation marks\n",
        "punctuation = ',!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giwzMHZCj3lh"
      },
      "source": [
        "If you run the cell below, you still see that the punctuation has been removed, but unfortunately the output is a list of characters, not words!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "voRnrvuQsM9F"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"['t', 'e', 'e', 'n', 'a', 'g', 'e', ' ', 'm', 'u', 't', 'a', 'n', 't', ' ', 'n', 'i', 'n', 'j', 'a', ' ', 't', 'u', 'r', 't', 'l', 'e', 's', '\\\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 't', 'e', 'e', 'n', 'a', 'g', 'e', ' ', 'm', 'u', 't', 'a', 'n', 't', ' ', 'n', 'i', 'n', 'j', 'a', ' ', 't', 'u', 'r', 't', 'l', 'e', 's', '\\\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 't', 'e', 'e', 'n', 'a', 'g', 'e', ' ', 'm', 'u', 't', 'a', 'n', 't', ' ', 'n', 'i', 'n', 'j', 'a', ' ', 't', 'u', 'r', 't', 'l', 'e', 's', '\\\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'h', 'e', 'r', 'o', 'e', 's', ' ', 'i', 'n', ' ', 'a', ' ', 'h', 'a', 'l', 'f', 's', 'h', 'e', 'l', 'l', ' ', 't', 'u', 'r', 't', 'l', 'e', ' ', 'p', 'o', 'w', 'e', 'r']\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# write a list comprehension that only keeps characters that aren't in punctuation\n",
        "# read on to the next section to see how to fix this output!\n",
        "[character.lower() for character in turtles if character not in punctuation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7adt3Qg4sWpc"
      },
      "source": [
        "#### `.join()`\n",
        "\n",
        "The list comprehension has returned a list of *characters*, but we wanted to retain the whitespace and other properties of the texts as a series of words. No worries, we can use the handy `string.join()` function to join a list of characters back into one string!\n",
        "\n",
        "`.join()` is sort of the bizzare cousin of `.split()`. `.join()` is actually a string method, meaning you need to attach a string to the front part of the `.join()`. The string that you attach to `.join` represents the nature of the join...the character that you want to join everything by. Much like `.split()`, you can choose whatever you like to join stuff with.\n",
        "\n",
        "But, if we simply wanted to glue back together a list of characters *without* making any other changes, we would then attach an empty string to `.join()`, indicated with two string delimiters: `''`, in which case we would type `''.join()`.\n",
        "\n",
        "Then, the thing that you want to join goes inside the `()` part of `''.join()`.\n",
        "\n",
        "```\n",
        "''.join([list of characters])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qRnGOhFxsjeJ"
      },
      "outputs": [],
      "source": [
        "# we just wrap the whole list comprehension in ''.join\n",
        "remove_punctuation = ''.join([character.lower() for character in turtles if character not in punctuation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-XV-oJJvs6A7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'teenage mutant ninja turtles\\n            teenage mutant ninja turtles\\n            teenage mutant ninja turtles\\n            heroes in a halfshell turtle power'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# it looks different now...but it's been reformed back into what we first had without punctuation\n",
        "remove_punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUA2RIaxRF8a"
      },
      "source": [
        "How else could we do this without using join?\n",
        "\n",
        "One way would be to write a loop which analyses each word in a text, removing punctuation from that word, and then puts that word into a list. This is made slightly difficult because strings are `immutable`, meaning that we cannot remove or replace individual elements of a string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JRshUaXIRZuJ"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'str' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\mingb\\OneDrive - Victoria University of Wellington - STUDENT\\200 level docs\\LING226\\Jupyter Notebook\\08_Preprocessing.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mingb/OneDrive%20-%20Victoria%20University%20of%20Wellington%20-%20STUDENT/200%20level%20docs/LING226/Jupyter%20Notebook/08_Preprocessing.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# this returns an error because we cannot modify strings in place\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mingb/OneDrive%20-%20Victoria%20University%20of%20Wellington%20-%20STUDENT/200%20level%20docs/LING226/Jupyter%20Notebook/08_Preprocessing.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m'\u001b[39;49m[\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\n",
            "\u001b[1;31mTypeError\u001b[0m: 'str' object does not support item assignment"
          ]
        }
      ],
      "source": [
        "# this returns an error because we cannot modify strings in place\n",
        "'string'[0] = 'b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53E7M730RgKU"
      },
      "source": [
        "One way to do this is scan through each character and then reconstruct the string as we go, only including characters that pass the test.\n",
        "\n",
        "String concatenation can be used for this, which is just a fancy way of saying that you can add two strings together to make a larger string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o_zzqtlRfgl"
      },
      "outputs": [],
      "source": [
        "# create an output container\n",
        "output = ''\n",
        "\n",
        "# loop through each character in the whole string\n",
        "for character in turtles:\n",
        "  # if the character is NOT in this list:\n",
        "  if character not in [',', '!']:\n",
        "    # add the lowercased version of the character to the list\n",
        "    output = output + character.lower()\n",
        "\n",
        "# results are identical to the ''.join() method above\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgfJp41EkTcQ"
      },
      "source": [
        "#### **using a regular expression**\n",
        "\n",
        "Another way, and probably the more computationally efficient way to do this, is to use a regular expression to clean the string. Regular expressions are covered in a later lesson, but it is worth looking at this preview for now.\n",
        "\n",
        "We will need to import the library for regular expressions, `re`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1mQ_oI3rkiB1"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR2JD_eKki0r"
      },
      "source": [
        "We can now use the `re.sub` function, which will substitute patterns in a string with another pattern. The syntax for `re.sub` is:\n",
        "\n",
        "`re.sub(pattern, string, replacement)`\n",
        "\n",
        "So you first type the pattern that you want to search for, then the string you want to search in, and then what you would like the pattern to be replaced with.\n",
        "\n",
        "If you say that the replacement should be an empty string, then the replacement will be nothing, meaning that you are effectively removing the pattern from the string. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xgzvRm-Ak6uh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'bnn'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove all the 'a' from the string 'banana'\n",
        "re.sub(pattern = 'a', string = 'banana', repl = '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEGiyy6HlaWp"
      },
      "source": [
        "Using this same logic, we can remove all of the punctuation from a string. Crucially, be sure to save the results as a variable, otherwise the replacements will not be saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-n32ad-qlhMn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'too! many! exclamation! points!'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# original string\n",
        "exclamation = 'too! many! exclamation! points!'\n",
        "exclamation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F8PgIKa5l1bB"
      },
      "outputs": [],
      "source": [
        "# substitute out the exclamation marks and make a new string\n",
        "exclamation = re.sub(pattern = '!', string = exclamation, repl = '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T8aR3q2Tl0wT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'too many exclamation points'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a cleaned string\n",
        "exclamation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PjvocaumDX9"
      },
      "source": [
        "Now, if we want to remove more than one punctuation mark, we can define a pattern which says \"anything in this pattern.\" To do so, write a string with brackets and put any character you want removed in those brackets, like this:\n",
        "\n",
        "```\n",
        "punctuation = [',!']\n",
        "```\n",
        "\n",
        "Then use that pattern in your `re.sub` call to replace those punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Omv808NqmZFC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'teenage mutant ninja turtles,\\n            teenage mutant ninja turtles,\\n            teenage mutant ninja turtles,\\n            heroes in a halfshell, turtle power!'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# original version of turtles\n",
        "turtles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XiNW7UcPmcGB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'teenage mutant ninja turtles\\n            teenage mutant ninja turtles\\n            teenage mutant ninja turtles\\n            heroes in a halfshell turtle power'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cleaned version of turtles (not saved to a variable)\n",
        "punctuation = '[,!]'\n",
        "re.sub(pattern = punctuation, string = turtles, repl = '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4tUgRpnuu8f"
      },
      "source": [
        "### **a cleaned FreqDist**\n",
        "Regardless of the method used to preprocess the text and remove punctuation, the resulting `FreqDist` will now look a bit different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lpUNmDsvvEIq"
      },
      "outputs": [],
      "source": [
        "# create a new frequency distribution\n",
        "cleaned_fdist = nltk.FreqDist(remove_punctuation.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SAJEBxDPXmfC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'teenage': 3, 'mutant': 3, 'ninja': 3, 'turtles': 3, 'heroes': 1, 'in': 1, 'a': 1, 'halfshell': 1, 'turtle': 1, 'power': 1})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# all the punctuation is gone, and all words are lowercased\n",
        "cleaned_fdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VnkSL40SvLE0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now we get proper results for turtles\n",
        "cleaned_fdist['turtles']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZMa2C36T9ke"
      },
      "source": [
        "## `nltk.word_tokenize()`\n",
        "\n",
        "Now we have a better way to use `.split()`, or at least knowledge that preprocessing is a necessary step for a function like `.split()`.\n",
        "\n",
        "However - what if we wanted to retain punctuation? Do you think it would be important to know the difference between words that come before / after punctuation? Could punctuation tell us something about the syntax of a sentence or the tone of voice of writing? These are questions without clear answers, but are worthy of consideration. Another more practical aspect of retaining punctuation is that punctuation markers could help with segmentation of strings into words and/or sentences. For this reason, we will actually stop using `.split()` as a means to create word tokens.\n",
        "\n",
        "Instead, we can use the NLTK segmentation functions which are improvements upon `.split()`. These function are `nltk.word_tokenize()` and `nltk.sent_tokenize()`. They convert raw strings into tokens or sentences, respectively. Let's just focus on word tokenization for now.\n",
        "\n",
        "In the cells below, compare the difference between using `.split()` and `nltk.word_tokenize()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bb9rgNMEID0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using .split(): \n",
            "['These', 'pretzels', 'are', 'making', 'me', 'thirsty!']\n",
            "\n",
            "Using nltk: \n",
            "['These', 'pretzels', 'are', 'making', 'me', 'thirsty', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# What is the difference between using `.split()` and `nltk.word_tokenize()`?\n",
        "pretzels = 'These pretzels are making me thirsty!'\n",
        "\n",
        "split_tokens = pretzels.split()\n",
        "nltk_tokens = nltk.word_tokenize(pretzels)\n",
        "\n",
        "print(f\"Using .split(): \\n{split_tokens}\\n\\nUsing nltk: \\n{nltk_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKVnaWihWB7l"
      },
      "source": [
        "The NLTK tokenizer has treated the punctuation as a separate word - so it is smart enough to recognise that words should be separated from punctuation. It does this using a set of additional rules as well as some splitting. This makes perfect sense for punctuation which occurs after words, such as commas, full stops, exclamation marks, and so on.\n",
        "\n",
        "What's going on in the cell below?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Z_sax5nWL7j"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'ca', \"n't\", 'even', '.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What is different about these tokens?\n",
        "nltk.word_tokenize('I can\\'t even.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOr4HBbHWa0u"
      },
      "source": [
        "The word \"can't\" was split into two tokens! Why is that? Well, if we think about it, \"can't\" actually stands for *two* words - \"can\" and \"not.\" The tokenizer has an additional set of rules to search these contractions and split them accordingly. Using `.split()`, on the other hand, would result in \"can't\" being stored as a single word. Moreover, removing the punctuation *before* tokenization would turn \"can't\" into \"cant\", and then `nltk.word_tokenize()` would treat \"cant\" as a single word. Is this an issue? Well, considering the word \"cant\" is its own word separate in meaning from \"can't\", it certainly could be.\n",
        "\n",
        "\n",
        "The point is that the order of pre-processing and normalisation steps is important, as are the different things you might want to do to a text. Many modern NLP libraries perform pre-processing automatically, and it is fundamental to understand how your data is being normalised in order to use these functions properly.\n",
        "\n",
        "As a general rule, using `nltk.word_tokenize()` is preferred to `.split()`, because with `word_tokenize()` you retain the punctuation as separate tokens, which you can then choose to use or not use in your analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUfuqEMg8gXC"
      },
      "source": [
        "# **Stopwords**\n",
        "\n",
        "Another form of preprocessing is to remove so-called stopwords. In English, stopwords are frequently occuring function words, such as determiners, articles, prepositions, and so on. Contrast these words with content words, such as nouns, verbs, and adjectives, and you should begin to see the difference.\n",
        "\n",
        "Sometimes, text analytic and NLP approaches remove stopwords. For example, stopwords are highly frequent and occur in most texts, so removing them can be helpful for frequency analyses. Other times, stopwords are removed to help with applications such as sentiment analysis. However, as NLP has advanced, the need to remove stopwords becomes lessened, and in fact removing stopwords can now sometimes be a detriment towards text analysis.\n",
        "\n",
        "Nonetheless, is is worthwhile to understand how to remove stopwords. The NLTK module has a list of stopwords built-in, run the cell below to see it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tl-OmYSz9aC9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mingb\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load in and inspect the stopwords resource\n",
        "import nltk\n",
        "nltk.download(['stopwords'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aKH4FEh89raX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import the entire stopwords resource\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# loop through all the the English stopwords\n",
        "[word for word in stopwords.words('english')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IauzzI1o-Cev"
      },
      "source": [
        "Have a look through the list above - you can see that there are a lot of words and pieces of words identified as stop words. You can use this list as a check to remove stopwords via a list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FxN4GNeu_GlF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Far',\n",
              " 'uncharted',\n",
              " 'backwaters',\n",
              " 'unfashionable',\n",
              " 'end',\n",
              " 'Western',\n",
              " 'Spiral',\n",
              " 'arm',\n",
              " 'galaxy',\n",
              " 'lies',\n",
              " 'small',\n",
              " 'unregarded',\n",
              " 'yellow',\n",
              " 'sun']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_of_stopwords = \"\"\"Far Out in the uncharted backwaters of the unfashionable end\n",
        "of the Western Spiral arm of the galaxy lies a small unregarded yellow sun\"\"\"\n",
        "\n",
        "# can you understand everything in the list comprehension?\n",
        "[word for word in nltk.word_tokenize(full_of_stopwords) if word.lower() not in stopwords.words('english')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO_rlch8ZaiP"
      },
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Spend some time becoming familiar with the differences between `.split()` and `nltk.word_tokenize()`.\n",
        "\n",
        "As part of your comparisons, create frequency distributions based on the results of `.split()` and `nltk.word_tokenize()` for the same strings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "myText = ' '.join(text5)\n",
        "\n",
        "split = nltk.word_tokenize(myText)\n",
        "\n",
        "#freq = [{word, 0} if freq[word] else {word, freq[word]+1} for word in split]\n",
        "#freq = [map for word in split if (map.get(word) == None)  else map.get(word) = map.get(word)+1]\n",
        "\n",
        "freq = {word: split.count(word) for word in split}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq2 = {}\n",
        "for word in split:\n",
        "    if(word not in freq2.keys()):\n",
        "        freq2[word] = 1\n",
        "    else:\n",
        "        freq2[word] = freq2[word] + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'now': 79,\n",
              " 'im': 128,\n",
              " 'left': 17,\n",
              " 'with': 152,\n",
              " 'this': 86,\n",
              " 'gay': 30,\n",
              " 'name': 27,\n",
              " ':': 343,\n",
              " 'P': 16,\n",
              " 'PART': 1016,\n",
              " 'hey': 264,\n",
              " 'everyone': 63,\n",
              " 'ah': 8,\n",
              " 'well': 81,\n",
              " 'NICK': 24,\n",
              " 'U7': 119,\n",
              " 'is': 372,\n",
              " 'a': 568,\n",
              " '.': 1435,\n",
              " 'ACTION': 346,\n",
              " 'gives': 6,\n",
              " 'U121': 36,\n",
              " 'golf': 2,\n",
              " 'clap': 3,\n",
              " ')': 938,\n",
              " 'JOIN': 1021,\n",
              " 'hi': 546,\n",
              " 'U59': 12,\n",
              " '26': 10,\n",
              " '/': 136,\n",
              " 'm': 81,\n",
              " 'ky': 2,\n",
              " 'women': 7,\n",
              " 'that': 274,\n",
              " 'are': 178,\n",
              " 'nice': 52,\n",
              " 'please': 21,\n",
              " 'pm': 109,\n",
              " 'me': 415,\n",
              " 'there': 120,\n",
              " 'ya': 101,\n",
              " 'go': 73,\n",
              " 'do': 168,\n",
              " \"n't\": 141,\n",
              " 'fuck': 15,\n",
              " 'you': 635,\n",
              " '@': 85,\n",
              " 'whats': 41,\n",
              " 'up': 160,\n",
              " 'to': 658,\n",
              " '?': 1103,\n",
              " 'i': 648,\n",
              " \"'ll\": 38,\n",
              " 'thunder': 1,\n",
              " 'your': 137,\n",
              " 'ass': 15,\n",
              " 'and': 335,\n",
              " 'dont': 75,\n",
              " 'even': 35,\n",
              " 'know': 103,\n",
              " 'what': 183,\n",
              " 'means': 5,\n",
              " 'sounds': 9,\n",
              " 'painful': 2,\n",
              " 'any': 123,\n",
              " 'ladis': 1,\n",
              " 'wan': 107,\n",
              " 'na': 144,\n",
              " 'chat': 142,\n",
              " '29': 5,\n",
              " 'my': 242,\n",
              " 'cousin': 2,\n",
              " 'drew': 2,\n",
              " 'messed': 2,\n",
              " 'pic': 26,\n",
              " 'on': 186,\n",
              " 'cast': 2,\n",
              " '24': 6,\n",
              " 'boo': 5,\n",
              " 'sexy': 23,\n",
              " 'lol': 705,\n",
              " 'U115': 36,\n",
              " 'he': 85,\n",
              " 'girl': 43,\n",
              " 'legs': 5,\n",
              " 'spread': 3,\n",
              " 'hope': 12,\n",
              " 'didnt': 28,\n",
              " 'draw': 1,\n",
              " 'penis': 1,\n",
              " 'ewwwww': 2,\n",
              " '&': 17,\n",
              " 'head': 12,\n",
              " 'between': 4,\n",
              " 'her': 71,\n",
              " 'good': 130,\n",
              " 'r': 28,\n",
              " 'u': 201,\n",
              " 'serious': 5,\n",
              " 'I': 576,\n",
              " 'take': 37,\n",
              " 'one': 87,\n",
              " ',': 690,\n",
              " 'have': 164,\n",
              " 'the': 646,\n",
              " 'docs': 1,\n",
              " 'tomorrow': 8,\n",
              " 'man': 33,\n",
              " 'am': 93,\n",
              " 'too': 125,\n",
              " '..': 406,\n",
              " 'Connected': 1,\n",
              " '...': 429,\n",
              " 'Slip': 1,\n",
              " 'away': 26,\n",
              " 'Fade': 1,\n",
              " 'Days': 2,\n",
              " 'Still': 1,\n",
              " 'feel': 19,\n",
              " 'Touching': 1,\n",
              " 'Changing': 1,\n",
              " 'Considerably': 1,\n",
              " 'killing': 6,\n",
              " 'heeeey': 1,\n",
              " '!': 1145,\n",
              " 'sharpie': 1,\n",
              " \"'re\": 38,\n",
              " 'back': 78,\n",
              " 'U129': 6,\n",
              " 'yep': 22,\n",
              " 'Any': 4,\n",
              " 'ladies': 27,\n",
              " 'hurry': 1,\n",
              " 'not': 172,\n",
              " 'fast': 7,\n",
              " 'enough': 14,\n",
              " 'U116': 6,\n",
              " 'bowl': 1,\n",
              " 'got': 80,\n",
              " 'blunt': 1,\n",
              " 'an': 58,\n",
              " 'bong': 1,\n",
              " '......': 33,\n",
              " 'glad': 4,\n",
              " 'it': 333,\n",
              " 'worked': 4,\n",
              " 'out': 108,\n",
              " 'chair': 2,\n",
              " 'hard': 10,\n",
              " 'Anyone': 10,\n",
              " 'from': 92,\n",
              " 'Tennessee': 1,\n",
              " 'in': 357,\n",
              " 'here': 181,\n",
              " 'as': 59,\n",
              " 'U68': 18,\n",
              " 'yet': 12,\n",
              " 'missing': 6,\n",
              " 'B': 2,\n",
              " 'complain': 2,\n",
              " 'about': 70,\n",
              " 'things': 10,\n",
              " 'being': 12,\n",
              " 'very': 19,\n",
              " 'often': 2,\n",
              " 'ok': 97,\n",
              " 'yes': 70,\n",
              " 'U30': 81,\n",
              " 'fire': 5,\n",
              " 'Nashville': 1,\n",
              " '``': 95,\n",
              " 'itch': 3,\n",
              " 'yo': 11,\n",
              " 'U133': 4,\n",
              " 'or': 88,\n",
              " 'ogan': 1,\n",
              " 'male': 12,\n",
              " 'show': 10,\n",
              " 'will': 40,\n",
              " 'let': 17,\n",
              " \"'s\": 208,\n",
              " 'talk': 56,\n",
              " 'haha': 44,\n",
              " 'brb': 33,\n",
              " 'opps': 2,\n",
              " 'sho': 3,\n",
              " '*': 123,\n",
              " 'keeps': 3,\n",
              " 's': 36,\n",
              " 'place': 13,\n",
              " 'warm': 5,\n",
              " 'guys': 58,\n",
              " 'cams': 1,\n",
              " 'play': 25,\n",
              " 'sits': 15,\n",
              " 'lap': 2,\n",
              " 'guyz': 3,\n",
              " 'female': 17,\n",
              " 'U126': 4,\n",
              " 'wonna': 1,\n",
              " 'f': 40,\n",
              " 'nc': 3,\n",
              " 'single': 17,\n",
              " 'mom': 5,\n",
              " 'where': 63,\n",
              " 'did': 92,\n",
              " 'gooo': 1,\n",
              " 'sure': 26,\n",
              " 'but': 90,\n",
              " 'e-bay': 1,\n",
              " 'like': 156,\n",
              " 'wrong': 13,\n",
              " 'room': 98,\n",
              " 'yeee': 1,\n",
              " 'haw': 1,\n",
              " 'considering': 1,\n",
              " 'changing': 2,\n",
              " 'nickname': 2,\n",
              " 'ihavehotnips': 1,\n",
              " 'want': 71,\n",
              " 'hot': 37,\n",
              " 'pics': 9,\n",
              " 'of': 202,\n",
              " 'can': 107,\n",
              " 'look': 26,\n",
              " 'mirror': 1,\n",
              " 'U64': 9,\n",
              " 'wb': 91,\n",
              " 'U139': 8,\n",
              " 'should': 29,\n",
              " 'U44': 28,\n",
              " 'dad': 5,\n",
              " 'ty': 69,\n",
              " 'Hi': 104,\n",
              " 'U138': 2,\n",
              " 'HAHAHA': 2,\n",
              " 'yw': 17,\n",
              " 'make': 44,\n",
              " \"'\": 52,\n",
              " 'iamahotnip': 1,\n",
              " 'alright': 4,\n",
              " 'fucking': 2,\n",
              " 'thought': 24,\n",
              " 'its': 66,\n",
              " 'so': 139,\n",
              " 'late': 5,\n",
              " 'lmao': 107,\n",
              " 'ahah': 1,\n",
              " 'iamahotniplickme': 1,\n",
              " 'ny': 8,\n",
              " 'would': 53,\n",
              " 'appearently': 1,\n",
              " 'she': 47,\n",
              " 'does': 35,\n",
              " 'cya': 5,\n",
              " 'later': 19,\n",
              " 'white': 9,\n",
              " 'ca': 24,\n",
              " 'sleep': 13,\n",
              " 'huh': 19,\n",
              " 'bye': 44,\n",
              " 'U20': 46,\n",
              " 'buy': 9,\n",
              " 'yeah': 75,\n",
              " 'U136': 3,\n",
              " 'get': 102,\n",
              " 'hell': 29,\n",
              " 'outta': 12,\n",
              " 'freaking': 5,\n",
              " 'PM': 42,\n",
              " 'box': 4,\n",
              " 'Im': 19,\n",
              " 'fiance': 8,\n",
              " 'answers': 2,\n",
              " 'for': 188,\n",
              " 'U101': 10,\n",
              " ';': 74,\n",
              " 'when': 48,\n",
              " 'U83': 8,\n",
              " 'iamahotnipwithpics': 1,\n",
              " 'plan': 2,\n",
              " 'wedding': 1,\n",
              " 'first': 22,\n",
              " 'warning': 1,\n",
              " 'uh': 8,\n",
              " 'A': 12,\n",
              " 'gold': 3,\n",
              " 'jeep': 1,\n",
              " 'charm': 1,\n",
              " 'necklace': 1,\n",
              " 'ring': 3,\n",
              " 'U6': 43,\n",
              " 'isnt': 3,\n",
              " 'falling': 1,\n",
              " 'again': 23,\n",
              " 'no': 155,\n",
              " 'right': 54,\n",
              " 'already': 16,\n",
              " 'yayayayayyy': 1,\n",
              " 'OOooOO': 1,\n",
              " 'how': 128,\n",
              " 'doin': 13,\n",
              " 'lmaoo': 4,\n",
              " 'iamahotnipwithhotnippics': 1,\n",
              " 'many': 23,\n",
              " 'kts': 1,\n",
              " 'females': 11,\n",
              " 'off': 47,\n",
              " 'really': 41,\n",
              " 'Just': 2,\n",
              " 'fine': 16,\n",
              " 'thanks': 50,\n",
              " 'freakishly': 1,\n",
              " 'dr': 3,\n",
              " 'seuss': 1,\n",
              " 'wow': 24,\n",
              " 'twice': 3,\n",
              " \"'m\": 135,\n",
              " 'impressed': 3,\n",
              " 'hiya': 78,\n",
              " 'omg': 29,\n",
              " 'Finger': 1,\n",
              " 'Deep': 1,\n",
              " 'within': 1,\n",
              " 'borderline': 1,\n",
              " 'Show': 1,\n",
              " 'love': 61,\n",
              " 'we': 63,\n",
              " 'belong': 2,\n",
              " 'together': 8,\n",
              " 'Relax': 1,\n",
              " 'Turn': 1,\n",
              " 'around': 24,\n",
              " 'And': 17,\n",
              " 'Hand': 1,\n",
              " 'just': 125,\n",
              " 'laughed': 1,\n",
              " 'hang': 5,\n",
              " 'finger': 1,\n",
              " 'yay': 6,\n",
              " 'such': 9,\n",
              " 'DIRTY': 1,\n",
              " 'word': 8,\n",
              " 'oh': 79,\n",
              " '14': 6,\n",
              " 'k': 11,\n",
              " 'U141': 3,\n",
              " 'Box': 1,\n",
              " 'hear': 21,\n",
              " 'geeshhh': 1,\n",
              " 'hes': 4,\n",
              " 'deaf': 2,\n",
              " 'ppl': 21,\n",
              " 'hug': 19,\n",
              " 'watches': 25,\n",
              " 'stuffin': 3,\n",
              " '....': 172,\n",
              " 'forward': 2,\n",
              " 'pervs': 3,\n",
              " 'hugs': 20,\n",
              " 'girls': 48,\n",
              " 'behave': 2,\n",
              " 'baby': 26,\n",
              " 'boy': 13,\n",
              " 'gottsa': 1,\n",
              " 'Only': 3,\n",
              " 'dirty': 4,\n",
              " 'minded': 1,\n",
              " 'ima': 6,\n",
              " 'aww': 11,\n",
              " 'fun': 16,\n",
              " 'forwads': 2,\n",
              " 'wrote': 3,\n",
              " 'wanted': 9,\n",
              " 'read': 14,\n",
              " 'theres': 3,\n",
              " 'nobody': 8,\n",
              " 'misbeahve': 1,\n",
              " 'happy': 9,\n",
              " '22': 7,\n",
              " 'wa': 1,\n",
              " 'him': 41,\n",
              " 'forwards': 1,\n",
              " 'ALL': 3,\n",
              " 'PErvs': 1,\n",
              " 'redirect': 1,\n",
              " 'VBox': 1,\n",
              " 'all': 165,\n",
              " 'who': 99,\n",
              " 'hate': 23,\n",
              " 'devotion': 1,\n",
              " 'christianity': 1,\n",
              " 'questioned': 1,\n",
              " 'honey': 11,\n",
              " 'Depends': 1,\n",
              " '31': 3,\n",
              " 'CA': 2,\n",
              " 'having': 11,\n",
              " 'babies': 2,\n",
              " 'laughs': 2,\n",
              " 'his': 38,\n",
              " 'handle': 5,\n",
              " 'On': 3,\n",
              " 'kinda': 8,\n",
              " 'lookin': 4,\n",
              " 'U92': 14,\n",
              " 'little': 9,\n",
              " 'jerks': 2,\n",
              " 'jerkettes': 2,\n",
              " 'jesus': 8,\n",
              " 'more': 34,\n",
              " 'than': 12,\n",
              " 'ANYONE': 2,\n",
              " 'ELSE': 1,\n",
              " 'lolololll': 1,\n",
              " 'serg': 1,\n",
              " 'beeehave': 1,\n",
              " 'loves': 7,\n",
              " 'ME': 8,\n",
              " 'most': 6,\n",
              " 'myself': 10,\n",
              " 'CO': 2,\n",
              " 'if': 98,\n",
              " 'didnts': 1,\n",
              " 'say': 34,\n",
              " 'anithing': 1,\n",
              " 'see': 75,\n",
              " 'eyes': 15,\n",
              " '=': 33,\n",
              " 'Jerketts': 1,\n",
              " 'wtf': 7,\n",
              " 'yah': 5,\n",
              " 'jerk': 4,\n",
              " 'kids': 11,\n",
              " 'awwwwww': 2,\n",
              " 'type': 9,\n",
              " 'much': 40,\n",
              " 'shut': 14,\n",
              " 'massage': 3,\n",
              " 'daughters': 2,\n",
              " 'ur': 21,\n",
              " 'annoying': 5,\n",
              " 'Now': 9,\n",
              " 'Playing': 1,\n",
              " '-': 174,\n",
              " 'Cradle': 1,\n",
              " 'Filth': 1,\n",
              " 'Gothic': 1,\n",
              " 'Romance': 1,\n",
              " '(': 722,\n",
              " 'Red': 1,\n",
              " 'Roses': 2,\n",
              " 'For': 1,\n",
              " 'The': 13,\n",
              " 'Devil': 1,\n",
              " 'Whore': 1,\n",
              " 'gettign': 1,\n",
              " 'cooler': 2,\n",
              " 'by': 25,\n",
              " 'minute': 6,\n",
              " \"'d\": 26,\n",
              " 'miss': 5,\n",
              " 'noo': 1,\n",
              " 'work': 38,\n",
              " 'why': 59,\n",
              " 'heyy': 8,\n",
              " 'U148': 3,\n",
              " 'boys': 5,\n",
              " 'naughtier': 1,\n",
              " 'Hmm': 2,\n",
              " 'Guess': 1,\n",
              " 'ugly': 4,\n",
              " 'had': 50,\n",
              " 'daughter': 4,\n",
              " 'regret': 1,\n",
              " 'bein': 1,\n",
              " 'What': 18,\n",
              " 'aw': 5,\n",
              " 'whys': 1,\n",
              " 'deep': 2,\n",
              " 'inside': 5,\n",
              " 'wants': 21,\n",
              " 'hates': 2,\n",
              " 'ahahah': 1,\n",
              " 'id': 4,\n",
              " 'put': 18,\n",
              " 'caution': 1,\n",
              " 'tape': 1,\n",
              " 'Your': 3,\n",
              " 'phone': 20,\n",
              " 'sexs': 1,\n",
              " 'best': 9,\n",
              " '--': 50,\n",
              " '>': 72,\n",
              " '27': 14,\n",
              " 'M': 5,\n",
              " 'UK': 1,\n",
              " 'profile': 9,\n",
              " 'Lmfao': 3,\n",
              " 'phil': 2,\n",
              " 'said': 23,\n",
              " 'wouldnt': 5,\n",
              " 'date': 4,\n",
              " 'Is': 7,\n",
              " '=-\\\\': 1,\n",
              " 'bad': 26,\n",
              " 'try': 16,\n",
              " 'arrange': 1,\n",
              " 'words': 6,\n",
              " 'they': 77,\n",
              " 'form': 1,\n",
              " 'coherent': 1,\n",
              " 'thoughts': 1,\n",
              " 'corner': 12,\n",
              " 'hello': 71,\n",
              " 'U84': 3,\n",
              " 'hmph': 2,\n",
              " 'mean': 19,\n",
              " 'has': 42,\n",
              " 'been': 57,\n",
              " 'grammar': 1,\n",
              " 'lesson': 1,\n",
              " 'day': 33,\n",
              " 'courtesy': 1,\n",
              " 'pokes': 5,\n",
              " 'still': 33,\n",
              " 'could': 20,\n",
              " 'counter': 1,\n",
              " 'wait': 14,\n",
              " 'sheesh': 3,\n",
              " 'perv': 11,\n",
              " 'guy': 23,\n",
              " 'lets': 15,\n",
              " 'course': 8,\n",
              " 'woohoo': 4,\n",
              " 'think': 54,\n",
              " 'exception': 2,\n",
              " 'rule': 7,\n",
              " 'land': 5,\n",
              " 'though': 14,\n",
              " 'wont': 11,\n",
              " 'bug': 2,\n",
              " 'em': 14,\n",
              " 'then': 40,\n",
              " 'someone': 28,\n",
              " 'find': 18,\n",
              " 'cheap': 4,\n",
              " 'flight': 2,\n",
              " 'spain': 1,\n",
              " 'need': 43,\n",
              " 'summer': 7,\n",
              " 'HUGE': 1,\n",
              " 'seriously': 13,\n",
              " 'was': 142,\n",
              " 'dumbass': 2,\n",
              " 'signed': 1,\n",
              " 'kev': 1,\n",
              " 'fetterline': 1,\n",
              " 'rap': 1,\n",
              " 'deal': 3,\n",
              " 'emergency': 1,\n",
              " 'Ug': 1,\n",
              " 'shit': 17,\n",
              " 'il': 7,\n",
              " 'bbl': 6,\n",
              " 'maybe': 20,\n",
              " 'LOL': 87,\n",
              " 'hahah': 10,\n",
              " '8082653953': 1,\n",
              " 'K-Fed': 1,\n",
              " 'kicked': 8,\n",
              " 'Lol': 10,\n",
              " 'class': 3,\n",
              " 'ticket': 1,\n",
              " 'texas': 6,\n",
              " 'anyone': 50,\n",
              " 'be': 107,\n",
              " 'bought': 1,\n",
              " 'enuff': 2,\n",
              " 'money': 2,\n",
              " 'webcams': 1,\n",
              " 're-thinks': 1,\n",
              " 'liking': 1,\n",
              " 'rethinks': 1,\n",
              " 'lmaoooo': 1,\n",
              " 'Aw': 1,\n",
              " 'U155': 2,\n",
              " 'only': 33,\n",
              " 'kidding': 12,\n",
              " 'douchebag': 1,\n",
              " 'Poor': 2,\n",
              " 'bored': 20,\n",
              " 'pick': 13,\n",
              " 'choose': 2,\n",
              " 're-thinking': 1,\n",
              " 're-think': 1,\n",
              " 'nothing': 18,\n",
              " 'meredith': 1,\n",
              " 'impression': 1,\n",
              " 'Cum': 1,\n",
              " 'shower': 6,\n",
              " 'U1370': 2,\n",
              " 'nads': 1,\n",
              " 'asl': 9,\n",
              " 'lot': 7,\n",
              " 'funny': 9,\n",
              " 'You': 42,\n",
              " 'friends': 5,\n",
              " 'nose': 5,\n",
              " 'But': 5,\n",
              " 'friend': 8,\n",
              " 'sticks': 1,\n",
              " 'fingers': 10,\n",
              " 'face': 11,\n",
              " 'lmaooo': 4,\n",
              " 'nad': 1,\n",
              " 'stick': 8,\n",
              " 'U23': 35,\n",
              " 'ewwww': 2,\n",
              " 'sniffs': 1,\n",
              " 'ewwwwww': 1,\n",
              " 'owww': 1,\n",
              " 'splinter': 1,\n",
              " 'licks': 3,\n",
              " 'ear': 3,\n",
              " 'ughhh': 1,\n",
              " 'charge': 4,\n",
              " 'IL': 1,\n",
              " 'gagas': 1,\n",
              " 'hangs': 1,\n",
              " 'number': 6,\n",
              " 'gags': 1,\n",
              " 'neck': 9,\n",
              " 'bites': 2,\n",
              " 'lip': 2,\n",
              " 'Meep': 1,\n",
              " 'resisting': 1,\n",
              " 'beeeeehave': 1,\n",
              " 'Remember': 1,\n",
              " 'LAst': 1,\n",
              " 'time': 50,\n",
              " 'smirks': 1,\n",
              " 'innocently': 1,\n",
              " 'wash': 3,\n",
              " 'hands': 14,\n",
              " 'dude': 7,\n",
              " 'gets': 12,\n",
              " 'jk': 7,\n",
              " '1-900-anal-sex': 1,\n",
              " 'dang': 9,\n",
              " 'predicted': 1,\n",
              " '1.99': 1,\n",
              " 'min': 5,\n",
              " 'innocent': 9,\n",
              " 'LOLOLOLLL': 1,\n",
              " 'U12': 30,\n",
              " 'thats': 45,\n",
              " 'asshole': 2,\n",
              " 'mouth': 4,\n",
              " 'erm': 1,\n",
              " 'free': 13,\n",
              " 'goes': 14,\n",
              " '-o': 12,\n",
              " 'Lies': 2,\n",
              " 'Check': 1,\n",
              " 'record': 1,\n",
              " 'lick': 5,\n",
              " 'old': 21,\n",
              " 'Way': 2,\n",
              " 'lie': 4,\n",
              " 'mmhmm': 1,\n",
              " 'polite': 1,\n",
              " 'calls': 2,\n",
              " 'may': 14,\n",
              " 'om': 2,\n",
              " 'y': 9,\n",
              " 'lix': 2,\n",
              " 'ummm': 4,\n",
              " 'U109': 3,\n",
              " 'cyber': 7,\n",
              " 'darlin': 4,\n",
              " 'youre': 3,\n",
              " 'dead': 9,\n",
              " 'Death': 2,\n",
              " 'Row': 1,\n",
              " 'call': 26,\n",
              " 'case': 8,\n",
              " 'hahaha': 14,\n",
              " 'catch': 1,\n",
              " 'covers': 2,\n",
              " 'mexican': 2,\n",
              " 'searching': 1,\n",
              " 'else': 13,\n",
              " 'darling': 3,\n",
              " 'bite': 2,\n",
              " 'ooo': 2,\n",
              " 'going': 27,\n",
              " 'bed': 8,\n",
              " 'dreams': 3,\n",
              " 'wishes': 3,\n",
              " 'cause': 13,\n",
              " 'incoming': 1,\n",
              " 'mins': 2,\n",
              " 'ditto': 2,\n",
              " 'nite': 17,\n",
              " 'lool': 1,\n",
              " 'kina': 1,\n",
              " 'give': 19,\n",
              " 'oo': 1,\n",
              " 'ooooook': 1,\n",
              " 'idiots': 4,\n",
              " 'ahead': 1,\n",
              " 'were': 38,\n",
              " 'early20s': 1,\n",
              " 'tears': 1,\n",
              " 'partial': 1,\n",
              " '#': 83,\n",
              " '30': 8,\n",
              " 'sext': 1,\n",
              " 'urself': 2,\n",
              " 'piff': 3,\n",
              " 'shows': 1,\n",
              " 'sexist': 1,\n",
              " 'student': 2,\n",
              " 'lazy': 1,\n",
              " 'profiles': 5,\n",
              " 'conceited': 1,\n",
              " 'busy': 17,\n",
              " 'peolpe': 1,\n",
              " 'either': 9,\n",
              " 'construction': 3,\n",
              " 'okay': 7,\n",
              " 'bit': 12,\n",
              " 'calm': 1,\n",
              " 'down': 24,\n",
              " 'crosses': 2,\n",
              " 'arms': 1,\n",
              " 'geeshh': 2,\n",
              " 'two': 10,\n",
              " 'fighting': 3,\n",
              " 'major': 1,\n",
              " 'probably': 4,\n",
              " 'sucks': 18,\n",
              " 'saw': 10,\n",
              " 'gon': 37,\n",
              " 'dream': 3,\n",
              " 'eats': 2,\n",
              " 'chococake': 2,\n",
              " 'yum': 3,\n",
              " 'child': 1,\n",
              " 'development': 1,\n",
              " 'cho-co-la-te': 1,\n",
              " 'near': 13,\n",
              " 'kc': 1,\n",
              " 'cheaking': 1,\n",
              " 'taken': 3,\n",
              " 'CHOCO': 2,\n",
              " 'night': 41,\n",
              " 'smax': 1,\n",
              " 'It': 21,\n",
              " 'three': 4,\n",
              " 'syllables': 1,\n",
              " 'cho-co-late': 1,\n",
              " 'hehehe': 7,\n",
              " 'sowwy': 5,\n",
              " 'VVil': 1,\n",
              " 'U36': 27,\n",
              " 'kisses': 9,\n",
              " 'Mmm': 1,\n",
              " 'PayPal': 1,\n",
              " 'credit': 2,\n",
              " 'alert': 1,\n",
              " 'hep': 1,\n",
              " 'against': 5,\n",
              " 'identity': 1,\n",
              " 'theft': 1,\n",
              " 'cold': 12,\n",
              " 'winter': 3,\n",
              " 'new': 20,\n",
              " 'cell': 2,\n",
              " 'chocolate': 2,\n",
              " 'smooch': 5,\n",
              " 'walks': 7,\n",
              " 'runs': 9,\n",
              " 'their': 19,\n",
              " 'thru': 6,\n",
              " 'hair': 17,\n",
              " 'closes': 6,\n",
              " 'gently': 5,\n",
              " 'Compliments': 6,\n",
              " 'wat': 2,\n",
              " 'paypal': 1,\n",
              " 'typed': 1,\n",
              " 'choco': 1,\n",
              " 'caps': 5,\n",
              " 'umm': 4,\n",
              " 'pounce': 2,\n",
              " 'U165': 6,\n",
              " 'jump': 3,\n",
              " 'mmm': 4,\n",
              " 'missed': 11,\n",
              " 'spank': 2,\n",
              " 'supposed': 5,\n",
              " 'over': 40,\n",
              " '20s': 1,\n",
              " 'pay': 5,\n",
              " 'makes': 13,\n",
              " 'better': 23,\n",
              " 'red': 15,\n",
              " 'color': 6,\n",
              " 'eye': 3,\n",
              " 'video': 7,\n",
              " 'tapes': 1,\n",
              " 'blind': 3,\n",
              " 'este': 1,\n",
              " 'dat': 5,\n",
              " 'chase': 2,\n",
              " 'people': 41,\n",
              " 'watch': 12,\n",
              " 'wana': 8,\n",
              " 'chik': 1,\n",
              " 'aussie': 1,\n",
              " 'supervisor': 1,\n",
              " 'Hello': 18,\n",
              " 'ignoring': 1,\n",
              " 'list': 6,\n",
              " 'wish': 5,\n",
              " 'cmon': 2,\n",
              " 'U128': 5,\n",
              " 'special': 4,\n",
              " 'cuz': 6,\n",
              " 'ignored': 2,\n",
              " 'untouchable': 1,\n",
              " 'hehe': 12,\n",
              " 'hows': 8,\n",
              " 'bout': 12,\n",
              " 'knows': 7,\n",
              " '2': 31,\n",
              " 'lists': 3,\n",
              " 'London': 1,\n",
              " 'enjoys': 1,\n",
              " 'roleplay': 1,\n",
              " 'whisper': 8,\n",
              " 'wats': 2,\n",
              " 'great': 20,\n",
              " 'aint': 9,\n",
              " 'tellin': 1,\n",
              " 'illin': 1,\n",
              " 'U11': 58,\n",
              " 'U9': 49,\n",
              " 'thank': 14,\n",
              " 'vm': 1,\n",
              " 'tv': 8,\n",
              " 'noise': 4,\n",
              " 'stupid': 3,\n",
              " 'political': 1,\n",
              " 'ads': 2,\n",
              " 'grief': 1,\n",
              " 'U15': 33,\n",
              " 'lost': 17,\n",
              " 'connection': 2,\n",
              " 'youer': 1,\n",
              " 'election': 2,\n",
              " 'season': 1,\n",
              " 'scorpion': 1,\n",
              " 'always': 25,\n",
              " 'yours': 4,\n",
              " 'same': 19,\n",
              " 'today': 29,\n",
              " 'Uhh': 1,\n",
              " 'Ahh': 2,\n",
              " '..............................': 1,\n",
              " 'Boyz': 1,\n",
              " 'II': 1,\n",
              " 'Men': 1,\n",
              " 'scorpions': 1,\n",
              " 'rock': 9,\n",
              " 'job': 5,\n",
              " 'went': 8,\n",
              " 'manhattan': 1,\n",
              " 'boght': 1,\n",
              " 'some': 58,\n",
              " 'stuff': 11,\n",
              " 'came': 5,\n",
              " 'home': 11,\n",
              " 'vindictive': 1,\n",
              " 'earrings': 1,\n",
              " 'coat': 1,\n",
              " 'U2': 16,\n",
              " 'horror': 1,\n",
              " 'scopes': 1,\n",
              " 'horrified': 1,\n",
              " 'byb': 1,\n",
              " 'Hey': 25,\n",
              " 'welcome': 35,\n",
              " 'U17': 40,\n",
              " 'flavour': 2,\n",
              " 'intersting': 1,\n",
              " 'blueberry': 1,\n",
              " 'martini': 2,\n",
              " 'liked': 6,\n",
              " 'laterssss': 1,\n",
              " 'kind': 2,\n",
              " 'consultant': 2,\n",
              " 'financial': 1,\n",
              " 'byeeee': 1,\n",
              " 'eat': 15,\n",
              " 'dinner': 5,\n",
              " 'biz': 2,\n",
              " 'Looking': 1,\n",
              " 'Through': 1,\n",
              " 'Patient': 1,\n",
              " 'Eyes': 1,\n",
              " '.....': 73,\n",
              " 'Dawn': 1,\n",
              " 'last': 22,\n",
              " 'song': 36,\n",
              " 'evening': 13,\n",
              " 'LIVE': 1,\n",
              " 'cool': 24,\n",
              " 'mauh': 1,\n",
              " 'mike': 2,\n",
              " 'keep': 14,\n",
              " 'makin': 1,\n",
              " '$': 6,\n",
              " 'others': 10,\n",
              " 'must': 19,\n",
              " 'sup': 9,\n",
              " 'seem': 6,\n",
              " 'stocks': 1,\n",
              " 'U4': 7,\n",
              " 'own': 17,\n",
              " 'advice': 2,\n",
              " 'client': 1,\n",
              " 'suffer': 1,\n",
              " 'days': 9,\n",
              " 'after': 8,\n",
              " 'finish': 3,\n",
              " 'meeting': 1,\n",
              " 'clients': 1,\n",
              " 'U18': 92,\n",
              " 'mango': 2,\n",
              " 'fav': 1,\n",
              " 'shaken': 1,\n",
              " 'stirred': 1,\n",
              " 'U19': 79,\n",
              " 'ques': 1,\n",
              " 'shakin': 2,\n",
              " 'strong': 3,\n",
              " 'strait': 1,\n",
              " 'way': 20,\n",
              " '<': 128,\n",
              " 'sorry': 28,\n",
              " 'drink': 7,\n",
              " 'smoker': 1,\n",
              " 'trying': 4,\n",
              " 'quit': 4,\n",
              " 'preference': 1,\n",
              " '4.20': 1,\n",
              " 'at': 98,\n",
              " 'house': 5,\n",
              " 'mine': 9,\n",
              " 'shook': 1,\n",
              " 'ice': 6,\n",
              " 'mad': 4,\n",
              " 'cali': 5,\n",
              " 'farmer': 1,\n",
              " 'hmmmmmmm': 2,\n",
              " '\\\\': 5,\n",
              " 'harvest': 1,\n",
              " 'year': 11,\n",
              " 'U10': 8,\n",
              " 'whoa': 2,\n",
              " 'hb': 9,\n",
              " 'teenagers': 2,\n",
              " 'finally': 3,\n",
              " 'ha': 14,\n",
              " '18': 38,\n",
              " '16': 6,\n",
              " 'latest': 1,\n",
              " 'gosh': 1,\n",
              " '8': 4,\n",
              " '6': 16,\n",
              " 'ruff': 1,\n",
              " 'talkin': 12,\n",
              " 'tonight': 20,\n",
              " 'choice': 4,\n",
              " 'martinis': 1,\n",
              " 'teens': 3,\n",
              " 'married': 5,\n",
              " 'sex': 10,\n",
              " 'drugs': 5,\n",
              " 'n': 21,\n",
              " 'roll': 5,\n",
              " 'wooooohoooo': 1,\n",
              " 'answer': 7,\n",
              " 'nope': 13,\n",
              " 'rest': 4,\n",
              " 'definitley': 1,\n",
              " 'yeppers': 4,\n",
              " ...}"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNZzuv1GRlf8N4bAkLtlFo4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
