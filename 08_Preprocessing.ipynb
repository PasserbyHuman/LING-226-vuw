{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNifmkjC30aXsnJra/01bG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/08_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The importance of pre-processing\n",
        "\n",
        "\n",
        "\n",
        "It's time to return to something we've already covered — tokenizing a text and defining what counts as a word. So far we've already been doing this with the `.split()` function, which has worked relatively well for us. But, there is one issue, which is that splitting on white space means that sometimes punctuation is included with our words.\n",
        "\n",
        "For example, running `.split()` on the example below will retain commas and exclamation marks as part of the words:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ee8n1A6AlehM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turtles = \"\"\"teenage mutant ninja turtles,\n",
        "            teenage mutant ninja turtles,\n",
        "            teenage mutant ninja turtles,\n",
        "            heroes in a halfshell, turtle power!\"\"\"\n",
        "\n",
        "turtles.split()"
      ],
      "metadata": {
        "id": "j2pTkAurmO82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, we might want to perform some operations on this text *before* we start processing it for linguistic information. These operations will work to normalize and standardize the text so that noise is removed. This is called preprocessing. Preprocessing comes in many options - you could remove just punctuation, or convert everything to lowercase, or remove very frequent words, or remove words that are not in the dictionary, or remove words that only occur one time, and so on. Different algorithms and approaches to NLP will all include their own methods and steps for preprocessing, which are tied to the goals of the analysis.\n",
        "\n",
        "For now, let's focus on the issue of punctuation in the turtles text."
      ],
      "metadata": {
        "id": "2AKrsYJGPYZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency and pre-processing\n",
        "\n",
        "What will happen if we run `.split()` and create a `FreqDist` from the turtles text without any preprocessing?\n",
        "\n",
        "\n",
        "Let's import the NLTK resources first...\n"
      ],
      "metadata": {
        "id": "tg5I3GW8mvk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the main nltk module\n",
        "import nltk\n",
        "\n",
        "# download the nltk.book resources\n",
        "nltk.download('book')\n",
        "\n",
        "# import the resources\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "e2vGV9dWQNlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a frequency distro of our turtles\n",
        "tfdist = nltk.FreqDist(turtles.split())"
      ],
      "metadata": {
        "id": "VLDFTXlymzSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we know that the world \"turtles\" occurs in the song, so why don't we see it?\n",
        "tfdist['turtles']"
      ],
      "metadata": {
        "id": "Khw_Q026nA4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# because the commas has been saved as part of the word! uhg!\n",
        "tfdist['turtles,']"
      ],
      "metadata": {
        "id": "2ysD5hQUnHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `.split()` clearly needs some help and introduces a fundamental topic in NLP and corpus linguistics — preprocessing or normalising a text.\n",
        "\n",
        "Why is this important? Well, if we want to calculate the frequency of a word in a corpus / text *properly*, we have to make sure all words are on an even playing ground. Before we even get into punctuation, consider the following:"
      ],
      "metadata": {
        "id": "3qM_3Z9RnZzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.FreqDist('Victoria University of WELLINGTON is in Wellington'.split())"
      ],
      "metadata": {
        "id": "HlhqdYiyoYBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the word \"Wellington\" occured twice in the string above, one version was in all capitals and one was not. The `FreqDist` function treated these as two separate words. Why? The answer reminds us about the way these strings are being compared by Python:"
      ],
      "metadata": {
        "id": "GgP_6Z4poxko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are two different values!\n",
        "'WELLINGTON' == 'Wellington'"
      ],
      "metadata": {
        "id": "8rBbx_g_o7n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we know that these are basically the same word, Python doesn't care because they are *not* the same word in terms of being 100% identical values. So, we want to consider performing some initial processing (i.e., *preprocessing*) on a text before counting the words as a means to normalize or control for these properties of words we might not care about. For example, we could solve the problem above by converting all of our words to lower case."
      ],
      "metadata": {
        "id": "xEjgKkd_qclu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hey we're the same now!\n",
        "'WELLINGTON'.lower() == 'Wellington'.lower()"
      ],
      "metadata": {
        "id": "LmsC7sPtqb1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhRcumclJek"
      },
      "source": [
        "### Lexical diversity and pre-processing.\n",
        "\n",
        "As another example, let's consider how pre-processing influences the effects of a measure we've already explored: lexical diversity. Compare what capitalization will do to measures of lexical diversity on these two texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkN1UbNQlpiN"
      },
      "source": [
        "# create two texts that only differ based on capitalization\n",
        "version1 = ['Soda', 'soda', 'Onion', 'onion']\n",
        "version2 = ['soda', 'soda', 'onion', 'onion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOH6FT_ElzZC"
      },
      "source": [
        "# remember how to measure ttr?\n",
        "def lexical_diversity(text):\n",
        "  ld = len(set(text))/len(text)\n",
        "  return ld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing leads to very different TTRs values for the \"same\" texts."
      ],
      "metadata": {
        "id": "AjsP1NVRQlBr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fOuhioNl6wF"
      },
      "source": [
        "lexical_diversity(version1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPwh7W-Nl8gq"
      },
      "source": [
        "lexical_diversity(version2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecwgkFxmFwi"
      },
      "source": [
        "We clearly would not want to think that `version1` is more lexically diverse than `version2`, unless we have strong reason to believe the capitalization results in a fundamentally different word.\n",
        "\n",
        "Hence, normalization is needed to address these issues.\n",
        "\n",
        "You might question this approach and wonder whether normalizing serves to remove important information about a text - perhaps capitalization matters? What if Soda is a proper name and soda is just the noun?\n",
        "\n",
        "These are important things to take into consideration when doing any sort of NLP - the scope of your research questions and the nature of the linguistic features you are interested in (and how you measure them) should drive these decisions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning punctuation\n",
        "\n",
        "But our problem above with `turtles` was also caused by the use of punctuation and `.split()`. What could we do? Well, we *could* remove all of the punctuation before splitting the text, and this would provide a satisfactory solution (for now).\n",
        "\n",
        "Based on what we know now about Python, how could we remove all of the punctuation from a text? We can actually do this quite simply and quickly using a list comprehension.\n",
        "\n",
        "We would want to set a condition that inspects each character in a string, and as long as that character is *not* a punctuation mark, keep it.\n",
        "\n",
        "Here is some pseudocode that expresses our goal:\n",
        "\n",
        "\n",
        "```\n",
        "[character for character in string if character not punctuation]\n",
        "```\n",
        "\n",
        "To exectute this code, we'd need to tell Python what we mean by \"punctuation\". One way is to define a string containing all the puncuation marks we don't want.\n",
        "\n",
        "At the same time, we can make sure to lower case everything in the same expression.\n"
      ],
      "metadata": {
        "id": "ZSFw1qyDnMn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation we don't like, in this case just commas and exclamation marks\n",
        "punctuation = ',!'"
      ],
      "metadata": {
        "id": "2UnRKy0vsIXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a list comprehension that only keeps characters that aren't in punctuation\n",
        "# read on to the next section to see how to fix this output!\n",
        "[character.lower() for character in turtles if character not in punctuation]"
      ],
      "metadata": {
        "id": "voRnrvuQsM9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `.join()`\n",
        "\n",
        "Hrmm, not quite what we wanted, because the list comprehension has returned a list of *characters*, but we wanted to retain the whitespace and other properties of the texts. No worries, we can use the handy `.join()` function to join a list of characters back into one string!\n",
        "\n",
        "`.join` is sort of the bizzare cousin of `.split()`. `.join` is actually a string method, meaning you need to attach a string to the front part of the `.join()`. The string that you attach to `.join` represents the nature of the join...the character that you want to join everything by. Much like `.split()`, you can choose whatever you like to join stuff with.\n",
        "\n",
        "But, if we simply wanted to glue back together a list of characters *without* making any other changes, we would then attach an empty string to `.join()`, indicated with two string delimiters: `''`, in which case we would type `''.join()`.\n",
        "\n",
        "Then, the thing that you want to join goes inside the `()` part of `''.join()`.\n",
        "\n",
        "```\n",
        "''.join([list of characters])\n",
        "```\n"
      ],
      "metadata": {
        "id": "7adt3Qg4sWpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we just wrap the whole list comprehension in ''.join\n",
        "remove_punctuation = ''.join([character.lower() for character in turtles if character not in punctuation])"
      ],
      "metadata": {
        "id": "qRnGOhFxsjeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it looks different now...but it's been reformed back into what we first had without punctuation\n",
        "remove_punctuation"
      ],
      "metadata": {
        "id": "-XV-oJJvs6A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How else could we do this without using join?\n",
        "\n",
        "One way would be to write a loop which analyses each word in a text, removing punctuation from that word, and then puts that word into a list. This is made slightly difficult because strings are `immutable`, meaning that we cannot remove or replace individual elements of a string.\n"
      ],
      "metadata": {
        "id": "tUA2RIaxRF8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this returns an error because we cannot modify strings in place\n",
        "'string'[0] = 'b'"
      ],
      "metadata": {
        "id": "JRshUaXIRZuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to do this is scan through each character and then reconstruct the string as we go, only including characters that pass the test.\n",
        "\n",
        "String concatenation can be used for this, which is just a fancy way of saying that you can add two strings together to make a larger string.\n",
        "\n"
      ],
      "metadata": {
        "id": "53E7M730RgKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an output container\n",
        "output = ''\n",
        "\n",
        "# loop through each character in the whole string\n",
        "for character in turtles:\n",
        "  # if the character is NOT in this list:\n",
        "  if character not in [',', '!']:\n",
        "    # add the lowercased version of the character to the list\n",
        "    output = output + character.lower()\n",
        "\n",
        "# results are identical to the ''.join() method above\n",
        "output"
      ],
      "metadata": {
        "id": "3o_zzqtlRfgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Regardless of the method used to preprocess the text, the resulting `FreqDist` will now look a bit different."
      ],
      "metadata": {
        "id": "b4tUgRpnuu8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new frequency distribution\n",
        "cleaned_fdist = nltk.FreqDist(remove_punctuation.split())"
      ],
      "metadata": {
        "id": "lpUNmDsvvEIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all the punctuation is gone, and all words are lowercased\n",
        "cleaned_fdist"
      ],
      "metadata": {
        "id": "SAJEBxDPXmfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we get proper results for turtles\n",
        "cleaned_fdist['turtles']"
      ],
      "metadata": {
        "id": "VnkSL40SvLE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nltk.word_tokenize()`\n",
        "\n",
        "Now we have a better way to use `.split()` and understand some potential solutions for the way that punctuation may interfere with our definition of words.\n",
        "\n",
        "However - what if we wanted to retain punctuation? Do you think it would be important to know the difference between words that come before / after punctuation? Could punctuation tell us something about the syntax of a sentence or the tone of voice of writing? These are questions without clear answers, but are worthy of consideration. Another more practical aspect of retaining punctuation is that punctuation markers could help with segmentation of strings into words and/or sentences. For this reason, using `.split()` can use some help.\n",
        "\n",
        "NLTK has two built-in segmentation functions which are improvements upon using `.split()`. These function are `nltk.word_tokenize()` and `nltk.sent_tokenize()`. They convert raw strings into tokens or sentences, respectively. Let's just focus on word tokenization for now.\n",
        "\n",
        "In the cells below, compare the difference between using `.split()` and `nltk.word_tokenize()`:"
      ],
      "metadata": {
        "id": "JZMa2C36T9ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the difference between using `.split()` and `nltk.word_tokenize()`?\n",
        "pretzels = 'These pretzels are making me thirsty!'\n",
        "\n",
        "split_tokens = pretzels.split()\n",
        "nltk_tokens = nltk.word_tokenize(pretzels)\n",
        "\n",
        "print(f\"Using .split(): \\n{split_tokens}\\n\\nUsing nltk: \\n{nltk_tokens}\")"
      ],
      "metadata": {
        "id": "bb9rgNMEID0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK tokenizer has treated the punctuation as a separate word - so it is smart enough to recognise that words should be separated from punctuation. It does this using a set of additional rules as well as some splitting. This makes perfect sense for punctuation which occurs after words, such as commas, full stops, exclamation marks, and so on.\n",
        "\n",
        "What's going on in the cell below?"
      ],
      "metadata": {
        "id": "dKVnaWihWB7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is different about these tokens?\n",
        "nltk.word_tokenize('I can\\'t even.')"
      ],
      "metadata": {
        "id": "7Z_sax5nWL7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word \"can't\" was split into two tokens! Why is that? Well, if we think about it, \"can't\" actually stands for *two* words - \"can\" and \"not.\" The tokenizer has an additional set of rules to search these contractions and split them accordingly. Using `.split()`, on the other hand, would result in \"can't\" being stored as a single word. Moreover, removing the punctuation *before* tokenization would turn \"can't\" into \"cant\", and then `nltk.word_tokenize()` would treat \"cant\" as a single word. Is this an issue? Well, considering the word \"cant\" is its own word separate in meaning from \"can't\", it certainly could be.\n",
        "\n",
        "\n",
        "The point is that the order of pre-processing and normalisation steps is important, as are the different things you might want to do to a text. Many modern NLP libraries perform pre-processing automatically, and it is fundamental to understand how your data is being normalised in order to use these functions properly."
      ],
      "metadata": {
        "id": "JOr4HBbHWa0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Spend some time becoming familiar with the differences between `.split()` and `nltk.word_tokenize()`.\n",
        "\n",
        "As part of your comparisons, create frequency distributions based on the results of `.split()` and `nltk.word_tokenize()` for the same strings.\n"
      ],
      "metadata": {
        "id": "UO_rlch8ZaiP"
      }
    }
  ]
}