{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/18_Bigrams_and_Collocations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdBQbS1xhJrm"
      },
      "source": [
        "# Word Collocations\n",
        "\n",
        "One of the most memorable quotes one learns when studying corpus linguistics is by [Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth): “you shall know a word by the company it keeps\". This quote embodies the concept of *collocational meaning* — the idea that the true meaning of words is only realised when used in the context of *other* words.\n",
        "\n",
        "Here is the definition of collocation from Google Dictionary:\n",
        "\n",
        "![](https://i.imgur.com/hz3L88z.png)\n",
        "\n",
        "Note how the linguistic definition is essentially the same thing as the non-linguistic definition, but includes an additional qualification about habitual co-occurence. Collocations are not just words which occurs next to another word, they are words which co-occur *non-randomly* - at a rate higher than chance.\n",
        "\n",
        "How do we know which words occur non-randomly or not? Think of the corpora that you've already seen thus far — large corpora such as Brown (and even larger than that) allow linguists to mine not just frequently occuring single words, but also frequently occuring word pairs, triplets, and so on. Statistical measures of word co-occurence provide insight into natural language and are also responsible for the ever-increasing accuracy of automatic NLP algorithms used today.\n",
        "\n",
        "Statistical knowledge of collocations is not just present in corpora, but is also a function of becoming proficienct in a language. This [Wikipedia article](https://en.wikipedia.org/wiki/English_collocations) contains a decent explanation of some collocations in English. Consider this table from the article, which shows how some word pairs seem natural / correct, whereas others do not:\n",
        "\n",
        "![](https://i.imgur.com/cWE5iO5.png)\n",
        "\n",
        "Note that none of the word combinations in the \"unnatural English\" column are ungrammatical, rather it's just that they seem to be odd combinations particularly when compared to the versions in the \"natural English\" column. Collocations are extracted from large corpora of language and are thought to reflect language use, which is in turn reflected by our interpretation of which of these collocations seems right and which seem odd.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gREesAiNlszP"
      },
      "source": [
        "## Bigrams\n",
        "\n",
        "As was shown above, collocations are defined at a minimum of two or more co-occuring words. Collocations can stretch beyond two words, to three, four, or more word partners.\n",
        "\n",
        "A related but crucially different term from collocations is the term **bigrams**. In NLP, the term `bigram` means a pair of words, and technically can mean *any pair of adjacent words in a text*. The more general terminology is `n-grams`, where the `n` can stand for any number. So you can have bigrams (two words), trigrams (three words), and so on. You may find that sometimes people conflate `collocation` with `ngram`, but what they probably are referring to are unusually frequent ngrams when compared to other ngrams, especially when taking into account individual word frequencies.\n",
        "\n",
        "Simply put, the relationship between collocations and bigrams is that all collocations are bigrams (or, more accurately, n-grams), but not all n-grams are collocations. In order to be upgraded from n-gram to collocation, it must be shown that the n-gram occurs more frequently than would be allowed by chance.\n",
        "\n",
        "So, collocations are defined based on statistical co-occurence frequencies: collocations are thus two words which are more likely to occur with one another when compared to other words, controlled for each word's individual word frequency.\n",
        "\n",
        "How would you locate bigrams in a text? With a `for loop` and functions like `enumerate`, it wouldn't be too difficult. However, NLTK has a neat function which calculates these values for us — `bigrams` and `collocations`. Let's try `bigrams` on some sample text first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mzazZUymjlf"
      },
      "source": [
        "### `nltk.bigrams()`\n",
        "\n",
        "The function to create bigrams in NLTK is relatively straightforward. We just need to call the function on a tokenized text (otherwise we would get bigrams of characters in a string!).\n",
        "\n",
        "Let's see an example. First download the needed resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9b-Yaqdmxgj"
      },
      "outputs": [],
      "source": [
        "# import the NLTK library and download tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c71_1GKDnMsu"
      },
      "outputs": [],
      "source": [
        "# create a sample sentence\n",
        "great_quote = 'we live in a society!'\n",
        "\n",
        "# use nltk bigrams (wrapping it in list() provides us with the output right away)\n",
        "list(nltk.bigrams(nltk.word_tokenize(great_quote)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERiR9LUFj28z"
      },
      "source": [
        "You should be able to inspect the output and get an idea for what's going on. This function is simply starting with the first word of the sentence, making a pair with the second word, then moving on to the second word, making a pair with the third word, and so on. We can conceptualise how this works with a pseudo formula:\n",
        "\n",
        "First we would loop through a sentence:\n",
        "\n",
        "> `for n:m in a sentence (where n = the first word and m = the final word)`\n",
        "\n",
        "Then we would simply iterate ahead by one and add that to the current iterator\n",
        "\n",
        "> `output = n + n1, n1 + n2, n2 + n3..., m-1 + m`\n",
        "\n",
        "Is it that simple, can we produce the bigrams in the same way that the NLTK module has? One excellent function which can help us with this is `enumerate()`!\n",
        "\n",
        "Check out the code below - you can see that it was relatively easy to get the same basic functionality from NLTK with our own code. Take a moment to study what I've had to do in order to prevent index errors.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKd2bkXIkmRa"
      },
      "outputs": [],
      "source": [
        "# use enumerate to make bigrams by asking for adjacent words until we get to the end of the sentence.\n",
        "def bootleg_bigram(tokens):\n",
        "  for i, word in enumerate(tokens):\n",
        "    if i != len(tokens)-1: # what is the role of this line?\n",
        "      print((tokens[i], tokens[i + 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZhCoPI5oZce"
      },
      "outputs": [],
      "source": [
        "# Test out bootleg_bigram on the same text.\n",
        "bootleg_bigram(nltk.word_tokenize(great_quote))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri0ngH1eonCI"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Spend some time using `nltk.bigram()` on some text/strings. Make sure you understand what it is doing, and also compare the output to the `bootleg_bigram()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW7U8Kr3rR3_"
      },
      "outputs": [],
      "source": [
        "# play with nltk.bigram() here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMXZF-WWlhT2"
      },
      "source": [
        "## Finding collocations\n",
        "\n",
        "The `collocations()` function will give us the bigrams which are unusually frequent when also considering the frequency of the individual words in the bigrams. If you look under the hood in the NLTK docs, you'll find they use calculations from [this paper](https://aclanthology.org/J90-1003.pdf) to determine strength of association (i.e., to distinguish collocations from bigrams).\n",
        "\n",
        "Let's return to some of the built-in texts and examine their collocations. We need to download the NLTK resources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHIc64yatOpi"
      },
      "outputs": [],
      "source": [
        "# bring in the nltk resources\n",
        "nltk.download('book')\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVCuCKPJtdkK"
      },
      "source": [
        "\n",
        "### **Your Turn**\n",
        "\n",
        "Examine the collocations for `text6` and `text9`.\n",
        "- What do you think it is about the text which is creating these collocations?\n",
        "- Do you think these same collocations would be found in the other texts?\n",
        "- What might this tell us about the power of using collocations / ngrams if we wanted to predict where documents came from (e.g., guessing the genre, guessing the author)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHGsl5_plr60"
      },
      "outputs": [],
      "source": [
        "# what are the collocations of Holy Grail?\n",
        "print(text6, '\\n')\n",
        "\n",
        "text6.collocations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRm-PmPm0H3f"
      },
      "outputs": [],
      "source": [
        "# examine the collocations of text9.\n",
        "# What do you know about this book, without having read it?\n",
        "print(text9, '\\n')\n",
        "\n",
        "text9.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at some collocations from the other book texts. Do the collocations make sense based on what you know about the books?"
      ],
      "metadata": {
        "id": "lHPXWoDx2d5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_v21YQO0t_n"
      },
      "source": [
        "## **How do people use LOL?**\n",
        "\n",
        "Now, while collocations can tell us about a text in general, we can also use bigrams as a means to explore a targeted use of language we might be interested.\n",
        "\n",
        "Let's consider `text5`, the webchat corpus. Perhaps we want to know how people use \"lol\", regardless of whether \"lol\" is a collocation or not. To do so, we can conditionally sort through the bigrams of the text.\n",
        "\n",
        "We will first obtain all the bigrams of `text5`. Then we will print out the bigrams only if they contain the acronym `lol` or `LOL`. Perhaps this will tell us how LOL is used?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEULDbAk1GdG"
      },
      "outputs": [],
      "source": [
        "# first get the bigrams\n",
        "webchat_bigrams = list(bigrams(text5))\n",
        "\n",
        "# we can see that there is going to be a lot of them!\n",
        "len(webchat_bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMWEuWHTvHAc"
      },
      "outputs": [],
      "source": [
        "# inspect a random part of the bigrams\n",
        "webchat_bigrams[1337:1350]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuHsjc9EwAty"
      },
      "source": [
        "Now that we have obtained all of the bigrams, let's use a list comprehension to fine the bigrams which contain variations of LOL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM20A2fY1lIf"
      },
      "outputs": [],
      "source": [
        "# create a new object named lol_grams\n",
        "# which are the bigrams of webchat_bigrams only if they contain 'lol' or 'LOL'\n",
        "lol_grams = [gram for gram in webchat_bigrams if 'lol' in gram or 'LOL' in gram]\n",
        "\n",
        "# we have a good number!\n",
        "len(lol_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1wqjWnuwOLp"
      },
      "outputs": [],
      "source": [
        "# you can examine the bigrams here\n",
        "# what do you notice?\n",
        "sorted(set(lol_grams))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdTpLGa_wnxG"
      },
      "source": [
        "There's a lot of stuff in that output, mainly because of the way usernames are represented. This is somewhat interesting/useful because it suggests 'lol/LOL' might be the first/only thing many people type (which makes sense). But we are more interested in seeing how 'lol/LOL' pairs with other words. There was also a lot of punctuation joined with 'lol/LOL'.\n",
        "\n",
        "Let's try cleaning it up a bit. We can add a condition that requires both words in the bigram must be `.isalpha()`. Why might this work? Because `.isalpha()` only returns `True` if every character in a string is an alphabetic character (a-z/A-Z). Any punctuation *or* numbers will cause `.isalpha()` to evaulate `False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "835I4JTtxH9n"
      },
      "outputs": [],
      "source": [
        "# Description of str.isalpha\n",
        "help(str.isalpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FQrk2Gj83E7"
      },
      "outputs": [],
      "source": [
        "# now use .isalpha() to only capture lol or LOL with other words\n",
        "lol_grams2 = [gram for gram in lol_grams if gram[0].isalpha() and gram[1].isalpha()]\n",
        "len(lol_grams2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOrxblbwx7gA"
      },
      "source": [
        "Doing this really cleans up the output, although we still have *some* words in there that probably aren't what we want (like the JOIN messages)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OUlqNxxxr_h"
      },
      "outputs": [],
      "source": [
        "# it's a lot easier to see these now\n",
        "sorted(set(lol_grams2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aQTbAGWah3y"
      },
      "source": [
        "### forwards and backwards lol_grams\n",
        "\n",
        "Let's now see if we can discern any interesting patterns with how lol/LOL is used. We'll create three sublists from `lol_grams2`. These sublists will be:\n",
        "\n",
        "- all bigrams which start with lol/LOL and the second word is not lol/LOL\n",
        "- all bigrams which end with lol/LOL and the first word is not lol/LOL\n",
        "- all bigrams where the first and second words are either lol/LOL\n",
        "\n",
        "I'll define all three below in one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTOfu7VfyPiK"
      },
      "outputs": [],
      "source": [
        "targets = ['lol', 'LOL']\n",
        "\n",
        "forward_lolgrams = [gram for gram in lol_grams2 if gram[0] in targets and gram[1] not in targets]\n",
        "backwards_lolgrams = [gram for gram in lol_grams2 if gram[1] in targets and gram[0] not in targets]\n",
        "double_lolgrams = [gram for gram in lol_grams2 if gram[0] in targets and gram[1] in targets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUWz1-huzMMd"
      },
      "outputs": [],
      "source": [
        "# what does this distribution tell us?\n",
        "print('forward lols:', len(forward_lolgrams),\n",
        "      '\\n', 'backwards lols:', len(backwards_lolgrams),\n",
        "      '\\n', 'double lols:', len(double_lolgrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSz4tluky_FY"
      },
      "outputs": [],
      "source": [
        "# explore the forward lolgrams\n",
        "sorted(set(forward_lolgrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY6a4Pe5zkIU"
      },
      "outputs": [],
      "source": [
        "# explore the backwards lolgrams\n",
        "sorted(set(backwards_lolgrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAbDHDkkzxb4"
      },
      "outputs": [],
      "source": [
        "# explore the double lolgrams\n",
        "# does it make sense to you why this sorted is so short?\n",
        "sorted(set(double_lolgrams))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6aUgAVxz7Ox"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Spend some time looking through the distribution of `lol/LOL` that I've created.\n",
        "\n",
        "- Can you draw any conclusions about how these words might be used in terms of how they pattern with other words?\n",
        "- What tweaks or changes might you make to my code?\n",
        "- what other bigrams might be interesting to search for?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigrams and FreqDist\n",
        "\n",
        "Remember that collocations are bigrams which occur more frequently than other pairs of words. The calculations for a collocation will take into account the frequency of a word collocation when compared to the overall frequency of a word.\n",
        "\n",
        "Can we create a makeshift collocation counter using a combination of FreqDist and bigrams?\n",
        "\n",
        "First, we would want to generate a list of bigrams for a text. Let's use some data from The Current."
      ],
      "metadata": {
        "id": "NBvtoxa73IaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# statement - NZ tourism should be limited to protect the environment\n",
        "# load the TP011 data to the notebook environment\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp011.txt'\n",
        "\n",
        "# read in the entire file\n",
        "tp011 = open('tp011.txt').read().rstrip()\n",
        "\n",
        "# remove any punctuation\n",
        "import re\n",
        "punctuation = '[#.,!\\'\"-]'\n",
        "tp011 = re.sub(pattern = punctuation, repl = '', string = tp011)"
      ],
      "metadata": {
        "id": "nTcojxcQ3vXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the comments\n",
        "tp011_comments = [comment.split('\\t')[1] for comment in tp011.split('\\n')]\n",
        "\n",
        "# look at the results. Notice the second comment is full of crap.\n",
        "tp011_comments[:5]"
      ],
      "metadata": {
        "id": "nERt3H0NCQ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's clean up those words that have symbols etc using `.isalpha()`.\n",
        "\n",
        "I will do so with a nested list comprehension.\n",
        "\n",
        "Note that the expression of the list comprehension includes an internal list comprehension.\n",
        "\n",
        "This list comprehension loops over comments, then for each comment loops over the results of running .split() on the comment. The `.isalpha()` removes any word that does not fully contain characters. So any words with numbers, symbols, or other non-letter characters get removed. This is a crude way to remove data, but is effective for our purposes here.\n"
      ],
      "metadata": {
        "id": "WXQth49nEFHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split each comment, then only keep each word which .isalpha(), then glue the comment back together\n",
        "tp011_comments = [' '.join([word for word in comment.split() if word.isalpha()]) for comment in tp011_comments]"
      ],
      "metadata": {
        "id": "mjfyoLiOEIcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the results - the second comment no longer has those words full of symbols.\n",
        "tp011_comments[:5]"
      ],
      "metadata": {
        "id": "PCdxWr4bESBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this data is in a list of individual comments. It would not make much sense to combine the comments into a single string if we want to create bigrams, because we don't want to combine the end of one comment with the start of another comment.\n",
        "\n",
        "So, let's create individual list of bigrams from the data, one for each comment."
      ],
      "metadata": {
        "id": "t-_9fudU5YZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize each comment\n",
        "tp011_comment_tokens = [nltk.word_tokenize(comment) for comment in tp011_comments]\n",
        "\n",
        "# turn tokens into bigrams\n",
        "tp011_comments_bigrams = [list(nltk.bigrams(comment)) for comment in tp011_comment_tokens]"
      ],
      "metadata": {
        "id": "jvAYKOtD41UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we now have a list of comments split into bigrams!\n",
        "tp011_comments_bigrams[:3]"
      ],
      "metadata": {
        "id": "njYZH5Jh756d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have lists of bigrams, we can join them into one giant list, since we know the comments have not cross contaminated one another. We can't use `''.join()` for this, because we have a list of lists.\n",
        "\n",
        "One solution is to use a loop to glue the list into another list using list concatenation. There is likely a more elegant way to do this, but this way gets the job done pretty quick."
      ],
      "metadata": {
        "id": "1_SAKlbH8TdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty output list\n",
        "tp011_bigrams_joined = []\n",
        "\n",
        "# loop through and add each comment to the list\n",
        "for bigram in tp011_comments_bigrams:\n",
        "  tp011_bigrams_joined = tp011_bigrams_joined + bigram"
      ],
      "metadata": {
        "id": "h4r95xQR9NXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the output\n",
        "tp011_bigrams_joined[:20]"
      ],
      "metadata": {
        "id": "D9sfByP_-HuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a single list of bigrams, we can use `FreqDist()` to create a frequency distribution of the bigrams.\n",
        "\n"
      ],
      "metadata": {
        "id": "1vlu4uWF-jIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the frequency distribution\n",
        "tp011_bigram_fdist = nltk.FreqDist(tp011_bigrams_joined)"
      ],
      "metadata": {
        "id": "ou4iA9et-oRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we see a frequency distribution of bigrams\n",
        "tp011_bigram_fdist"
      ],
      "metadata": {
        "id": "llnGoSRr-upf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the most frequent bigrams in this data?\n"
      ],
      "metadata": {
        "id": "6fn-zLzF_YTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp011_bigram_fdist.most_common(10)"
      ],
      "metadata": {
        "id": "N5sNbWHcA2b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most frequent bigram in this output is \"we need\". Does this mean it is also a collocation? Potentially. In order to fully investigate this, we need to compare the individual word frequency against the bigram frequency. So, we could create a frequency distribution of the entire comment section to obtain single word frequency, and compare that to bigram frequency."
      ],
      "metadata": {
        "id": "1LH1A19THDuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create FreqDist from tp011\n",
        "# run the same line extracting just the comments, but joint the results into a single string.\n",
        "tp011_raw = ' '.join([comment.split('\\t')[1] for comment in tp011.split('\\n')])\n"
      ],
      "metadata": {
        "id": "fM6IRXo3He9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp011_raw[:150]"
      ],
      "metadata": {
        "id": "37huNTN4H4IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a FreqDist of the entire set of words\n",
        "tp011_fdist = nltk.FreqDist(nltk.word_tokenize(tp011_raw))"
      ],
      "metadata": {
        "id": "xw37f_LDIEDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a frequency distribution of both single words and bigrams, we can compare individual word against bigram frequencies. One simple way to determine the strength of attraction between two words is the relative proportion of that word occuring with one word compared to all other words. This is related to word probabilities which is covered later, but for now we can view this is a percentage of occurance."
      ],
      "metadata": {
        "id": "9IzWqjn4KaGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the word \"we\" occurs 365 times\n",
        "tp011_fdist['we']"
      ],
      "metadata": {
        "id": "B0hvVTdyKi3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the bigram 'we  need' occurs 100 times\n",
        "tp011_bigram_fdist[('we', 'need')]"
      ],
      "metadata": {
        "id": "B4O7hS1rKnLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So out of the 365 times the word \"we\" occurs, 100 of those times are with the word \"need\". This means about 27% of the occurances of we are in this combination, which could be taken as evidence that this is a strong collocation for we.\n",
        "\n",
        "In order to fully verify this, we would want to find all of the other bigrams that we occurs with. How could we do this?  Firstly, we could obtain a set of all the bigrams that start with 'we', using a similar strategy to the backwards and forwards lolgrams shown above."
      ],
      "metadata": {
        "id": "omqBEBsZLBOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain a set of all bigrams starting with we\n",
        "we_bigrams = set([bigram for bigram in tp011_bigrams_joined if bigram[0] == 'we'])\n",
        "we_bigrams"
      ],
      "metadata": {
        "id": "ECXQrZnTLWn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having obtained this set, we could find the relative proportion of each bigram compared to the overall occurance of we.\n",
        "\n",
        "Look at the output. You'll see that 'we need' is the most common pair and takes up the bulk of we. From this data, we can conclude that \"we\" prefers to come before \"need\" at a rate much higher than many other words. What is the second strongest collocation in this data?\n",
        "\n",
        "And, what could be done to improve this code so that it automatically finds the strongest collocations? Right now it just prints the data which means you have to use your human eyes and brain to locate the most / least strongest collocations."
      ],
      "metadata": {
        "id": "oBPrbqbNLvov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for wb in we_bigrams:\n",
        "  # print frequency of bigram then percentage of bigram based on word occurance\n",
        "  print(f'bigram {wb} occurs {tp011_bigram_fdist[wb]} times, for a total % of {(tp011_bigram_fdist[wb]/tp011_fdist[\"we\"])*100}')"
      ],
      "metadata": {
        "id": "ksVj23FfL1V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using NLTK collocation function on our data\n",
        "\n",
        "We can also convert our text into an NLTK Text object to locate collocations.\n"
      ],
      "metadata": {
        "id": "qbFMWcYtPy-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supply a list of tokens to nltk.Text()\n",
        "tp011_nltk_corpus = nltk.Text(nltk.word_tokenize(tp011_raw))"
      ],
      "metadata": {
        "id": "8CPyjHCMP44g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the top 25 strongest collocations from the text - what do you notice? Crucially, we see that \"we need\" is not among the collocations! Instead, we have a range of other collocations. What is the difference?\n",
        "\n",
        "Well, simply put, [NLTK uses more advanced calculations to find collocations](https://tedboy.github.io/nlps/_modules/nltk/collocations.html). What we see below are word pairs which are strong based on the frequency of *both* sides of the bigram. So not only is \"new\" very likely to occur before \"zealand\", \"zealand\" is also very likely to occur after \"new.\" These word contexts are larger than the simple percentages calculated above.\n",
        "\n",
        "The example of `'blah blah'` might also help this point - how likely is it to see the word 'blah' in different word contexts? Not very - therefore we have very high expectations that when we see the word \"blah\", we are very likely to see another \"blah\" coming afterwords.\n",
        "\n",
        "Compare that with \"we\" - the data above showed that even though \"need\" occured at a high rate after \"we\", the word \"we\" was still found in many other words contexts.\n",
        "\n",
        "The NLTK `.collocations()` function is thus a powerful and fast way to locate collocations in a text. But going through the manual process above also provides some good practice and understanding of what is going into these calculations."
      ],
      "metadata": {
        "id": "Jtg436LXQ5uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp011_nltk_corpus.collocations(num = 25)"
      ],
      "metadata": {
        "id": "kquSMn9BQAUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Consider looking at some other bigrams in some data, or converting texts and exploring collocations with the NLTK `Text()` module.\n",
        "\n"
      ],
      "metadata": {
        "id": "h65capNrPW_C"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "44a9cdcbdccbf05a880e90d2e6fe72470baab4d1b82472d890be0596ed887a6b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}