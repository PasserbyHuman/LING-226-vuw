{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/20_additional_lexical_resources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T70uyRKKuif"
      },
      "source": [
        "# Additional Lexical Resources\n",
        "\n",
        "NLTK provides you with a lot of built-in resources. In this notetbook, I show you how to read in some other resources that are also interesting, including some I have used in my own research. To do so, I rely on loading in a variety of different lexicons, which are sets of words and associated values, scores, ratings, or similar. \n",
        "\n",
        "To use this notebook, you can read the basic description about each resource and then copy the code in case you would like to incorporate these resources into your final project. You should be able to directly copy and paste this code if you'd like to use it!\n",
        "\n",
        "I read each resource in as a dictionary and demontrate the keys/values for you. If you would prefer to download them yourself [you can grab them here](https://github.com/scs-vuw/LING226). But this notebook will also point you to the relevant sources to cite if you do use these resources. All of the resources I have here are for English, although some of these authors also provide data in other languages – you'll need to search that up yourself if you're interested in similar lexicons for languages other than English. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV4iPtyVXXQg"
      },
      "outputs": [],
      "source": [
        "# make a helper function to create dictionaries. \n",
        "\n",
        "# to grab the resource by url, we'll import requests\n",
        "import requests\n",
        "\n",
        "def get_word_rating_resource(url):\n",
        "  \"\"\"helper function to get lexical resources for LING226 students\n",
        "  resources are hosted on github as .txt in the form of Word\\tValue\\n\n",
        "  \"\"\"\n",
        "  # read the raw text and split on newlines\n",
        "  raw = requests.get(url).text.split('\\n')\n",
        "  \n",
        "  # split each pair and convert value to rounded float\n",
        "  # the if statement is there to avoid indexing errors when a row in a resource doesn't have complete data\n",
        "  raw_list = [(pair.split('\\t')[0], round(float(pair.split('\\t')[1]), 3)) for pair in raw if len(pair.split('\\t')) == 2]\n",
        "\n",
        "  # create a dictionary and return it\n",
        "  return dict(raw_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX-5y_birw-Q"
      },
      "source": [
        "# Word Frequency Values\n",
        "\n",
        "Word frequency refers to how frequent a word appears in general usage. One simplistic analysis one can make is to associate less frequent words with more difficult vocabulary. There are a lot of different frequency lists out there. The resource I will give you here are the [SUBTLXus frequency measures.](https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus)\n",
        "\n",
        "These frequency measures are taken from a large corpus of subtitles in American television and movies. Using this method, the authors have created frequency lists for a variety of different languages, which you can explore on their website. \n",
        "\n",
        "I have only included the Log10WF, (logarithmic 10 word frequency), which is a normalized measure of frequency that takes into account the logarithmic distribution of words in corpora. A higher value means that the word is more frequent. Also, keep in mind this data includes both capitalized and lowercased versions of some words – this was because the authors wanted to take proper names into account.\n",
        "\n",
        "You would want to use frequency for any comparison of vocabulary between texts in English. For example, you could compute the average frequency of the words in a text. \n",
        "\n",
        "## How are these values different from `FreqDist` or `ConditionalFreqDist`?\n",
        "\n",
        "Note that this is *different* than calculating the frequency of the words in your corpus / texts. The frequency values from SUBTLEXus are more representative of the *general* frequency of a word, and this can be thought to be an expected frequency for any given word from general usage. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEwEA1wTsT7y"
      },
      "outputs": [],
      "source": [
        "# load in frequency values\n",
        "freq_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/subtlxus_frequency.txt'\n",
        "freq_dict = get_word_rating_resource(freq_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wtxHm6ntPEE"
      },
      "outputs": [],
      "source": [
        "# one of the most frequent words in the English language\n",
        "freq_dict['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUFuBMkbtdf2"
      },
      "outputs": [],
      "source": [
        "# what about some lower frequency words?\n",
        "# (.477 is the lowest value in this resource)\n",
        "freq_dict['Tyrannosaur']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdSrBvTOgDR3"
      },
      "outputs": [],
      "source": [
        "# try some more words\n",
        "\n",
        "word_targets = ['cabbage', 'Klondike', 'sconce', 'Yes', 'car', 'think']\n",
        "\n",
        "for target in word_targets:\n",
        "    if target in freq_dict.keys():\n",
        "        print(f'Word frequency for {target} is {freq_dict[target]}')\n",
        "    else:\n",
        "        print(f'Sorry, {target} not found')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTe3G6v5u6ra"
      },
      "source": [
        "# Age of acquisition\n",
        "\n",
        "Age of acquisition (AoA) is another measure of vocabulary complexity and sophistication. The AoA values represent the average age where native English speakers think they first learned a particular word. These values were gathered via surveys. \n",
        "\n",
        "A lower value means mper people believed they learned those words earlier in life, suggesting those words are more frequent, more concrete, and less sophisticated. \n",
        "\n",
        "You could include this in an analysis as a measure of the overall complexity or sophistication of vocabulary, but keep in mind it is only one dimension of this feature. You would also want to report the *coverage* of this resource - which is a measure of how many words in your corpus were included/excluded in this lexicon. \n",
        "\n",
        "You [can read the paper here.](https://link.springer.com/article/10.3758/s13428-012-0210-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brrVwrx8vey8"
      },
      "outputs": [],
      "source": [
        "# read in aoa data\n",
        "\n",
        "aoa_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/AoA_Brysbart.txt'\n",
        "aoa_dict = get_word_rating_resource(aoa_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au8lDLHJvroN"
      },
      "outputs": [],
      "source": [
        "# which words do people say they learned, on average, after 20 years of age?\n",
        "for word in aoa_dict.keys():\n",
        "  if aoa_dict[word] > 20:\n",
        "    print(word, aoa_dict[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3cBBWngDR5"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_targets = ['cabbage', 'sconce', 'car', 'think', 'no']\n",
        "\n",
        "for target in word_targets:\n",
        "    if target in aoa_dict.keys():\n",
        "        print(f'Age of acquisition for {target} is {aoa_dict[target]}')\n",
        "    else:\n",
        "        print(f'Sorry, {target} not found')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgV8QbqsW7uL"
      },
      "source": [
        "# Word Concreteness\n",
        "\n",
        "Word concreteness is a measure of how abstract or concrete the concept associated with a lexical item is. Words are not necessarily only concrete or only abstract, but rather slide along a scale. For example, the meaning of the noun \"tree\" is more concrete than the meaning of the noun \"peace.\" The list includes mostly single word items but also some compound, two word items. This list was also collected using survey methods. \n",
        "\n",
        "In general, words which are more concrete are more frequent and easier to learn, suggesting that language which is more concrete may be less complex. But this is not a hard and fast rule. Concreteness, AOA, and frequency are three measures you might want to consider as a union when investigating anything related to vocabulary complexity. \n",
        "\n",
        "This resource is a list of average concreteness ratings for 40,000 English words. [You can find the paper here.]('https://link.springer.com/article/10.3758/s13428-013-0403-5')\n",
        "\n",
        "Annotators from Amazon Mechnical Turk were asked to rate how concrete a word was on a scale of 1-5, with a 1 meaning abstract and a 5 meaning concrete. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrMEQREGZmrp"
      },
      "outputs": [],
      "source": [
        "# create concreteness dictionary\n",
        "concrete_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/concreteness.txt'\n",
        "\n",
        "concrete_dict = get_word_rating_resource(concrete_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc9UJvs_Z1N4"
      },
      "outputs": [],
      "source": [
        "# you can now access mean concreteness ratings\n",
        "for word in concrete_dict.keys():\n",
        "  # I put a lot of conditions here just to limit the output. there are a LOT of words in here :)\n",
        "  if concrete_dict[word] == 5 and ' ' not in word and len(word) > 10:\n",
        "    print(word, concrete_dict[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chqv9P7EgDR6"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_targets = ['cabbage', 'sconce', 'car', 'think', 'no']\n",
        "\n",
        "for target in word_targets:\n",
        "    if target in concrete_dict.keys():\n",
        "        print(f'Average concreteness for {target} is {concrete_dict[target]}')\n",
        "    else:\n",
        "        print(f'Sorry, {target} not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPhzTvw_aS8W"
      },
      "source": [
        "# Semantic Diversity Ratings\n",
        "\n",
        "This lexicon is a computationally derived measure of word ambiguity. The authors analysed 1000-word spans (i.e. slices of text 1000 words in length) from the British National Corpus and calculated the probability of any one particular word appearing in the spans. A word which has a higher likelihood of appearing in more spans will be more semantically diverse, meaning it can have more polysemous meanings and senses. \n",
        "\n",
        "A word with lower ratings means it should have a more specific meaning. \n",
        "\n",
        "[You can read the paper here.](https://link.springer.com/article/10.3758%2Fs13428-012-0278-x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASe7QzHcbdDr"
      },
      "outputs": [],
      "source": [
        "semd_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/semantic_diversity.txt'\n",
        "semd_dict = get_word_rating_resource(semd_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW97YwWKdOOo"
      },
      "outputs": [],
      "source": [
        "# what are some of the most restricted words in terms of the contexts they can appear in?\n",
        "for word in semd_dict.keys():\n",
        "  if semd_dict[word] < 0.2:\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjhPFszQgDR7"
      },
      "outputs": [],
      "source": [
        "# are words with more specific meanings also associated with lower semd? \n",
        "\n",
        "word_targets = ['cabbage', 'sconce', 'car', 'think', 'no', 'the', 'pretentious']\n",
        "\n",
        "for target in word_targets:\n",
        "    if target in semd_dict.keys():\n",
        "        print(f'Average semantic diversity for {target} is {semd_dict[target]}')\n",
        "    else:\n",
        "        print(f'Sorry, {target} not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OufczDkvK5cR"
      },
      "source": [
        "# Humor Ratings\n",
        "\n",
        "This is a list of humor ratings for almost 5000 English words. [You can read the paper here.](https://link.springer.com/article/10.3758%2Fs13428-017-0930-6) \n",
        "\n",
        "Basically, they asked ~800 people on Amazon Mechanical Turk to rate how humorous a word was using a 1-5 scale. The values in this lexicon are the mean humor ratings for each word, after the authors trimmed and cleaned the data.\n",
        "\n",
        "You could use this resource to see whether a certain text/genre uses words with higher or lower individual humor ratings, or if a text even has words which are deemed to be funny. I'm not convinvced this measure can actually represent how humorous a text is, but nonetheless this might be an interesting or fun resource to try. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhPRG9F3X_D8"
      },
      "outputs": [],
      "source": [
        "# create humor dictionary\n",
        "humor_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/humor.txt'\n",
        "humor_dict = get_word_rating_resource(humor_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4heYMfaWLld"
      },
      "outputs": [],
      "source": [
        "# you can now access mean humor ratings\n",
        "for word in humor_dict.keys():\n",
        "  if humor_dict[word] > 4:\n",
        "    print(word, humor_dict[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtCDnmZHgDR7"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_targets = ['cabbage', 'sconce', 'car', 'think', 'no', 'the', 'pretentious']\n",
        "\n",
        "for target in word_targets:\n",
        "    if target in humor_dict.keys():\n",
        "        print(f'Average humour for {target} is {humor_dict[target]}')\n",
        "    else:\n",
        "        print(f'Sorry, {target} not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP5lYsXSdNxb"
      },
      "source": [
        "# Emotion Words\n",
        "\n",
        "This resource is a bit different because rather than being a single value associated with each word, this resource is a series of words and whether or not they are associated with a particular emotion. The emotions include:\n",
        "\n",
        "- anger\n",
        "- anticipation\n",
        "- disgust \n",
        "- fear\n",
        "- joy\n",
        "- negative\n",
        "- positive \n",
        "- sadness\n",
        "- surprise\n",
        "- trust\n",
        "\n",
        "The values associated with the words in the dictionary are a 0 or 1 for each of these emotions. A 0 means the word does not have an association, whereas a 1 means it does. So it is binary - either or. With a resource like this, one would likely then want to see how many words with these associations a particular text has. Notice that it also has positive/negative, so in a way this can also be used as a sentiment analysis resource. \n",
        "\n",
        "This is just one of many resources provided [on this website.](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjufYXEbe_So"
      },
      "outputs": [],
      "source": [
        "# get emotion resource and split on newlines\n",
        "emotion_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/emotion_lexicon.txt'\n",
        "raw_emotion = requests.get(emotion_url).text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1mTWxTLfa7i"
      },
      "outputs": [],
      "source": [
        "# create a list, but this time of triples\n",
        "emotion_list = [(triple.split('\\t')[0], triple.split('\\t')[1], round(float(triple.split('\\t')[2]),2)) for triple in raw_emotion]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPYnwrxri9OQ"
      },
      "outputs": [],
      "source": [
        "# create empty dictionary with defaultdict having another dictionary inside\n",
        "from collections import defaultdict\n",
        "emotion_dict = defaultdict(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yUTGe6xi-fg"
      },
      "outputs": [],
      "source": [
        "# add each entry to the new dictionary\n",
        "for triple in emotion_list:\n",
        "  word, category, value = triple\n",
        "  emotion_dict[word][category] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcBwjiXAjtZj"
      },
      "outputs": [],
      "source": [
        "# you can now look up words for their associations \n",
        "emotion_dict['nepotism']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQcsirNRlcPm"
      },
      "outputs": [],
      "source": [
        "# and you can index deeper to get specific categories\n",
        "emotion_dict['nepotism']['negative']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICpLHVqPgDSA"
      },
      "outputs": [],
      "source": [
        "\n",
        "word_targets = ['fight', 'play', 'shout', 'clown', 'tornado']\n",
        "\n",
        "for target in word_targets:\n",
        "    if target in emotion_dict.keys():\n",
        "        print(f'Emotions for {target} is {emotion_dict[target]}')\n",
        "    else:\n",
        "        print(f'Sorry, {target} not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IutcNMBpliy"
      },
      "source": [
        "# General Inquirer List\n",
        "\n",
        "This is another lexicon which groups words into particular categories. The different categories are [explained here](http://www.wjh.harvard.edu/~inquirer/homecat.htm)\n",
        "\n",
        "There are a *lot* of categories and you should look through them in order to know if there is anything interesting in here for you. For example, this list includes words associated with different emotions, activities, feelings, etc. The lists work in that words thought to be included with those particular emotions / feelings are simply grouped in that list. \n",
        "\n",
        "So if a word is in the list, you can assume it is associated with that concept. You would use a resource like this to find how many words of a particular category exist in a text. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUKZ6twIp3Y3"
      },
      "outputs": [],
      "source": [
        "# GI list\n",
        "gi_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical%20resources/inquirerbasic.txt'\n",
        "raw_gi = requests.get(gi_url).text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuhUnfn2p-I5"
      },
      "outputs": [],
      "source": [
        "# we need to do something a bit different for this resource\n",
        "gi_dict = dict()\n",
        "\n",
        "for category in raw_gi:\n",
        "  gi_dict[category.split('\\t')[0]] = category.split('\\t')[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNBX6lEgqzB-"
      },
      "outputs": [],
      "source": [
        "# which words are associated with strength?\n",
        "gi_dict['Strong_GI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJN_FcAyq8Y1"
      },
      "outputs": [],
      "source": [
        "# which words are associated with persistance?\n",
        "gi_dict['Persist_GI']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "44a9cdcbdccbf05a880e90d2e6fe72470baab4d1b82472d890be0596ed887a6b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}