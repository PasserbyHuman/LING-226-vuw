{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "d6ORa3vqfkbN",
        "iBsO25N44O6o",
        "_MBVS4C9Ma3r"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmsW8I0zvI0u"
      },
      "source": [
        "# **You can make your own grammar!!**\n",
        "\n",
        "Chapter 07 of the NLTK book discusses applications of NLP related to \"information extraction.\" The point of this section is to have you reflect upon the ways one can computationally represent relationships among different entities, such as locations and people (i.e., can we query where a given person is at a given time based on text data?). When it comes to completely unstructured text, such as text read in raw from the internet, this task is quite difficult. However, when some degree of structure is imposed on the information, some form of sense can be obtained. \n",
        "\n",
        "That's all well and good, but we will also use this chapter to think about patterns of language above the word level. This is where we dip our toes into *syntax* using some of the techniques we've played with in the prior chapters. \n",
        "\n",
        "This is where **chunking** comes into the picture. Chunking is related to larger patterns in language above the word level, now at the *phrase* level. \n",
        "\n",
        "Noun phrases, verb phrases, prepositional phrases, relative clauses — all of these can be chunks. However, as the book makes clear, these things are not to be equated. \n",
        "\n",
        "Noun phrase chunks (NP chunks) are \"smaller\" noun phrases within a larger noun phrase. As the book explains, a noun phrase could be quite long, such as \n",
        "\n",
        "`the politician who paid the woman who paid the clerk who sold the car`\n",
        "\n",
        "Within that (odd) sentence, there are a lot of smaller NP chunks: \n",
        "  - the politician who paid the woman\n",
        "  - the politician\n",
        "  - the woman who paid the clerk\n",
        "  - the woman\n",
        "  - the clerk who sold the car\n",
        "  - the clerk\n",
        "  - the car\n",
        "\n",
        "The entire sentence itself is also a noun phrase - `the politician` is the head noun and the rest of the sentence is a relative clause working to provide more information about the noun. \n",
        "\n",
        "\n",
        "Finding all of the smaller Noun Phrases in a document/text is one strategy used for the purpose of information extraction (if you've worked with other NLP libraries you may have noticed they almost always have a NP chunk method). When we string together phrases into larger sentences, we begin to unravel the syntax or grammar of a text. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbCNbWjZJyEJ"
      },
      "source": [
        "## **Creating a chunk/phrase parser**\n",
        "\n",
        "We've worked so far with regular expressions and part of speech tags to parse text for specific patterns. The next step is to use NLTK's method for parsing larger \"chunks\", which are specific sequences of words. To do so, you get to define your own grammar — cool! \n",
        "\n",
        "Note that the book provides a sentence, which has been tagged, and then manually defines a grammar in the form of a dictionary. \n",
        "\n",
        "Instead of me trying to walk through each thing the NLTK authors do, I'd like to just focus on the gist, so you can walk away from this notebook/workshop and the reading/skimming of NLTK to get an idea on how to write your own grammar parser. \n",
        "\n",
        "Let's begin!\n",
        "\n",
        "> *please note that you will not be able to use the .draw() function to see the syntax trees in Colab. Yeah they are really cool but they don't work unless you can run Python on your local machine. We'll be able to draw dependency trees when we work with spaCy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si_gsntAKTgT"
      },
      "source": [
        "# download the resources we will need\n",
        "import nltk\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t35NJ5N4fe4"
      },
      "source": [
        "## Defining a noun phrase\n",
        "\n",
        "Let's start first and foremost by using NLTK to define a grammar which can identify simple noun phrases in a text. In linguistics, the definition of different phrases follow what are known as [phrase structure rules](https://en.wikipedia.org/wiki/Phrase_structure_rules). We can represent a noun phrase using \"NP\", and define it using a system of rules.\n",
        "\n",
        "For example, the most basic definition of a NP is any noun preceded by a determiner/article. We will start with that as a definition and express it in terms of Part of Speech tags:\n",
        "\n",
        "```\n",
        "NP —> (DET) + NN\n",
        "```\n",
        "\n",
        "The definition above says that a noun phrase is any noun preceded by an optional determiner (the brackets around (DET) mean it is optional). \n",
        "\n",
        "We are going to use NLTK's regex parser to define and parse for noun phrases using text which is tagged for part of speech. The \"grammar\" will use the NLTK regex parser function, which means we can use the `<tag>` method of creating patterns in a more readable way that the base regex. To convert the above rule into the NLTK regex, we can use the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a simple NP\n",
        "grammar1 = \"NP: {<DT>?<NN>}\""
      ],
      "metadata": {
        "id": "Ymgqo7FLHwp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell above, I saved the regex pattern to a variable named `grammar`. The regex pattern itself is encased in the curly braces `{}`, and I have provided a name for that pattern using \"`NP:`\" before the pattern. The NLTK parser will be able to use this information. To do so, we need to create a parser from the `nltk.RegexpParser()` function, and pass our grammar as an argument to the function. \n",
        "\n",
        "We will save the parser to a variable."
      ],
      "metadata": {
        "id": "wsu6S8gRJIM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a parser with our grammar from the cell above\n",
        "# note that our grammar1 pattern is being fed to the parser\n",
        "np_parser = nltk.RegexpParser(grammar1)"
      ],
      "metadata": {
        "id": "ZDoHpzxrJ0lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crucially, we must use our parser on text which has part of speech tags applied. We know how to do that! Let's create a tagged text and use our parser on it. \n",
        "\n",
        "I'll create a short text below to get started. \n"
      ],
      "metadata": {
        "id": "3VjmlVGhKtM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a short tokenized and tagged text\n",
        "sent = \"My friends, the sea was angry that day. Like an old man sending back soup in a deli.\"\n",
        "sent_tagged = nltk.pos_tag(nltk.word_tokenize(sent))\n",
        "\n",
        "# do you see any POS tag combinations which should be matched by our parser? \n",
        "sent_tagged"
      ],
      "metadata": {
        "id": "shpa680EUc95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a tagged text, we can parse the text for noun chunks using the parser we created. To do so, we simply pass the tokenized/tagged text to the `.parse()` method from the parser we made.\n"
      ],
      "metadata": {
        "id": "Q-JT6d3lOr3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the example for noun chunks\n",
        "sent_parsed = np_parser.parse(sent_tagged)"
      ],
      "metadata": {
        "id": "w97rqLKEUy9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parser will create a `nltk.Tree` object. You can see the full tree by printing it either using `print()` or the `.pprint()` method from the object. "
      ],
      "metadata": {
        "id": "5ppnVOb2pWei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the output (you can also use print())\n",
        "sent_parsed.pprint()"
      ],
      "metadata": {
        "id": "fhVAn1FBpI0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wE8T6QHUfre"
      },
      "source": [
        "### Inspecting the output... \n",
        "\n",
        "- the \"S\" means the \"top\" of the sentence\n",
        "- any nested brackets within the \"S\" denote different chunks we have asked for. \n",
        "  -  Each open paren `(` has the chunk label as the first thing. This is a special NLTK object (including the 'S')\n",
        "  - Note that NPs match based on the tags, so that determiners such as `the` and `that` are both being matched by `<DET>`.\n",
        "- in this case, we see that our grammar has found *five* noun chunks. Nice!\n",
        "- All of the other words are listed separately with their POS tags, they were not part of the chunks we defined. \n",
        "\n",
        "We can interact with the `Tree` object in more ways than printing, but before doing so, let's update our noun grammar. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding to our noun phrase\n",
        "\n",
        "Our noun phrase in the grammar above is quite simple and doesn't capture the range of other possible ways a noun phrase can exist. For example, and as pointed out in the NLTK book, in addition to optional determiners before a noun phrase, nouns can also have optional adjectives before the noun phrase. \n",
        "\n",
        "NLTK uses this pattern to define a possible NP chunk:\n",
        "\n",
        "`\"NP: {<DT>?<JJ>*<NN>}\"`\n",
        "\n",
        "This pattern says a noun phrase us:\n",
        "  - zero or one determiners (`<DT>?`), \n",
        "  - followed by zero or more adjectives (`<JJ>*`)\n",
        "  - followed by a required noun (`<NN>`).\n",
        "\n",
        "Note the difference between the rules for `DT` and `JJ`. Why would we restrict this rule to only one determiner but effectively infinite adjectives? Remember that determiners are words like `the` and `a` - based on your knolwedge of English, is it common for more than one determiner to come before a noun? (no, it's not).\n",
        "\n",
        "Then consider that `JJ` means adjectives, so words like `big`, `red`, etc. Can you place more than one adjective before nouns in English? Of course! The sentence `the small blue bird` has the tags `DT JJ JJ NN`, whereas the sentence `the bird` has the tags `DT NN`. The pattern defined above allows for these possibilities. \n",
        "\n",
        "So, let's update our NP gramar with this new pattern"
      ],
      "metadata": {
        "id": "RCevdkYjsGrQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMiQalBYK-1u"
      },
      "source": [
        "# save the new np grammar rules\n",
        "grammar2 = \"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqM_Uh1XQQK5"
      },
      "source": [
        "# we need to make a new parser with out new rule\n",
        "np_parser2 = nltk.RegexpParser(grammar2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUVdGygHQb-5"
      },
      "source": [
        "# parse our sentence again with the new parser\n",
        "sent_parsed2 = np_parser2.parse(sent_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the outputs from the two different parsers. \n",
        "\n",
        "What is different in our second example compared to the first? "
      ],
      "metadata": {
        "id": "QSOcdl-AtsSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_parsed)\n",
        "print('\\n') \n",
        "print(sent_parsed2)"
      ],
      "metadata": {
        "id": "QL0mzUzctrtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX2yR0Yj4jjc"
      },
      "source": [
        "Here is an explanation for all the matches returned from the second iteration of our parser. \n",
        "\n",
        "`\"NP: {<DT>?<JJ>*<NN>}\"`\n",
        "\n",
        "Let's look through the matches:\n",
        "\n",
        "- `the sea` is a `<DT><NN>`, so it is matched, because the adjective (`<JJ>`) is optional\n",
        "- `that day` is also a `<DT><NN>` and matches for the same reasons\n",
        "- `an old man` is matched, and represents a full match from our pattern – `<DT><JJ><NN>`!\n",
        "- `soup` is matched because both the `<DT>` and `<JJ>` tags are optional, whereas `<NN>` is the minimum required to form a noun phrase.\n",
        "- `a deli` is matched because it is a `<DT><NN>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqTfSXRIXHf6"
      },
      "source": [
        "## Navingating the Tree structure\n",
        "\n",
        "The NLTK `Tree` object is made up of smaller `subtrees`, which we can navigate using the `.subtrees` method. The subtrees include the entire `S` tree and then any of the different parsed chunks of the tree. Because we have specified our grammar to create only NP chunks, our subtree includes only those chunks. \n",
        "\n",
        "In the cell below, I first save the subtrees to a list comprehension. I then loop throught that list comprehension, but I skip the first subtree, because that is the total tree (i.e, the whole sentence, and I just don't want to see it again!). Using this method, I can get all of the NP chunks from my parsed text (we will come to find out there are other, perhaps easier methods to do this). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzpqHoIWXBIU"
      },
      "source": [
        "# list comprehensionto find the chunks\n",
        "subtrees = [subtree for subtree in sent_parsed2.subtrees()]\n",
        "for subtree in subtrees[1:]: # skip the first subtree becuse its the \"S\" tree\n",
        "  print(f'Chunk: {subtree}') # we can see each chunk from the Tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oArMpRb_yhjf"
      },
      "source": [
        "The trees will also have a `.label()` method for the chunks to see the chunk type — the label is whatever name we gave the chunk in the grammar *before* the curly brackets `{}`. In this case, we specified \"NP:\".\n",
        "\n",
        "In the cell below, I again loop through, but ask for the label as well as the tree:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVZ70WENyW2H"
      },
      "source": [
        "# now do the same using the .label() method\n",
        "subtrees = [subtree for subtree in sent_parsed2.subtrees()]\n",
        "for subtree in subtrees[1:]:\n",
        "  print(f'Chunk type: {subtree.label()}, Chunk: {subtree}, leaves: {subtree.leaves()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the `.leaves()` of the tree provide us with the original version of the input, which is the tagged text. "
      ],
      "metadata": {
        "id": "rhkrctsfi_A_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTiJIvkmUwKC"
      },
      "source": [
        "# Using .leaves() will provide you with the original input. \n",
        "subtrees = [subtree for subtree in sent_parsed2.subtrees()]\n",
        "for subtree in subtrees[1:]:\n",
        "  print(f'Chunk type: {subtree.label()}, Chunk: {subtree}, leaves: {subtree.leaves()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Create some sample sentences which contain chunks that do and do not follow our grammar above. \n",
        "\n",
        "- Practice parsing these sentences and inspect the outputs \n",
        "- Play around - how many adjectives can you chain together?\n",
        "- Are you able to give the NPs a different chunk label?\n",
        "- Are you able to search for chunks based on the label?\n",
        "- What other NPs are you noticing that are *not* being captured in our parser grammar? \n",
        "  - What could be done to capture these NPs?"
      ],
      "metadata": {
        "id": "d6ORa3vqfkbN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrshOaw9X2NQ"
      },
      "source": [
        "## Adding a verb phrase to our rules\n",
        "\n",
        "All right, now let's think about how we could further parse our sentence so that it might contain all of the chunks in our sentence. To do so, we need to update our grammar!\n",
        "\n",
        "\n",
        "Let's introduce a rule to make a verb phrase or `VP`. Just like a noun phrase must contain a noun, a verb phrase must contain a verb. What part of speech tags are associated with verbs? We can find out by asking the nltk help and passing in `V.` as a regex pattern to our help:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find all POS tags which start with \"V\" and are followed by anything\n",
        "nltk.help.upenn_tagset('V.')"
      ],
      "metadata": {
        "id": "zybFFT8FlwLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to write a rule which will allow for any of these types of verbs to be parsed as a VP. We can do that with regex quite easily, using the wildcard `.` and zero or more quantifier `*`. \n",
        "\n",
        "Using the NLTK syntax, our VP rule could thus be:\n",
        "\n",
        "```\n",
        "VP: {<V.*}\n",
        "```\n",
        "\n",
        "We can now add our two rules together.\n",
        "\n",
        "To do so, I will use triple quotes and put each rule on a new line."
      ],
      "metadata": {
        "id": "Y0C0F3uJmb3t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ualb4QYJNjj"
      },
      "source": [
        "# define a grammar to find NP and VPs\n",
        "np_vp_grammar = \"\"\"\n",
        "NP: {<DT>?<JJ>*<NN>} \n",
        "VP: {<V.*>}\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to make a new parser with our new grammar:"
      ],
      "metadata": {
        "id": "ZUsyGGj9nKG7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In6retQqgtgN"
      },
      "source": [
        "# update the parser and create a new version of our sentence\n",
        "np_vp_parser = nltk.RegexpParser(np_vp_grammar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our new parser out on our same example:"
      ],
      "metadata": {
        "id": "g8333T4KnPwC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2905M5Zl37F4"
      },
      "source": [
        "# parse the same sentence from above\n",
        "sent_parsed3 = np_vp_parser.parse(sent_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8oTLLqD401v"
      },
      "source": [
        "Inspect the output – note that printing the labels helps us to see the type of chunk as well as the associated phrases. \n",
        "\n",
        "Again, I'm skipping the first index of `.subtrees()` to avoid printing out the entire tree again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om6Iv1q94EK6"
      },
      "source": [
        "# look at each Chunk - we are getting almost the entire text back\n",
        "subtrees = [subtree for subtree in sent_parsed3.subtrees()]\n",
        "for subtree in subtrees[1:]:\n",
        "  print(f'Chunk type: {subtree.label()}, Chunk: {subtree}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice about the output? Adding in a rule for finding a VP now returns two more chunks, but those two chunks start to clarify the nature of our example. In other words, we can see how important NP and VP chunks are for constructing sentences and utterances. "
      ],
      "metadata": {
        "id": "fb3Iuo6VoF7V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlmD7Kg-FLsm"
      },
      "source": [
        "### Make a chunk printer function\n",
        "\n",
        "We're at the point where we are repeating ourselves over and over when we print out the chunks and subtrees. We should thus create a function which helps print out our parsed input. This way we don't have to keep copying and pasting our cells over and over.\n",
        "\n",
        "In the cell below, I make a function which loops through the subtrees and prints the chunks and their labels, just as I have been doing above. I'll also add a flag, `print_full`, which means that I can ask for the full sentence to be printed out or not. The default will be `False`, meaning that the user will have to explicitly ask for the full sentence to be printed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayeaCT1nFPBf"
      },
      "source": [
        "def chunk_printer(chunk, print_full = False):\n",
        "  \"\"\"\n",
        "  prints all chunks with tags, \n",
        "  and optionally prints entire text\n",
        "  \"\"\"\n",
        "  # gather subtrees\n",
        "  subtrees = [subtree for subtree in chunk.subtrees()]\n",
        "\n",
        "  # check if print_full is set to True\n",
        "  # we could also just type `if print_full` right?\n",
        "  if print_full == True:\n",
        "    print(f'Full sentence is: {chunk}')\n",
        "\n",
        "  print('=============================')\n",
        "  print('The chunks are:')\n",
        "  for subtree in subtrees[1:]:\n",
        "    print(f'Chunk type: {subtree.label()}, \\nChunk: {subtree}\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out on our parsed sentence. "
      ],
      "metadata": {
        "id": "8_kJYOmspIA1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz0L5dVxFiWi"
      },
      "source": [
        "# print default, without the full chunk\n",
        "chunk_printer(sent_parsed3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW24ghGGEai"
      },
      "source": [
        "# ask for the full chunk (it's quite ugly tho)\n",
        "chunk_printer(sent_parsed3, print_full = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Continue parsing some examples, but now with the VP and NP grammar. You can use the `chunk_printer()` function to inspect the output.\n",
        "\n",
        "- Just as you considered the nouns above, are there are verbs being missed?\n",
        "- Are there any additional words you think should be added to the verb phrases?\n",
        "- We used `<V.*>` to find all the verbs, but our pattern for nouns is `<NN>`. Should we do something about that? "
      ],
      "metadata": {
        "id": "UMnwKIiCptVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what different types of nouns exist in our tagset? ? \n",
        "nltk.help.upenn_tagset('N.')"
      ],
      "metadata": {
        "id": "olhBWY9kqMbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding a full clause to our grammar\n",
        "\n",
        "Let's now look at the definition of a (simple) *clause*:\n",
        "\n",
        "```\n",
        "Clause: NP + VP\n",
        "```\n",
        "\n",
        "That's it!! Well, we've already defined our NP and VP chunks, so now we just need to add a rule to our grammar which combines these chunks into a larger chunk. The rule would follow the same format, but we can use our chunk labels as the search pattern:\n",
        "\n",
        "```\n",
        "Clause: {<NP><VP>}\n",
        "```\n",
        "\n",
        "This will work because the NLTK parser will parse our data *in the order of the rules*, so that chunks made as part of the first rule can then be used in subsequent rules — neat-o. \n",
        "\n",
        "Let's add the clause rule to our grammar"
      ],
      "metadata": {
        "id": "vlefX6zHr3P9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkLq-B9iGZJf"
      },
      "source": [
        "# update our grammar to include a clause. \n",
        "clause_grammar =  \"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN>} # NOUN PHRASE\n",
        "  VP: {<V.*>} # VERB PHRASE\n",
        "  CLAUSE: {<NP><VP>} # CLAUSE!\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE_lRadRGkTS"
      },
      "source": [
        "# update the parser\n",
        "clause_parser = nltk.RegexpParser(clause_grammar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYeFpiVeGkTS"
      },
      "source": [
        "# parse the example again \n",
        "sent_parsed4 = clause_parser.parse(sent_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGjxUH4J8Cyt"
      },
      "source": [
        "# inspect the ouput\n",
        "chunk_printer(sent_parsed4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update chunk printer function\n",
        "\n",
        "Take a moment to look at the output and note the nested nature of the chunks. We are retaining the smaller constituents of the clauses (i.e., the separate NP and VP chunks), as well as the larger clauses. It does become a bit tricky to read though eh? \n",
        "\n",
        "Let's update `chunk_printer()` so that we can control the types of chunks the parser returns. We do that with a new argument, `chunks`, which can take a list of chunk tags. If the list is empty, it will just return all the chunks. \n",
        "\n",
        "Otherwise, we can supply a list of chunks to control the output. "
      ],
      "metadata": {
        "id": "g2tDE3OrxJSq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U3j4mp68pVF"
      },
      "source": [
        "# a third argument is added, chunks, set with any empty list as default. \n",
        "def chunk_printer(chunk, print_full = False, chunks = []):\n",
        "  \"\"\"\n",
        "  prints all chunks with tags, \n",
        "  and optionally prints entire text. \n",
        "  lets use pass a list of chunks to be printed\n",
        "  \"\"\"\n",
        "\n",
        "  subtrees = [subtree for subtree in chunk.subtrees()]\n",
        "\n",
        "  if print_full == True:\n",
        "    print(f'Full sentence is: {chunk}')\n",
        "\n",
        "  # if chunks exists (has a value)\n",
        "  if chunks:\n",
        "    print('=============================')\n",
        "    print(f'Your requested chunks {chunks}:\\n')\n",
        "    for subtree in subtrees[1:]:\n",
        "      if subtree.label() in chunks: # using the labels to match the input to any existing chunks\n",
        "        # if the user puts in a chunk that doesn't exist it won't print it - something to worry about later\n",
        "        print(f'Chunk type: {subtree.label()}, \\nChunk: {subtree}\\n')\n",
        "\n",
        "  else:\n",
        "    print('=============================')\n",
        "    print('The chunks are:\\n')\n",
        "    for subtree in subtrees[1:]:\n",
        "      print(f'Chunk type: {subtree.label()}, Chunk: {subtree}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDQFVXxy9OYF"
      },
      "source": [
        "We can now control our program to give us the chunks of our choice, below I am asking for NP and VP only. This program would probably break if I passed chunks that don't exist, but since we're the only one using this so far, we'll keep this secret between us. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVu0rJZCGgkt"
      },
      "source": [
        "# So we could find just the NPs and VPs...\n",
        "chunk_printer(sent_parsed4, chunks = ['NP', 'VP'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lhpkBoJ9Xuc"
      },
      "source": [
        "# Or just the clauses\n",
        "chunk_printer(sent_parsed4, chunks = ['CLAUSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn it up to 11\n",
        "\n",
        "Right now our grammar is still relatively simple, and it does not capture all the possible ways nouns, verbs, noun phrases, and verb phrases could be defined (and honestly, it probably never will). \n",
        "\n",
        "Rather than go through a bunch of iterative updates, I'm going to dump a few updates to the grammar on you in the next cell. \n",
        "\n",
        "This grammar is heavily influenced by the example I've been using as well as phrase structure rules gleaned from websites [like this one](http://faculty.washington.edu/cicero/370syntax.htm). \n",
        "\n",
        "One of the things I'm doing in the grammar below is defining smaller nouns before then defining larger NPs. Same thing with verbs. This, for instance, allows us to find noun-noun combinations and put them together as one noun phrase. \n",
        "\n",
        "Another thing I'm doing is accounting for prepositional phrases which can nest with sentences. We have two prepositions in our sentence: `like` and `in` (although `like` is probably a bit iffy in this category — but that's what `pos_tag()` gives us, so we'll go with it). They are both tagged with `IN` \n",
        "\n",
        "We also define our NP to now be anything we tag with an N, which is now expanded in its scope:"
      ],
      "metadata": {
        "id": "jZtnBZG15Eoh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FSL4UgvDXJj"
      },
      "source": [
        "# update our grammar to find smaller nouns/verbs before NP/VP\n",
        "# definition of noun has also been expanded. \n",
        "grammar1 = \"\"\"\n",
        "N: {<DT>?<JJ>*<N.*><IN>*} # optional determiner, followed by any number of adjectives, followed by noun, then preps\n",
        "NP: {<N><N>*} # we can use the smaller pattern of N to stand for nouns now\n",
        "V: {<V.*>}\n",
        "VP: {<V>}\n",
        "CLAUSE: {<NP><VP>} # any adjacent NP + VP is a clause\n",
        "\"\"\"\n",
        "\n",
        "# initalise the parser\n",
        "cp1 = nltk.RegexpParser(grammar1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWiO1zxADXJq"
      },
      "source": [
        "# Look at the NP and VPs, which will contain the smaller Ns and Vs\n",
        "chunk_printer(cp1.parse(sent_tagged), chunks = ['NP', 'VP', 'CLAUSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KfQFr9nEeJr"
      },
      "source": [
        "# Look at the clauses, which gives a nice picture of the nested nature of our design. \n",
        "chunk_printer(cp1.parse(sent_tagged), chunks = ['CLAUSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isDrePtcEvmO"
      },
      "source": [
        "### Add adverbs\n",
        "\n",
        "Our grammar is improved in terms of finding the main constituents of the test sentence, but could still be improved. For example, we are still missing \"angry that day my friends\" and the \"sending back soup\" parts of the sentence (here it is again in case you forgot the whole thing):\n",
        "\n",
        "```\n",
        "My friends, the sea was angry that day. Like an old man sending back soup in a deli.\n",
        "```\n",
        "\n",
        "We need to expand what a VP can be, since we're still missing out on the word `angry` and `back`. Let's check the \"basic\" phrase structure rules for English,  [taken from here](http://faculty.washington.edu/cicero/370syntax.htm)\n",
        "\n",
        "```\n",
        "S → NP  VP \n",
        "\n",
        "NP → (det)(adv)* (adj)*N\n",
        "\n",
        "VP → V (NP) (ADV)*\n",
        "```\n",
        "\n",
        "This means we can define a verb phrase as a verb followed by an optional noun phrase and zero or more optional adverbial phrases. The use of `back` in our sentence is tagged as an adverb (`RP`), so we can add that to our VP. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note that depending on which site I've linked to, the phrase structure rules are defined slightly differently, fun ;). For instance, this site defines NP + VP as sentence, whereas I've been using the word \"clause\". \n",
        "\n",
        "Anyhow, let's update our grammar so that a VP can be a verb followed by a noun or an adverbial (`<RP>`), followed by yet another noun. Our example actually has the adverb before the noun phrase, so we already know this might not be 100% accurate at parsing our data. I'll tweak the pattern accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BAUMIMZFQg5"
      },
      "source": [
        "# update our grammar to increase scope of what a VP is\n",
        "grammar2 = \"\"\"\n",
        "N: {<DT>?<JJ>*<N.*><IN>*} # create smaller nouns first\n",
        "V: {<V.*>}                # then create smaller verbs\n",
        "NP: {<N><N>*}         # create NPs after VPs are created - any N that did not get put into VP will become NP\n",
        "VP: {<V><N>*<RP>*<NP>*}  # create our VPs\n",
        "CLAUSE: {<NP><VP>}        # any adjacent NP + VP is a clause\n",
        "\"\"\"\n",
        "cp2 = nltk.RegexpParser(grammar2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ-XX1hEFQg5"
      },
      "source": [
        "# Look at the NP and VPs\n",
        "chunk_printer(cp2.parse(sent_tagged), chunks = ['NP', 'VP', 'CLAUSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmh4eg0KH8bg"
      },
      "source": [
        "### Add adjectives\n",
        "\n",
        "Holy crap , did you see that? We got a VP which is the pattern `V — Adverb — N — N`, nice!\n",
        "\n",
        "The word `angry` is still not being captured by our rule, because we have not allowed the smaller V to capture adjacent adjectives/adverbs - let's update it again. (The word `like` is also not being captured, but let's leave that alone for now. )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj-0MJlAJe-W"
      },
      "source": [
        "# update our grammar to let Vs take adjectives/adverbs\n",
        "grammar3 = \"\"\"\n",
        "N: {<DT>?<JJ>*<N.*><IN>*} # create smaller nouns first\n",
        "V: {<V.*><JJ|RP>*}        # then create smaller verbs (can have adjectives/adverbs afterwards)\n",
        "VP: {<V><N>*<RP>*<NP>*}  # create our VPs\n",
        "NP: {<N><N>*}         # create NPs after VPs are created - any N that did not get put into VP will become NP\n",
        "\n",
        "CLAUSE: {<NP><VP>}        # any adjacent NP + VP is a clause\n",
        "\"\"\"\n",
        "cp3 = nltk.RegexpParser(grammar3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RKu7eNgJe-d"
      },
      "source": [
        "# Look at the NPs, VPs, and clauses - what changed? \n",
        "chunk_printer(cp3.parse(sent_tagged), chunks = ['NP', 'VP', \"CLAUSE\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clause level looking good\n",
        "chunk_printer(cp3.parse(sent_tagged), chunks = ['CLAUSE'])"
      ],
      "metadata": {
        "id": "lq2ae6EbQvwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgRlQ4X4Jfj4"
      },
      "source": [
        "### Add personal possessive pronouns\n",
        "\n",
        "We're getting pretty close. We are still missing a NP at the start of the sentence: `my friends`. \n",
        " \n",
        "The word \"my\" has been tagged with `PRP$`. What does that tag mean? \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('PRP$')"
      ],
      "metadata": {
        "id": "vn2tiRmMx7XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `PRP$` tag is associated with personal possessive pronouns, which commonly pattern before nouns in English. We can easily integrated this into our existing grammar by including an optional `PRP$` in the same pattern we ask for determiners `DT`.\n",
        "\n",
        "Because `PRP$` includes a regex meta character (`$`), we need to `escape` the character with a `\\` so that the character is read literally, rather than as a special character. "
      ],
      "metadata": {
        "id": "5nD9lXgyyLlq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYVypxa2yex1"
      },
      "source": [
        "# update our grammar to let Vs take adjectives/adverbs\n",
        "grammar4 = \"\"\"\n",
        "N: {<DT|PRP\\$>?<JJ>*<N.*><IN>*} # adding PRP$ to DT, escaping the $\n",
        "V: {<V.*><JJ|RP>*}        # then create smaller verbs (can have adjectives/adverbs afterwards)\n",
        "VP: {<V><N>*<RP>*<NP>*}  # create our VPs\n",
        "NP: {<N><N>*<IN>*}         # create NPs after VPs are created - any N that did not get put into VP will become NP\n",
        "CLAUSE: {<NP><VP>}        # any adjacent NP + VP is a clause\n",
        "\"\"\"\n",
        "cp4 = nltk.RegexpParser(grammar4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is going on now? How well is this parsed? \n",
        "chunk_printer(cp4.parse(sent_tagged), chunks = [\"NP\", \"VP\", 'CLAUSE'])"
      ],
      "metadata": {
        "id": "Pvv2ksILysyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "We have fine-tuned a grammar based on our knowledge of phrase structure rules as well as an example sentence. \n",
        "\n",
        "- Try our grammar on some other examples. How accurate is it?\n",
        "- Do you see any problems with the rules? Any tweaks that should be made?\n",
        "- We didn't actually finish parsing the example above, as we missed out on the word \"like\" and the word \"Oh\" at the start. How important do you think these are to capture? "
      ],
      "metadata": {
        "id": "iBsO25N44O6o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD4PZPf-KHVe"
      },
      "source": [
        "## Does our grammar generalise?\n",
        "\n",
        "What next? We could continue this exploration by writing more functions which would let us \"ask\" our parser which nouns are associated within verb phrases, or check if certain verbs are in our text and then ask for the program to give us the nouns associated with those verbs, and so on. The book shows you some of this, and this is effectively what information extraction is all about. \n",
        "\n",
        "However, let's see how well our parser performs on some other text. How about a State of the Union address? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNIz2D9jKVW1"
      },
      "source": [
        "# download then import the state_union resource\n",
        "nltk.download('state_union')\n",
        "from nltk.corpus import state_union"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmiNuU8JKhfy"
      },
      "source": [
        "# check out Clinton's final state of the union speech\n",
        "clinton = state_union.sents('2000-Clinton.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmHzilteKtFo"
      },
      "source": [
        "# let's pick a random sentence from the clinton speech and parse it. \n",
        "clinton[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ztrZ29DLC6W"
      },
      "source": [
        "Look at the tagged text. What predictions can we make about the accuracy of our grammar?  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZuUPCHYK3PN"
      },
      "source": [
        "clinton_tagged = nltk.pos_tag(clinton[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYZVWGQNLAoB"
      },
      "source": [
        "clinton_tagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PItsjvgWLUK_"
      },
      "source": [
        "What happens when we try to parse this with our grammar?\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjFszu29LIPV"
      },
      "source": [
        "chunk_printer(cp4.parse(clinton_tagged), chunks = ['VP', 'NP', \"CLAUSE\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have found no clauses, only VPs and NPs. This means our clause rule is not meeting its condition. \n",
        "\n",
        "Why is that? Inspect the fully parsed version - basically, we have to work harder to connect our prepositional phrases between the VP and NPs."
      ],
      "metadata": {
        "id": "3_8ZxbsT42nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cp4.parse(clinton_tagged))"
      ],
      "metadata": {
        "id": "n2IZAssB6Far"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBVS4C9Ma3r"
      },
      "source": [
        "### **Open Challenge**\n",
        "\n",
        "Can you update our grammar to parse the above sentence more accurately? \n",
        "\n",
        "You'll probably find the best strategy is to to separately define a prepositional phrase, then update other rules to incorporate those phrases into NPs, then VPs, then clauses. There are likely some other tricks you can think of. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yet another generalisation attempt\n",
        "\n",
        "*(this is more bonus challenge - can you extend the grammar to work for this text?)*\n",
        "\n",
        "I'll do the same thing as above, but this time with the `grasshopper.txt.` I'll read in the grasshopper short story from the LING 226 GitHub. I use the `requests` module which let's me read in files from the internet. I point the `.get()` function at the direct URL for my text and then use the `.text` method to return the raw text. "
      ],
      "metadata": {
        "id": "4WkXGqvoVkYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests library\n",
        "import requests\n",
        "\n",
        "# use .get() to target direct URL and download the raw text\n",
        "grasshopper = requests.get('https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/texts/grasshopper.txt').text\n",
        "\n",
        "# inspect the results\n",
        "grasshopper"
      ],
      "metadata": {
        "id": "k04VRtUsKOdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize then tag\n",
        "grasshopper_tagged = nltk.pos_tag(nltk.word_tokenize(grasshopper))\n",
        "\n",
        "# inspect a slice of the text - do you see any NPs?\n",
        "grasshopper_tagged[:20]"
      ],
      "metadata": {
        "id": "we2M2WAJNYGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse our text for NPs\n",
        "grasshopper_parsed = cp4.parse(grasshopper_tagged)\n",
        "\n",
        "# inspect the results: \n",
        "chunk_printer(grasshopper_parsed, chunks = ['CLAUSE'])"
      ],
      "metadata": {
        "id": "vz6nFu70Oz3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitless Possibilities\n",
        "\n",
        "We've used the chunk parser to create rules adhering to the syntactic constraints of English. These analyses are facilitated by the fact we have an automatic tagger we can use in NLTK. \n",
        "\n",
        "But, you *could* decide to make your own tags and then build a parser to read those tags. \n",
        "\n",
        "Remember, you could use the `nltk.str2tuple()` function to convert a string in the form of `string/tag` into a string:tag combination of any meaning that you like. \n",
        "\n",
        "Then, using the `nltk.RegexpParser()`, you could develop a parser that reads those tags to make chunks. You could for example define language which code switches into chunks of different languages.\n",
        "\n",
        "Because strings are parsed left-to-right, and chunks are evaluated in the order they are made, I could explore this to find the sequences which are certain stretches of language in the text. \n",
        "\n",
        "> *This is just an idea off the top of my head which could use more refinement. the point is to show you that you can leverage this parser to do things insead of POS tags.*"
      ],
      "metadata": {
        "id": "an8Q284NqXjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_switching = '她/zh 是/zh 一/zh 个/zh 很/zh cool/en 的/zh 人/zh'\n",
        "\n",
        "code_switching_tagged = [nltk.str2tuple(w) for w in code_switching.split()]\n",
        "\n",
        "code_switching_tagged"
      ],
      "metadata": {
        "id": "kqEsY9TLknA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_switch_grammar= \"\"\"\n",
        "All_English: {<EN><EN>*} # required English chunk followed by any number of English chunks\n",
        "All_Chinese: {<ZH><ZH>*}\n",
        "\"\"\"\n",
        "\n",
        "code_switch_parser = nltk.RegexpParser(code_switch_grammar)"
      ],
      "metadata": {
        "id": "OcDmhZeYkZFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my goal was to find contiguous stretches of English and Mandarin\n",
        "print(code_switch_parser.parse(code_switching_tagged))"
      ],
      "metadata": {
        "id": "LWrjd71gmCvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try it on another text\n",
        "another_example = [nltk.str2tuple(w) for w in 'I/EN 懂/ZH 您/ZH 的/ZH meaning/EN'.split()]\n",
        "print(code_switch_parser.parse(another_example))"
      ],
      "metadata": {
        "id": "-CmAg5J8nYy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}