{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/07_Word_Frequencies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVxGe75vjJ1U"
      },
      "source": [
        "# Word Frequencies\n",
        "\n",
        "In this notebook we will learn how to explore the numerical distributions of words in a text - the relative frequencies of a word in a text. `Word frequency` represents the overall frequency of a word in general language use. It is a very interesting property of language because it correlates with other constructs, such as word length (shorter words are more frequent) and word difficulty (more complex words are less frequent).\n",
        "\n",
        "One of the interesting things about frequency is a phenomena called Zipf's law, which states that the most frequent word occurs at least twice as much as the second most frequent word, and this this relationship persists. You can read a [reddit post about it here](https://www.reddit.com/r/linguistics/comments/830nf5/zipfs_law_was_so_cool_that_i_performed_and/), or at least look at the person's graph they made explaining the phenomenon:\n",
        "\n",
        "\n",
        "<img src = https://www.etymologynerd.com/uploads/1/5/8/8/15888322/website.png height = \"300\">\n",
        "\n",
        "\n",
        "Moreover, counting the frequency in which words occur with *other* words has proven very insightful for linguistics and NLP. The most basic insight is that words tend to co-occur with other specific words in predictable ways. Corpus linguists call these pairs of words `collocations`, and define them using a variety of different statistical measures. Finding these larger collocational patterns has given strength to functional lingusitic theories of grammar such as construction grammar, which argue that both meaning and syntax determine the way a word is used in language (contrast this with a purely structural approach, which argues grammar rules exist independently of meaning).\n",
        "\n",
        "Word co-occurence statistics are also used to create co-occurence distributions and vector spaces - these are what large-scale NLP algorithms and artificial intelligence applications rely on for word predictions in both processing and production (more on that later!).\n",
        "\n",
        "The second half of NLTK Chapter 1 begins to introduce these important concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ALUFQLq0lw"
      },
      "source": [
        "## Frequency distributions\n",
        "\n",
        "\n",
        "The simpliest form of a frequency distribution is a count of how many times each word `type` appears in a text. It's worth pausing for a moment and considering how you might construct your own frequency distribution â€” what might be the steps for doing so? Here is one general approach you could take:\n",
        "\n",
        "1. You start a loop over some words\n",
        "2. At the first word, you note down the word and store it in a separate data container, alongside a value representing its frequency\n",
        "3. You then move to the next word and check if the next word already exists in your data container,\n",
        "      - if it does already exist, you increase its count by 1\n",
        "      - If it does not exist, you add it to the data container and set an initial count of 1\n",
        "\n",
        "Here is what that might look like using pseudocode:\n",
        "\n",
        "```\n",
        "output_container = []\n",
        "\n",
        "for word in my words:\n",
        "  if word in output_container\n",
        "    increase count of word + 1\n",
        "  else\n",
        "    add word to output_container\n",
        "    increase count of word + 1\n",
        "```\n",
        "\n",
        "Now, what kind of data container would make sense for this? A `list` might be able to work, but this would require some careful slicing and indexing and might become a pain. There is another data container better designed for this known as a dictionary. We will learn how to create dictionaries in a later lesson. But for now, we can rely on a built-in NLTK function named `FreqDist()`, which creates a dictionary of `value:frequency` pairs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `nltk.FreqDist()`\n",
        "\n",
        "We can pass a sequence (e.g., a string, a list, etc) to the `nltk.FreqDist()` function and it will count the number of times different values in the sequence occur. For example, we can count the frequency of letters in a word or words in a sentence.\n",
        "\n",
        "To do so, we simply pass whatever sequence we want as an argument to `nltk.FreqDist()`. Ideally, save the results to a variable.\n",
        "\n",
        "Run the cell below as an example:\n"
      ],
      "metadata": {
        "id": "msG5As1oZrFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the FreqDist from nltk\n",
        "from nltk import FreqDist\n",
        "\n",
        "# define a string containing multiple characters\n",
        "turtles = \"\"\"teenage mutant ninja turtles\n",
        "            teenage mutant ninja turtles\n",
        "            teenage mutant ninja turtles\n",
        "            heroes in a halfshell, turtle power\"\"\"\n",
        "\n",
        "# split the string into tokens/words:\n",
        "turtles_tokens = turtles.split()\n",
        "\n",
        "# save the frequency distribution to a variable\n",
        "turtle_fdist = FreqDist(turtles_tokens)\n",
        "\n",
        "# inspect the results\n",
        "turtle_fdist"
      ],
      "metadata": {
        "id": "_sfTYTOfaPCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting frequency distribution is another Python data object called a `dictionary` which stores `key:value` pairs. In this case, our keys are the words, and the values are the frequencies.\n",
        "\n",
        "We can query a dictionary for specific `key:value` pairs using the following syntax:\n",
        "\n",
        "> `dictionary['key']`\n",
        "\n",
        "This should look familiar, because it is similiar to how one can index characters in strings (e.g., `turtles[1]`) or words in lists (e.g., `['one', 'two'][0]`)\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "016Nks4Aadz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtles?\"\n",
        "turtle_fdist['turtles']"
      ],
      "metadata": {
        "id": "Gv8qr37a3uw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtle?\"\n",
        "turtle_fdist['turtle']"
      ],
      "metadata": {
        "id": "pCqS9l6l39yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what happens if we ask for a word not in the dictionary?\n",
        "# the NLTK FreqDist gives us a 0 rather than an error, which is handy!\n",
        "\n",
        "turtle_fdist['shredder']"
      ],
      "metadata": {
        "id": "f-f31JQybyo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also ask for the most frequent terms from a frequency distribution using the `.most_common()` method. We can specific the number of top results we want by putting a number in the brackets `()` used by `.most_common()`. The code below has a `3` in the brackets, so the function will return the top-three most frequent words in the frequency distribution."
      ],
      "metadata": {
        "id": "kMSZrRTE4Csl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the most common word?\n",
        "turtle_fdist.most_common(3)"
      ],
      "metadata": {
        "id": "swc9GHLj4GUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a search with frequency\n",
        "\n",
        "Lets calculate word frequencies for a larger, more interesting data set. Create a frequency distribution of the webchat corpus included with `nltk`, `text5` using `FreqDist()`. You'll need to import `nltk` and download the book resource:"
      ],
      "metadata": {
        "id": "TgOteauK4UU1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjC8cj2mq8eu"
      },
      "source": [
        "# import the main nltk module\n",
        "import nltk\n",
        "\n",
        "# download the nltk.book resources\n",
        "nltk.download('book')\n",
        "\n",
        "# import the resources\n",
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdwfZBrdXw_-"
      },
      "source": [
        "# Now create a FreqDist of the webchat text\n",
        "webchat_fdist = FreqDist(text5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWWHK6mCZurG"
      },
      "source": [
        "What are the 50 most common words in the webchat corpus? Examine the output - what do you see? Are there items in the output you did or did not expect? What do you think is happening?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Rj9MVHZuS4"
      },
      "source": [
        "webchat_fdist.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Q5rxIDaEp-"
      },
      "source": [
        "Let's now look at how people use the phrase \"lol\" - both the individual frequency and the overall percentage of \"lol\" in the corpus.\n",
        "\n",
        "What do you think about the results? 1.5% might seem low, but is actually a rather strong result considering how many possible words *could* be in the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxsvV-_OaGbB"
      },
      "source": [
        "# index the value by using the key (in this case, the word we want to check)\n",
        "webchat_fdist['lol']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXZeygy7aM8z"
      },
      "source": [
        "# divide the frequency of 'lol' by the total length of the corpus, then multiply by 100\n",
        "webchat_fdist['lol']/len(text5)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZpSoCv8cwOt"
      },
      "source": [
        "We can now include word frequency as an additional condition when looking for certain words. Do you recall how list comprehensions and conditional for loops worked? For example, if we wanted to ask for all words which are three letters long:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ibJEPgeJLq"
      },
      "source": [
        "# all tokens which are 3 letters long (list comprehension)\n",
        "# this says: give me every word in text5 but only if the length of the word is equal to 3\n",
        "[w for w in text5 if len(w) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurxcAVLeQaW"
      },
      "source": [
        "The output is not very readable, is it? We are getting every single token which is 3 characters long, including repetitions. We can reduce this firstly by wrapping the list comprehension in `set()` so that we get a list of types, rather than tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcEV-c0FeEGI"
      },
      "source": [
        "# add set()\n",
        "set([w for w in text5 if len(w) == 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUvNzOCYesJI"
      },
      "source": [
        "If you look through that output, you can see that there are a lot of things that look like codes or other non-word stuff, usually in UPPERCASE. We can try removing those using `.islower()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZvWxzUpdNh0"
      },
      "source": [
        "# all tokens which are 3 letters long and all characters are lowercase\n",
        "# give me the set of all words in text5 if the word is 3 characters long and each character is in lower case\n",
        "set([w for w in text5 if len(w) == 3 and w.islower()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PZ0dP1LejH6"
      },
      "source": [
        "Now it's getting more manageable. It's still quite a long list though. Let's add another condition - asking for the same output as the previous code, but this time setting a minimum frequency. We can embed a FreqDist as part of the condition.  Let's also adjust our length so that we let both 3 and 4 letter words appear.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxfbvIxRd4De"
      },
      "source": [
        "# adding minimum frequency, allow for both 3 and 4 letter words (how else could you write that conditional?)\n",
        "\n",
        "# give me the set of all words in text 5 if the word is 3 or 4 letters long, and is lower case, and occurs over 100 times in the fredist\n",
        "set([w for w in text5 if len(w) <= 4 and len(w) >= 3 and w.islower() and webchat_fdist[w] > 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa5o-hh_f62F"
      },
      "source": [
        "What do you see in that output? Any words stand out as representative of a chat corpus? What kinds of words do you think you will find using the same criteria but on a different corpus? The point, which was made in the NLTK book regarding the length of words, is that a single line of code with the right tuning can provide relatively precise insight into the nature of a text and/or corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Spend some time to play around with one of the other built-in texts (`text1` through `text8`) from the NLTK data.\n",
        "\n",
        "Your goal is to try and refine some search patterns to find words which seem to capture the nature of the different texts. For example, you could think about a minimum frequency and minimum or maximum length, such as I have done with `text3` above.\n",
        "\n",
        "You can see what the name of each text is by typing `text1-9` into a cell and running it, for example:"
      ],
      "metadata": {
        "id": "l9PaxO1LkKe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# typing just the text's id tells you the actual document.\n",
        "text6"
      ],
      "metadata": {
        "id": "G19htjRHk8IO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}