{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/exercise_set_01_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex6kUJfeAXhQ"
      },
      "source": [
        "# Exercise set 01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aycnAyW1KIi3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0RxgHOIU76e"
      },
      "source": [
        "☼ 5. Compare the lexical diversity scores for humor and romance fiction in 1.1. Which genre is more lexically diverse?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ceZf_hQU7PD"
      },
      "source": [
        "# Humour and fiction is more diverse because it has a higher TTR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhtuO0-QVFdP"
      },
      "source": [
        "☼ 6. Find the collocations in `text5`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yuUhVS2VGy3"
      },
      "source": [
        "# Your code here\n",
        "text5.collocations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFMimMTJVOvp"
      },
      "source": [
        "☼ 10. Define a variable `my_sent` to be a list of words, using the syntax `my_sent = [\"My\", \"sent\"]` (but with your own words, or a favorite saying).\n",
        "\n",
        "Use `' '.join(my_sent)` to convert this into a string.\n",
        "Use `split()` to split the string back into the list form you had to start with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv0wUs3uVQ-m"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "my_sent = ['Soda', 'and', 'Diet', 'chased', 'a', 'rabbit.']\n",
        "\n",
        "my_sent2 = ' '.join(my_sent)\n",
        "\n",
        "print(my_sent2)\n",
        "\n",
        "my_sent3 = my_sent2.split()\n",
        "\n",
        "print(my_sent3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHNwD4UsVXQ8"
      },
      "source": [
        "☼ 12. Consider the following two expressions, which have the same value. Which one will typically be more relevant in NLP? Why?\n",
        "```\n",
        "\"Monty Python\"[6:12]\n",
        "[\"Monty\", \"Python\"][1]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apVjzNl2VlgD"
      },
      "source": [
        "# Your typed answer here\n",
        "\n",
        "print(\"Monty Python\"[6:12])\n",
        "\n",
        "print(['Monty', 'Python'][1])\n",
        "\n",
        "# the list will be more relevant because we want to operate on words, not letters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V3rxF15Vgdl"
      },
      "source": [
        "☼ 13. We have seen how to represent a sentence as a list of words, where each word is a sequence of characters. What does `sent1[2][2]` do? Why? Experiment with other index values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpllYR0iVi6x"
      },
      "source": [
        "# Your code here\n",
        "sent1[2][2]\n",
        "\n",
        "# Your typed answer here\n",
        "# you are indexing deeper into structures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZZ7oM6WFk6"
      },
      "source": [
        "◑ 17. Use `text9.index()` to find the index of the word `sunset`. You'll need to insert this word as an argument between the parentheses. By a process of trial and error, find the slice for the complete sentence that contains this word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwP9VRpZWHaH"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "text9.index('sunset')\n",
        "\n",
        "' '.join(text9[621:644])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d4rMZbhWZGS"
      },
      "source": [
        "◑ 19. What is the difference between the following two lines? Which one will give a larger value? Will this be the case for other texts?\n",
        "\n",
        " \t\n",
        "`sorted(set(w.lower() for w in text1))` \n",
        "\n",
        "`sorted(w.lower() for w in set(text1))`\n",
        "\n",
        "\n",
        "TEXT ANSWER: \n",
        "\n",
        "(This question is asking you to think what `set()` is doing in each version of the code. The first version first creates a lower case set of all words in the text, whereas the second version creates a set BEFORE applying lower case, therefore there will naturally be more words because words like \"The\" and \"the\" will be counted as separate words). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w95mL-sRlMK"
      },
      "source": [
        "len(sorted(set(w.lower() for w in text1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AZYNPIcRwZu"
      },
      "source": [
        "len(sorted(w.lower() for w in set(text1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmiahw41WkWd"
      },
      "source": [
        "◑ 22. Find all the four-letter words in the Chat Corpus (`text5`). With the help of a frequency distribution (`FreqDist`), show these words in decreasing order of frequency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRU50wu8Wltf"
      },
      "source": [
        "# Your code here (hint, use sorted with reverse = True)\n",
        "\n",
        "four_letter_words = [w for w in text5 if len(w) == 4]\n",
        "\n",
        "four_letter_words_fd = FreqDist(four_letter_words)\n",
        "\n",
        "#print(four_letter_words_fd.most_common(10))\n",
        "\n",
        "# need to use reverse = True to do descending order\n",
        "sorted(four_letter_words_fd, reverse = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZh_a0JjWw_s"
      },
      "source": [
        "◑ 23. Review the discussion of looping with conditions in 4. Use a combination of `for` and `if` statements to loop over the words of the movie script for Monty Python and the Holy Grail (`text6`) and print all the uppercase words, one per line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rBt-zhIW17C"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "for w in text6:\n",
        "  if w.isupper():\n",
        "    print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26euVANXBNq"
      },
      "source": [
        "◑ 24. Write expressions for finding all words in text6 that meet the conditions listed below. The result should be in the form of a list of words: `['word1', 'word2', ...].`\n",
        "\n",
        "- Ending in ise\n",
        "- Containing the letter z\n",
        "- Containing the sequence of letters pt\n",
        "- Having all lowercase letters except for an initial capital (i.e., titlecase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6zAdrmZWFxx"
      },
      "source": [
        "[w for w in text6 if w.endswith('ise')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfN9_yREWQea"
      },
      "source": [
        "[w for w in text6 if 'z' in w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us_cBPGTWVdB"
      },
      "source": [
        "[w for w in text6 if 'pt' in w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37GgE2NnW_Nt"
      },
      "source": [
        "# is there a better way to do this one? \n",
        "[w for w in text6 if len(w) > 1 and w[0].isupper() and w[1].islower()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj_td7i7DeKP"
      },
      "source": [
        "# Exercise set 02\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VOgDOzcT7lJ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OK5jgahugs6"
      },
      "source": [
        "4. ☼ Read in the texts of the State of the Union addresses, using the state_union corpus reader. Count occurrences of men, women, and people in each document. What has happened to the usage of these words over time?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0HbtryMujg8"
      },
      "source": [
        "# your answer here\n",
        "\n",
        "# to solve this we first need to figure out how to represent this over time - we need the year. We can find that in the file id\n",
        "print(nltk.corpus.state_union.fileids())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJEZ7eaBmvc3"
      },
      "source": [
        "# now we can follow the same approach used for the inagural speech\n",
        "# use nltk.ConditionalFreqDist\n",
        "# they use a really long list comprehension which is nested and not in brackets\n",
        "from nltk.corpus import state_union\n",
        "su_cfd = nltk.ConditionalFreqDist(\n",
        "    (target, fileid[:4]) \n",
        "    for fileid in state_union.fileids()\n",
        "    for w in state_union.words(fileid)\n",
        "    for target in ['men', 'women', 'people']\n",
        "    if w.lower().startswith(target))\n",
        "\n",
        "su_cfd.tabulate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVysR1cCm36O"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (20, 10))\n",
        "su_cfd.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJgdJYYGm0Ho"
      },
      "source": [
        "# Here is what the loop is doing. \n",
        "for file in state_union.fileids():\n",
        "  year = file[:4]\n",
        "  for w in state_union.words(file):\n",
        "    if w in ['man', 'woman', 'people']:\n",
        "      print(year, w, end = ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdyxdxDXBQNy"
      },
      "source": [
        "7. ☼ According to Strunk and White's *Elements of Style*, the word `however`, used at the start of a sentence, means \"in whatever way\" or \"to whatever extent\", and not \"nevertheless\". They give this example of correct usage: [However you advise him, he will probably do as he thinks best.](http://www.bartleby.com/141/strunk3.html) Use the concordance tool to study actual usage of this word in the various texts we have been considering. See also the LanguageLog posting [\"Fossilized prejudices about 'however'.\"](http://itre.cis.upenn.edu/~myl/languagelog/archives/001913.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQOlkpt7BQ4H"
      },
      "source": [
        "# this one is annoying because you have to remember how to get the text into the right format\n",
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma.concordance(\"however\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8_qOdZ5CaYW"
      },
      "source": [
        "# contrast with a different corpus\n",
        "for file in state_union.fileids():\n",
        "  nltk.Text(state_union.words(file)).concordance('however')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ohmMMODEo3F"
      },
      "source": [
        "# as list comprehension\n",
        "print([nltk.Text(state_union.words(i)).concordance('however') for i in state_union.fileids()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IROdBwQ-M05m"
      },
      "source": [
        "15. ◑ Write a program to find all words that occur at least three times in the Brown Corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-d7CTmwM2mx"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown_fd = nltk.FreqDist(w.lower() for w in brown.words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krExpdCWOCx9"
      },
      "source": [
        "brown_df = nltk.FreqDist(brown.words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aBlqNCFOMVp"
      },
      "source": [
        "triple_words = [w for w in brown_df.keys() if brown_df[w] > 2]\n",
        "print(len(brown.words()))\n",
        "print(len(triple_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mIkcyo3OYfa"
      },
      "source": [
        "16. ◑ Write a program to generate a table of lexical diversity scores (i.e. token/type ratios), as we saw in Chapter 2 Section 1.1. Include the full set of Brown Corpus genres (`nltk.corpus.brown.categories()`). Which genre has the lowest diversity (greatest number of tokens per type)? Is this what you would have expected?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps0hS5QlOb52"
      },
      "source": [
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text)\n",
        "for genre in nltk.corpus.brown.categories():\n",
        "    print(genre + ': ' + str(lexical_diversity(brown.words(categories = genre))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glx1Cy2cXZ80"
      },
      "source": [
        "19. ◑ Write a program to create a table of word frequencies by genre, like the one given in Chapter 2, Section 1 for modals. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings. \n",
        "\n",
        "NOTE: Use the Brown corpus and its built-in genres.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IyYtosXXbF9"
      },
      "source": [
        "# I can only assume they want us to use the brown corpus because it has the genre categories\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "  (genre, word)\n",
        "  for genre in brown.categories()\n",
        "  for word in brown.words(categories = genre))\n",
        "\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "\n",
        "target_words = ['space', 'war', 'fight', 'happy', 'sad', 'man', 'woman', 'child']\n",
        "\n",
        "cfd.tabulate(conditions = genres, samples = target_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbam5O70YHeY"
      },
      "source": [
        "20. ◑ Write a function `word_freq()` that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm7HKbHkYKLH"
      },
      "source": [
        "# This is actually a really nice program to make :)\n",
        "def word_freq(word, section):\n",
        "  words = brown.words(categories = section)\n",
        "  fdist = nltk.FreqDist(w.lower() for w in words if w == word)\n",
        "  return fdist\n",
        "\n",
        "word_freq('war', 'romance')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}