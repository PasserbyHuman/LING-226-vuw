{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP4LCRSlJ1mQNDdjDoKBhss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/14_doing_more_with_dictionaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nujejxMHIXY"
      },
      "source": [
        "# Doing more with Dictionaries\n",
        "\n",
        "If you've inspected the `nltk.FreqDist()` and `nltk.ConditionalFreqDist()` functions, you'll see they store data as dictionaries. We have already covered dictionaries in general. By now you should see that dictionaries are quite useful for storing linguistic information, which NLTK refers to as mapping from one thing to another (e.g., mapping a POS tag to a word). In this notebook we will return to dictionaries as a refresher, and also learn about extensions to dictionaries used by NLTK. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IBVGAg-fWQT"
      },
      "source": [
        "# download necessary resources for this notebook\n",
        "import nltk\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets', 'treebank', 'universal_tagset', 'book'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, we can manually create a dictionary using a pair of curly braces `{}`. Below, I define a dictionary where words are keys and POS tags are the values:"
      ],
      "metadata": {
        "id": "46w0qoxWq94B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-uqRa48LUMI"
      },
      "source": [
        "# manually create a dictionary\n",
        "pos = {}\n",
        "pos['colorless'] = 'ADJ'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU-7GaS-LaGM"
      },
      "source": [
        "# add entries which are word:pos_tag key/value pairs\n",
        "pos['ideas'] = 'N'\n",
        "pos['sleep'] = 'V'\n",
        "pos['furiously'] = 'ADV'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LAihy4jLaIq"
      },
      "source": [
        "# inspect our dictionary\n",
        "pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r22Tgz6UL_rm"
      },
      "source": [
        "# Using list() will give all the keys\n",
        "list(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvzluQt6Mo67"
      },
      "source": [
        "## DefaultDict\n",
        "\n",
        "A `defaultdict` is an extension of Python dictionaries from the `collections` module (the same module which gave us `namedtuple`). Learning how to use `defaultdict` is helpful if you are performing operations where you want automatic values set to keys which do not yet exist in a dictionary. \n",
        "\n",
        "\n",
        "\n",
        "First of all, consider what happens when we ask a dictionary for a key that does not exist. I'll create a dictionary with fictional frequency values for two words.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a small dictionary with word:pos_tag key/values\n",
        "fake_frequencies = {'the': 1337, 'dog': 42}\n",
        "fake_frequencies"
      ],
      "metadata": {
        "id": "OKiAV2Ny0oOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# everything is fine when I ask for a key already in the dictionary. \n",
        "fake_frequencies['the']"
      ],
      "metadata": {
        "id": "516E9IQ406o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but if I ask for a key not in the dictionary, we get a key error\n",
        "fake_frequencies['ran']"
      ],
      "metadata": {
        "id": "1N2Mx_8J07fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a `defaultdict` allows you to be a bit more defensive and avoid errors when looking up values that might not exist. And, by extension, allows us to think about how this might be useful for things like frequency dictionaries — if a word does not exist in our frequency dictionary, we could assume that it has a frequency of zero. Same thing with POS tags or other lexical information — rather than finding out that something is not in the dictionary, it would be preferable to update the dictionary with a value indicating the lack of such information. \n",
        "\n",
        "When you create a default dictionary, you can specify the default value (or function) which will trigger if a value does not exist in the dictionary. In the cell below, I import `defaultdict` and then specify the default value for a key not in the dictionary will be an `int`. Because I do not supply any other information about the `int`, the default value will be `0`. \n",
        "\n"
      ],
      "metadata": {
        "id": "CflZ2VDR1AyG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBivI72hMrCc"
      },
      "source": [
        "# import defaultdict\n",
        "from collections import defaultdict\n",
        "\n",
        "# create a defaultdictionary where default values are `int` of 0.\n",
        "frequency = defaultdict(int)\n",
        "\n",
        "# add a key/value to the dictionary\n",
        "frequency['colorless'] = 4\n",
        "\n",
        "# inspect key/value\n",
        "frequency['colorless']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chskn44KQrT-"
      },
      "source": [
        "# now look up something not in the dictionary\n",
        "# the key was not there, so was added with the DEFAULT value of int = 0\n",
        "frequency['ideas']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to understand why `int` gives us zero:\n",
        "int()"
      ],
      "metadata": {
        "id": "7HajDfcgAdha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wanted to set the default value to be a specific number, you can use (`lambda: value`) as the default argument:\n"
      ],
      "metadata": {
        "id": "AphIEEmL_sgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I use an anonymous function (lambda) to set default value to a number that is not zero. \n",
        "any_integer = defaultdict(lambda: 1337)\n",
        "\n",
        "any_integer['test']"
      ],
      "metadata": {
        "id": "QlV7VwSe_uSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extend this to other informatoin such as parts of speech. Below, I indicate the `defaultdict` should have an empty `list` as the default value, and the same process occurs when we look up words not in the dictionary. \n",
        "\n",
        "Words with no POS would be added to the dictionary, and functions such as `len()` could be used to determine whether they do or do not have POS and update them accordingly. "
      ],
      "metadata": {
        "id": "WJ27hHC33HLj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp1sRUpBQjbw"
      },
      "source": [
        "# default is an empty list for tag sets\n",
        "pos = defaultdict(list)\n",
        "# add a key value\n",
        "pos['sleep'] = ['NOUN', 'VERB']\n",
        "# works as intended\n",
        "pos['sleep']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a word not in the dictionary is updated to default value (an empty list)\n",
        "pos['green']"
      ],
      "metadata": {
        "id": "_LFVME6L0W-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzweb02BRIMM"
      },
      "source": [
        "For something like parts of speech, we might actually want to have a default POS tag instead of an empty list. Again we can use the `lambda` function to supply a default POS tag. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXvLaS-PRN9v"
      },
      "source": [
        "# tell defaultdict that the defaul POS tag is \"NOUN\"\n",
        "pos = defaultdict(lambda: 'NOUN')\n",
        "\n",
        "pos['colorless'] = 'ADJ'\n",
        "pos['colorless']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1_S1V1qRUjK"
      },
      "source": [
        "# if an entry doesn't exist it is added to the dictionary \n",
        "# there are other ways to do this using the basic dictionary type as well\n",
        "pos['blog']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSyb1-1lRR_4"
      },
      "source": [
        "# remember that list() can be used to inspect \n",
        "list(pos.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVmApnNLc2NF"
      },
      "source": [
        "## **Tagging unknown words**\n",
        "\n",
        "Now, instead of supplying a default tag such as \"NOUN\" (which could be dangerous), NLTK shows how labels such as \"unknown\" can be used to tag words which are \"out of vocabulary.\" \n",
        "\n",
        "Below, we read in *Alice in Wonderland* from the Gutenberg data built into NLTK. We access the `.words` of the book to get the tokenized list, and then create a frequency distribution of those words using `nltk.FreqDist()`. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3CuMs1Mc4Mk"
      },
      "source": [
        "# create a frequency distribution of all the words from alice in wonderland\n",
        "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
        "alice_vocab = nltk.FreqDist(alice)\n",
        "\n",
        "# take a peek at some of the most common words\n",
        "alice_vocab.most_common(100)[90:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next code cell, we use a `list comprehension` to extract the most frequent 1000 words, and we also toss away the frequency values (because we just want the words). "
      ],
      "metadata": {
        "id": "sI5UdqPjCqdS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2T_5kqrfN3t"
      },
      "source": [
        "# gather the most frequent 1000 tokens\n",
        "alice_top_1000 = [word for (word, frequency) in alice_vocab.most_common(1000)]\n",
        "alice_top_1000[90:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create a default dictionary in which the default tag will be 'UNK' for unknown. "
      ],
      "metadata": {
        "id": "OOKo9ymqDfww"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAYvA3z9dcTj"
      },
      "source": [
        "# create default dict with a default of \"UNK\" if not in the dict.\n",
        "from collections import defaultdict\n",
        "alice_known = defaultdict(lambda: 'UNK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then add our top 1000 words to the dictionary, and in doing so, will make both the key and the value the word. We do so using a for loop which loops over each word and sets that word as they key and the value. \n",
        "\n",
        "Why are we doing this? We are simulating a list of \"known\" words. Any word which has itself as the value is considered \"known.\" "
      ],
      "metadata": {
        "id": "BxOR3gqVD2KW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je8tLdTKdrBJ"
      },
      "source": [
        "# add each word from top 1000 as itself to the dictionary\n",
        "for v in alice_top_1000:\n",
        "  alice_known[v] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyDzEAHxgVNo"
      },
      "source": [
        "# make sure you understand what is going on here. \n",
        "list(alice_known.items())[90:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can perform a `list comprehension` which loops over all of the words in the book (contained in the variable `alice`). This will simulate reading the book one word at a time. \n",
        "\n",
        "The list comprehension is asking for the value of each word from our default dictionary `alice_known`. Remember, we've already added the top 1000 words to this dictionary, so if they are checked, they will return themselves. \n",
        "\n",
        "For the words in `alice` which are *not* in `alice_known`, the default dictionary will return our unknown tag \"UNK\".\n",
        "\n",
        "The end result is a list of tokens the same length as `alice`, but any word not in the top 1000 more frequent words will be replaced with \"UNK\". "
      ],
      "metadata": {
        "id": "4LziKbUEEMt1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSvtZiYqdIeW"
      },
      "source": [
        "# create a new vocab which then looks at all the words in alice (not just the top 1000)\n",
        "alice_complete = [alice_known[v] for v in alice]\n",
        "\n",
        "# anything not already in the top 1000 words gets assigned to UNK\n",
        "alice_complete[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the two versions - because the words `Lewis` and `Carroll` were not in the top 1000 most frequent words, they are replaced with `UNK`"
      ],
      "metadata": {
        "id": "UkDwIKM6IrZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract just the first 10 real words from each version\n",
        "first_10_raw = ' '.join([v for v in alice[1:10]])\n",
        "first_10_processed = ' '.join([v for v in alice_complete[1:10]])\n",
        "\n",
        "# print them for comparison\n",
        "print(f'Original version:\\n {first_10_raw} \\n Processed version:\\n {first_10_processed}')"
      ],
      "metadata": {
        "id": "ooxO7NajGzEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We can confirm the two lists of tokens are the same lengths(i.e., have the same number of words: "
      ],
      "metadata": {
        "id": "5grnPmJDIZcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(alice)"
      ],
      "metadata": {
        "id": "UxBYwgobIUhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(alice_complete)"
      ],
      "metadata": {
        "id": "r80kP4FFIVaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in the end, this was a really convoluted method which replaced all of the words in the book that are *not* in the top 1000 most frequent words with the tag \"UNK\". \n",
        "\n",
        "\n",
        "You are probably wondering, why might we ever want to do something like this? It of course depends on the goals of your analysis. Extracting the most-frequent 1000 words from a text may in turn include a good portion of the text. In fact, if we ask that question about our text here, we can see that the 1000 most-frequent words account for over 90% of all the words in the book. "
      ],
      "metadata": {
        "id": "4tHV4UDTIomZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all words that have a value in our known values default dict\n",
        "in_vocab = [w for w in alice if w in alice_known.values()]\n",
        "\n",
        "# all those that do not have values in the default dict\n",
        "out_vocab = [w for w in alice if w not in alice_known.values()]"
      ],
      "metadata": {
        "id": "wVDID9_9Ku_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate proportion based on total words for top 1000\n",
        "len(in_vocab) / len(alice) * 100"
      ],
      "metadata": {
        "id": "MCLut85vL37w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same for those without\n",
        "len(out_vocab) / len(alice) * 100"
      ],
      "metadata": {
        "id": "e-VtS-9MMOS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done by looking at the top 1000 most frequent words is reduced the search space for any task or procedure we might want to perform on this text. The NLTK book points out that a procedure such as this can help with Part of Speech tagging, because it means a tagger would not need to consider any word tagged with \"UNK\" and thus increase accuracy and performance. \n",
        "\n",
        "We can also use such information and compare the top 1000-most frequent words in this text to other texts and/or word lists as a way to assess topics or similarity between two texts, or assess the overall difficulty of the vocabulary by comparing this list to lists of word frequency built on even larger corpora. "
      ],
      "metadata": {
        "id": "w0AsVgPRMiGJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5VaS0x7eO3i"
      },
      "source": [
        "## Incrementally updating a dictionary\n",
        "\n",
        "The NLTK book shows us how to use a very handy operator, the `+=` which iterates a value by a set value. Most commonly we see this used as `+= 1`, which means increase something by 1. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkv3sez1f4MN"
      },
      "source": [
        "# set the value 1 to the variable a\n",
        "a = 1\n",
        "\n",
        "# increment the value by 1\n",
        "a += 1\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose any number we like as the units of increment:"
      ],
      "metadata": {
        "id": "HQk1Q88wOScv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbusAXnqf5qR"
      },
      "source": [
        "# same as above but increment by 1000 instead of 1\n",
        "a = 1\n",
        "a += 1000\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method for incrementing can be used to update information each time something is encountered, such as increasing the frequency counts of words, pos tags, or anything else we are interested in counting during a loop procedure. \n",
        "\n",
        "For example, the NLTK book shows us that we might be interested in counting the total incidence of different Part of Speech tags in the Brown corpus, and shows us how to do it. \n",
        "\n",
        "We create a `defaultdict` named `counts` and set the default to `int` (which means 0). \n",
        "\n",
        "We then import `brown`, and loop through the tagged version of the corpus. The loop checks the tag in `counts` and increments the value by 1. Because the default value of `counts` will be 0, the very first time a tag is found, that value will increment to 1, and so on. \n",
        "\n"
      ],
      "metadata": {
        "id": "Oxkq7JtaOyNp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQg2xgxUftcE"
      },
      "source": [
        "# create the default dictionary with default of 0\n",
        "counts = defaultdict(int)\n",
        "\n",
        "# read in brown corpus\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# NLTK authors use the += operator to increment the tag count by 1 each time it is seen.\n",
        "for (word, tag) in brown.tagged_words(categories = 'news', tagset = 'universal'):\n",
        "  counts[tag] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04TOZTc78X3o"
      },
      "source": [
        "# the dictionary is frequency count of POS tags\n",
        "counts.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do you have any ideas why this might be the most frequent tag? \n",
        "counts['DET']"
      ],
      "metadata": {
        "id": "FXwhsUXLPohY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If-tNpr_-Ty_"
      },
      "source": [
        "## **Anagram dictionary**\n",
        "\n",
        "Although it is very brief the NLTK book talks about finding anagrams using a `defaultdict`. I've tried to add some more detail here to make it clear how this function works and what it is doing to the words. \n",
        "\n",
        "Basically, more than one word might include all of the same letters, albeit in a different order. By alphabetically sorting the letters of all words, any word with the same letters will be associated with the same alphabetic pattern, which can then be used as a common key for different words.\n",
        "\n",
        "For example, the words `heart` and `earth` all use the same letters but in a different sequence. If we used `sorted()` on both strings, they give us the same result:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# both words are the same when you sort them!\n",
        "print(''.join(sorted('earth')))\n",
        "print(''.join(sorted('heart')))"
      ],
      "metadata": {
        "id": "FS3Xyw9gQsDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXDdKriM-c8p"
      },
      "source": [
        "# import NLTK's giant list of English words\n",
        "words = nltk.corpus.words.words('en')\n",
        "words[123456:123460]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlHLXjVZ-nC7"
      },
      "source": [
        "# create anagrams as a default dict with list as the default\n",
        "anagrams = defaultdict(list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCh1RK7k-1hu"
      },
      "source": [
        "# loop over each word (e.g., 'earth')\n",
        "for word in words:\n",
        "  # the KEY will be the sorted version (e.g., 'aehrt')\n",
        "  key = ''.join(sorted(word))\n",
        "  # append the VALUE to that key (e.g., 'earth')\n",
        "  anagrams[key].append(word) # each new word which has the same key will be added to this value (using list.append())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz89265zAJ9H"
      },
      "source": [
        "# this might help you see what's going on here \n",
        "# we can find any pattern which has more than a certain number of words - here I choose more than 6\n",
        "# although, some of these words sure seem odd to me!\n",
        "for key in anagrams:\n",
        "  if len(anagrams[key]) > 6: # play around with this value to get different results. \n",
        "    print(key, ':', anagrams[key])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7oggLDzBL2P"
      },
      "source": [
        "# `nltk.Index`\n",
        "\n",
        "As the authors of NLTK seem fond of doing, they briefly introduce a custom way for doing the same thing using a special NLTK version. Honestly, don't worry too much about this, basically they are just providing their own version of `defaultdict`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iycKa_KBWQh"
      },
      "source": [
        "help(nltk.Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM7V2XLnBSKN"
      },
      "source": [
        "anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)\n",
        "# choose one that's funny\n",
        "anagrams['aeilnrt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook we considered an extension to dictionaries which allows for default values. The main benefits of using such dictionaries for our purposes is to count various properties of words, such as word frequency and parts of speech. The convenience of using `defaultdict` is that we don't have to account for missing keys when constructing such objects. \n",
        "\n",
        "There are a number of other demonstrations in this chapter which can be interesting to study but are not necessary to understand for our purposes. "
      ],
      "metadata": {
        "id": "L-iE-aQPY6yj"
      }
    }
  ]
}