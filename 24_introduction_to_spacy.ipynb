{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNjnqUMmfFVdNVGZk1r+2BF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/24_introduction_to_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UTTY79qfcb3"
      },
      "source": [
        "# **spaCy**\n",
        "\n",
        "There are other libraries in Python which we can use to perform linguistic analysis. One of these is called spaCy, you can [view their website to get all the details](https://spacy.io/), but the goal of spaCy is to provide a flexible and arguably easier way to do many of the things we have been doing with NLTK. In this manner, spaCy can tokenize and tag your text, just like NLTK can, and can also perform automatic noun phrase chunking, named entity recognition, and allow you to customize the entire process, and mostly in a more streamlined manner when compared to NLTK.\n",
        "\n",
        "So, if one were only concerned with using a package that will provide a quick and accurate parse of some text data, spaCy is a good choice (and you might want to look at other libraries, such as [AllenNLP](https://allennlp.org/) or transformers [Huggingface](https://huggingface.co/course/chapter1/1)). \n",
        "\n",
        "You might be thinking *well why the hell have we been learning NLTK this whole time?!*, and that's a good question!\n",
        "\n",
        "The simple answer is that NLTK has worked us through a lot of linguistic concepts that aren't covered in the modern NLP libraries in the same manner. NLTK included corpus linguistics, WordNet, CMU dictionary, etc...And, we haven't even touched a lot of the advanced things in NLTK, or the later chapters. In this sense, NLTK might be the library that a *linguist* uses when doing computational linguistics, since they can store and analyse their own custom grammars, etc. It might also be a bit old fashioned. \n",
        "\n",
        "For people who may not identify as pure linguists (or even impure linguists) but still want to do NLP, spaCy and other NLP libraries allow for access to what is likely the most basic needs in NLP – parsing a text using linguistic information. Now that you have read through and thought about some of the concepts in NLTK, you should have a better understanding of what these more advanced libraries are doing, should you continue to use them. However, one huge difference between NLTK and more modern NLP libraries is the data that they use to perform tagging, chunking, and parsing. Specifically, one reason spaCy and the newer libraries perform so well is because they have different language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKt4qJ-iiYjA"
      },
      "source": [
        "# **spaCy and language models**\n",
        "\n",
        "What exactly is a language model? Conceptually, one way to think of language models is to picture a huge set of word co-occurance probabilites and/or matrices. These models take into account the distributional properites of words over a huge amount of input and thus \"learn\" which words go with other words in certain contexts. We actually have trained our own small language models when we used the `.bigrams()` function in NLTK and generated some text, as well as what the models created in the word probabilities notebook. In fact, `ngram` models are a form of language model (and is what NLTK uses) and have been widely using in NLP tasks. Newer language models use more advanced methods for calculating these word co-occurance probabilities.\n",
        "\n",
        "**spaCy has models for languages other than English**\n",
        "\n",
        "You can test spaCy out for different language models using their demos (wait a few seconds for each page to load)\n",
        "\n",
        "[Here is an example in English](https://explosion.ai/demos/displacy?text=Victoria%20University%20of%20Wellington&model=en_core_web_sm&cpu=1&cph=1)\n",
        "\n",
        "[Here is an example in Mandarin Chinese](https://explosion.ai/demos/displacy?text=你准备好了吗&model=zh_core_web_sm&cpu=0&cph=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kwIAVg3nYO0"
      },
      "source": [
        "# **basic spaCy usage**\n",
        "\n",
        "You can take a free online class offered by the creators of spaCy [here](https://course.spacy.io/en/), so please use that material if you find spaCy to be cool and want to use it in the future. There is also a nice [Real Python](https://realpython.com/natural-language-processing-spacy-python/) tutorial for spaCy, and likely others floating around out there. \n",
        "\n",
        "In this notebook I'll show you how to perform some of the same functions using spaCy that we've already done with NLTK. \n",
        "\n",
        "The first thing we do with spacy is import the library and then load a language model. You can see all the available language models [here](https://spacy.io/usage/models). \n",
        "\n",
        "If you want to run spaCy on your local machine, you will first have to pip install spaCy and then manually download models (e.g., in the terminal run `python -m spacy download en_core_web_sm`). This is all explained on their website, should you be interested.\n",
        "\n",
        "For English, spaCy provides a small, medium, and large model, which reflects the search space that the models were trained on and the amount of word vectors that exist (i.e., larger means more information about word distribution). We can get by using the small model, but the larger models might be useful once you start performing more advanced tasks with spaCy. \n",
        "\n",
        "If you want to install a larger language model and are using Colab, you can use pip to install them this way:\n",
        "\n",
        "```\n",
        "# download the larger english model (should only need to do this once per notebook session)\n",
        "!python -m spacy download en_core_web_lg\n",
        "```\n",
        "For now, we'll stick with the default small language model. \n",
        "\n",
        "In the cell below, I import spaCy and then create a variable `nlp` from the small model. The use of `nlp` as a variable name is arbitrary, but the creators of spaCy use it in all their examples and suggest that it is best practice to use `nlp`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1awe3K4oMvI"
      },
      "outputs": [],
      "source": [
        "# import spacy and save the parser to a variable\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQPp69WookL2"
      },
      "source": [
        "The `nlp` function is now very simple to use. We pass raw text directly to the `nlp` variable, using it as a function, to create parsed text. We should save the text to a variable to see all of the options we now have access to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDoNvScroq_u"
      },
      "outputs": [],
      "source": [
        "parsed_text = nlp('The sea was angry that day my friends. Like an old man sending back soup in a deli.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwuXaEAbo1kx"
      },
      "source": [
        "Calling parsed text just gives us the text back, but..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6irAI9TUozSC"
      },
      "outputs": [],
      "source": [
        "parsed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfL2DDxKo6Ll"
      },
      "source": [
        "What we have done is create a [`Doc`](https://spacy.io/api/doc) object, which is a parsed text using the spaCy model we chose. The `Doc` object will have a ton of built-in features we can use to extract various pieces of linguistic information. Unlike NLTK, the `Doc` object will already contain tokens, parts of speech, noun chunks, and more. You can see a [full description of spaCy's linguistic features](https://spacy.io/usage/linguistic-features). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADova_6o4Ru"
      },
      "outputs": [],
      "source": [
        "# confirm that we now have a specific spaCy object\n",
        "type(parsed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrkyATXmvXe7"
      },
      "source": [
        "The `Doc` object contains tokens, sentences, chunks, and many other sources of information, all by calling that one function above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIChu_p1v7-9"
      },
      "outputs": [],
      "source": [
        "# You can get sentences...\n",
        "list([sent for sent in parsed_text.sents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVMvdF2gv_gV"
      },
      "outputs": [],
      "source": [
        "# You can get noun chunks...\n",
        "list([chunk for chunk in parsed_text.noun_chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knVcj-wBwE4R"
      },
      "outputs": [],
      "source": [
        "# And of course we can get tokens!\n",
        "list([token for token in parsed_text])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example"
      ],
      "metadata": {
        "id": "_ac4GbiRf7d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# anything innacurate in this output?\n",
        "\n",
        "nin = nlp('my moral standing is laying down')\n",
        "\n",
        "for token in nin: \n",
        "  print(token, token.tag_, token.pos_)"
      ],
      "metadata": {
        "id": "02KSqzZwf91t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "\n",
        "Load in some text and parse it using spaCy. Make sure you can write some list comprehensions or other methods for finding the attributes of the tokens for a `Doc` objects."
      ],
      "metadata": {
        "id": "ZpBcJXwFePy0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7s_BUtm1-4V"
      },
      "source": [
        "# Token Attributes\n",
        "\n",
        "spaCy provides a lot of information with each token, called `attributes`, which you can access using the `.` notation. For instance, we can quickly create tuples with the word and POS tag for the text. We do so with the `token.tag_` method. Notice that underscore after `tag` – spaCy stores information as both numeric and readable, and we use the underscore to get the readable version. (You can try using the `.tag` without the underscore to see what happens).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atVJLYjM2ihr"
      },
      "outputs": [],
      "source": [
        "# print word/tag tuples\n",
        "# tag_ will give us tags similar to Penn Treebank\n",
        "list([(token, token.tag_) for token in parsed_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4ElvTuC-TKR"
      },
      "outputs": [],
      "source": [
        "# pos_ will give us more generalized part of speech\n",
        "# notice that we just have NOUN, ADJ, etc, instead of NN, NNS, etc.\n",
        "list([(token, token.pos_) for token in parsed_text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhjh5ser-eqz"
      },
      "source": [
        "So, that's something we can also do with NLTK, and rather easily, but spaCy has even more syntactic information compared to NLTK, and parses it automatically. That's what we'll explore for the rest of this notebook. You can also explore [the full list of token attributes here.](https://spacy.io/api/token#attributes)\n",
        "\n",
        "I'll write a quick function which gives us a ton of information for a spaCy doc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ask for a series of different token attributes. \n",
        "\n",
        "def super_spacy(text_input):\n",
        "  parsed = nlp(text_input)\n",
        "  # create a crappy header\n",
        "  print('token\\tPOS\\tsimplePOS\\tstopword?')\n",
        "  # output token, detailed pos, simple pos, and whether the word is a stopword\n",
        "  for token in parsed:\n",
        "    # need two tabs for stopword\n",
        "    print(f'{token}\\t{token.tag_}\\t{token.pos_}\\t\\t{token.is_stop}')"
      ],
      "metadata": {
        "id": "hWu7Lr2CW1DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_spacy('We live in a society!')"
      ],
      "metadata": {
        "id": "VHLEVoxwXiqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "\n",
        "1. Explore some other token attributes by looking at the spaCy website. What applications can you think of for the various tokens? \n",
        "\n",
        "2. Compare the tags and information provided by spaCy to similar functions in NLTK. Which one seems more accurate? Make sure to explore a variety of texts/inputs. \n",
        "\n"
      ],
      "metadata": {
        "id": "Vsh6kBHkayvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `token.similarity`\n",
        "\n",
        "spaCy also has a (somewhat crude) way to measure semantic similarity among two words. Remember how we did this using WordNet? In WordNet, similarity was a function of how close concepts were in terms of hypernym/hyponym hierarchical category structures. \n",
        "\n",
        "spaCy calculates similarity different using a construct known as **word vectors**. Word vectors are a numerical representation of words and/or topics in a text, and there are a number of ways to calculate them. One relatively straigtforward way is through word co-occurance probabilities, such as the GloVe model developed by Stanford NLP. [You can read about how these vectors work here.](https://nlp.stanford.edu/projects/glove/)\n",
        "\n",
        "To calculate similarity is easy, you just need to use the `.similarity` method of a `Doc` on another `Doc`. However, you should probably only do this for single tokens, rather than phrases or sentences. THis is because if there is more than one token, spaCy will just take the average distance of all the words in one sentence from all the words in the other sentence, resulting in always high similarity. If you're really interested in this problem, you could [read my paper here](https://escholarship.org/content/qt5fd5r301/qt5fd5r301.pdf) where I used someone else's spaCy pipeline to use sentence rather than token similarity. \n",
        "\n",
        "Anyhow, see below for some examples to use similarity. "
      ],
      "metadata": {
        "id": "vnhrAr2eT-gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a basic similiarity function\n",
        "\n",
        "def spacy_sims(word1, word2):\n",
        "  return nlp(word1).similarity(nlp(word2))"
      ],
      "metadata": {
        "id": "bhW6p54_R6Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will get a warning about how the model is only using tensors instead of vectors. Don't worry about it for now. You could manually download the large English model to get \"better\" similarity scores. "
      ],
      "metadata": {
        "id": "TDAbDFYLScRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animals = ['dog', 'cat', 'horse']\n",
        "\n",
        "# why is a cat the most similar to giraffe??\n",
        "for animal in animals:\n",
        "  print(animal, '\\n',  spacy_sims('giraffe', animal))"
      ],
      "metadata": {
        "id": "P8sTcBeeSDHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "birds = ['robin', 'tui', 'penguin']\n",
        "\n",
        "# robin is the most hawk-like of our birds?\n",
        "for bird in birds:\n",
        "  print(bird, '\\n', spacy_sims('hawk', bird))"
      ],
      "metadata": {
        "id": "1I065A-nS4mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "\n",
        "1. Does the similarity function make sense to you? What sorts of applications might this information be useful for? \n",
        "2. Compare the spaCy similarity functions to the NLTK WordNet `.path_similarity()` measures of similarity. Which one seems more accurate/reliable? "
      ],
      "metadata": {
        "id": "RsfGYeZBe5-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading in additional languages from spaCy\n",
        "\n",
        "If we have time, here is a short tutorial showing you how to load in and use models for other languages besides English. \n",
        "\n",
        "You can find a [list of the available models for different languages in spaCy here](https://spacy.io/usage/models).\n",
        "\n",
        "You will see a drop down list of different languages, and choosing any one language provides you with instructions on how to load a different language model. For example, here is what you see when you select Chinese:\n",
        "\n",
        "\n",
        "<img src = https://i.imgur.com/qkTSkvn.png height = \"500\" width = \"600\">\n",
        "\n",
        "Based on that advice, I should be able to load in the Chinese model using the following code:"
      ],
      "metadata": {
        "id": "JNY-bPCZUOPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load(\"zh_core_web_sm\")"
      ],
      "metadata": {
        "id": "gdiCPpGDUQM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, if I try to do this in Colab, I get this error:\n",
        "\n",
        "<img src = https://i.imgur.com/cdioO3Z.png>\n",
        "\n",
        "This error occurs because we have not downloaded the Chinese language model - basically the same thing that happens when we need to work with some NLTK resources which are not available to us. \n",
        "\n",
        "To download language models, we need to use the code supplied in the screenshot above this one:\n",
        "\n",
        "```\n",
        "python -m spacy download zh_core_web_sm\n",
        "```\n",
        "\n",
        "This is easy to do on a local computer with a terminal prompt, but we need to do something slighty different for Colab - we can put an exclamation mark in front of the code to have Colab run the code as a terminal command rather than a Python. If you do this in Colab, you will see something like this:\n",
        "\n",
        "\n",
        "<img src = https://i.imgur.com/OKufvyA.png>\n",
        "\n",
        "After downloading the model, you should then be able to load it as per the spaCy documentation: "
      ],
      "metadata": {
        "id": "5K-_Bdy7URto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you would need to do this each time you load a notebook\n",
        "\n",
        "!python -m spacy download zh_core_web_sm"
      ],
      "metadata": {
        "id": "kepiYpa5UkcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# success!\n",
        "nlp = spacy.load(\"zh_core_web_sm\")"
      ],
      "metadata": {
        "id": "FvVfG5JiUS-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a new language model loaded, we can use spaCy in the exact same way as we have been using, except now it will accept Chinese as input. "
      ],
      "metadata": {
        "id": "cFf1Q6qkUUMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save a Mandarin sentence as a string\n",
        "ni_hao = \"大家好。你们喜不喜欢这个课?\"\n",
        "\n",
        "# print each token, the token part of speech, and the token's language\n",
        "for token in nlp(ni_hao):\n",
        "    print(token, token.pos_, token.lang_)"
      ],
      "metadata": {
        "id": "0jquMkYaUVOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this in mind, you could potentially expand your final project analysis to a language beyond English.\n",
        "\n",
        "One downside is that you are limited to the range of existing language models offered by spaCy, so it may not contain the language you are most interested in. "
      ],
      "metadata": {
        "id": "v-kahNSTUW9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "\n",
        "1. Pick a language other than English from the list of language models and parse some text. Are there any different token attributes compared to English? You can try using `dir()` on a token to see the possible attributes.\n",
        "2. If you do not want to try using another language, try downloading and comparing the performance of the small and large English language models. "
      ],
      "metadata": {
        "id": "1ui7nomnfOXW"
      }
    }
  ]
}