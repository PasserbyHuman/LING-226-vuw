{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/29_Word_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_9jfAPWDYyx"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "In its most basic form, sentiment analysis tries to measure the overall positivity and negativity from the tone of a text, with the idea that you are able to capture the opinion and/or mood of the author(s). You might also see this referred to as *valence* or *polarity*. The classic examples of sentiment analysis usually discuss how one can use sentiment analysis to detect overall negative versus positive tone in movie or product reviews.\n",
        "\n",
        "A lot of sentiment analysis libraries are rule-based, which means that the creator of the resources has spent time trying to program the best set of rules to analyse language for these features.\n",
        "\n",
        "## Where does sentiment come from?\n",
        "\n",
        "Where does one obtain measures of positivity and negativity? In many cases the sentiment ratings are obtained from human perceptions of how positive or negative individual words are in isolation. These lexicons or wordlists will include high frequency content words (in particular adjectives, which should make sense), and store them in a manner where each word has a \"score\" indicating how positive or negative it is. These scores differ in how they are done - for instance someone could rate a word from 0 to 1, with 0 being negative and 1 being positive. Or one could use a Likert scale from 1 negative  to 7 positive, or one could gather votes for whether a word is negative or positive from a variety of people and then let the feature with the most votes \"win.\" The point is, there are a few different approaches to capturing these perceptions.\n",
        "\n",
        "## Who is giving these ratings?\n",
        "\n",
        "Crowdsourcing is one method, where researchers can hire people on platforms such as Amazon Mechanical Turk to provide ratings for words. This allows for rapid and cheap data annotation (in fact Amazon Mechanical Turk has been called \"artificial artificial intelligence\" because so many annotation tasks were originally farmed out to workers on that platform). In this way, we can view the most simple form of sentiment analysis as a lexical resource, similar to the names corpus, WordNet, and a variety of other things we have already explored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TNDL7rSoDtZ"
      },
      "source": [
        "# AFINN Sentiment Lexicon\n",
        "\n",
        " Many sentiment lexicons are literally lists of words with scores. For example, I downloaded the [AFINN Sentiment Lexicon](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html). Inside the folder are three files: an older version of the list, a newer version of the list, and a README file explaining the resource. Here is the description from the README file:\n",
        "\n",
        "\n",
        "*AFINN is a list of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. The file is tab-separated. There are two versions.*\n",
        "\n",
        "*FINN-111: Newest version with 2477 words and phrases.*\n",
        "\n",
        "*AFINN-96: 1468 unique words and phrases on 1480 lines. Note that there are 1480 lines, as some words are listed twice. The word list in not entirely in alphabetic ordering.*\n",
        "\n",
        "The readme even provides the code for how to import the resource into Python, cool :)\n",
        "\n",
        "Let's play with the AFINN lexicon. I've uploaded the file `AFINN-111.txt` to the course GitHub repository so it can be downloaded directly from there. I'll use the `requests` library to ask for the file via a URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJA8YTU6NfdB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# save the URL as a string to a variable\n",
        "afinn_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical-resources/AFINN-111.txt'\n",
        "\n",
        "# call the url and ask for the .text\n",
        "AFINN = requests.get(afinn_url).text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FvVGqMAoqcM"
      },
      "source": [
        "Look at the AFINN file - it's literally a single string containing a word then a rating, but it also has those funky `\\t` and `\\n` characters in there, which mean TAB and NEWLINE, respectively. Ugh, we have to figure out a way to load this text in (let's pretend the authors of AFINN didn't give us a solution. I'm also not going to use their solution, although there is nothing wrong with it).\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leySXvE8ofVN"
      },
      "outputs": [],
      "source": [
        "# just reading in AFINN gives us this, explore the first 500 characters\n",
        "AFINN[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ziTmDnpX_I"
      },
      "source": [
        "\n",
        "We know that we can use `.split()` on a string to split a string wherever we like, with the default being whitespace. So we can use `.split('\\n')` to split the string on newlines. Let's do that with our initial call to the resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd5zr5FzpghK"
      },
      "outputs": [],
      "source": [
        "AFINN = requests.get(afinn_url).text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccUQLyxDplMj"
      },
      "outputs": [],
      "source": [
        "# AFINN but now split on the newline characters.\n",
        "AFINN[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXY-UMN2qR2O"
      },
      "source": [
        "Now the resource is is a list where each element is a string in the form of `Word\\tRating`. We can exploit this structure to loop through each string and use `.split()`\n",
        "\n",
        "Let's follow the advice of the AFINN author and turn this into a dictionary. We want to create a dictionary, and then add each word as a key and the rating as a value. One way, and perhaps an easy way to do this, is to create a list comprehension first and then run `dict()` on the list comprehension. I'll break down the steps for you here, but you could probably figure out a way to do this in one line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF-K9NSirEq3"
      },
      "outputs": [],
      "source": [
        "# Use a list comprehension to split each word into a word/rating tuple\n",
        "afinn_list = [pair.split('\\t') for pair in AFINN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jhgjg6_roOt"
      },
      "outputs": [],
      "source": [
        "# Check it out - each element is now the word and the rating!\n",
        "afinn_list[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54V8z0mpB88K"
      },
      "source": [
        "A remaining issue is that our numbers are represented by strings, but we want the numbers to be numbers, so we can calculate averages, etc. Let's change `afinn_list`  so that each rating is converted using `int()`. (We could have done this in one go above, but splitting it here to break down the process.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFdulPCHB-HU"
      },
      "outputs": [],
      "source": [
        "# convert the rating to int() using another list comprehension\n",
        "afinn_list = [(word, int(rating)) for word, rating in afinn_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWjzIHrUCVeE"
      },
      "source": [
        "Now let's put the list into a dictionary so that we can access word entries and their ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7jX8qLVrtfb"
      },
      "outputs": [],
      "source": [
        "# chuck that list into a dictionary\n",
        "afinn_dict = dict(afinn_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLX1dFfp7xZX"
      },
      "outputs": [],
      "source": [
        "# it works!\n",
        "afinn_dict['happy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfT6-OtWtQYy"
      },
      "source": [
        "We can now start thinking about the values associated with the words. Remember, the author of AFINN has told us that words range from -5 (negative) to +5 (positive). Look at the examples I've chosen, do their ratings make sense to you? Also, the word \"banana\" is not in the lexicon – think about it for a second – does that make sense? What polarity would *you* assign to the word banana?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxjZb21CrxZo"
      },
      "outputs": [],
      "source": [
        "# we can now look up words in the dictionary to get their ratings\n",
        "test_words = ['ironic', 'tired', 'happy', 'banana', 'alive', 'hurt']\n",
        "\n",
        "# loop through words, check if in dict, if so, print word and rating\n",
        "for word in test_words:\n",
        "  if word in afinn_dict.keys():\n",
        "    print(word, afinn_dict[word])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f4sHubkCbiE"
      },
      "source": [
        "If you were to loop through the dictionary and search for words with a rating of negative 5, you will find some words which are offensive and vulgar. Of course, we have to remain somewhat dispassionate here, because we would expect these words to exist in natural language and thus attempt to account for them in any analysis of natural language. I'll leave it to you to write and execute such a loop, but fair warning you will see some words which might offend you.\n",
        "\n",
        "```\n",
        "# print words with -5 or 5 as a rating\n",
        "for word in afinn_dict.keys():\n",
        "  if afinn_dict[word] == 5 or afinn_dict[word] == -5:\n",
        "    print(word, afinn_dict[word])\n",
        "```\n",
        "\n",
        "Instead, I'll pick out some of the less offensive words (at least, I think so!) and show you what counts as fully negative or fully positive. There are actually fewer then 20 words with such extreme ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duEuvW5GtiiQ"
      },
      "outputs": [],
      "source": [
        "# these words have a -5 or +5 rating\n",
        "extreme_words = ['bastard', 'twat', 'superb', 'thrilled']\n",
        "\n",
        "for word in extreme_words:\n",
        "  print(word, afinn_dict[word])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMDBzuw7E-YA"
      },
      "source": [
        "## Using the AFINN sentiment scores\n",
        "\n",
        "Let's see how one could use this resource, and whether or not the output makes sense.\n",
        "\n",
        "Let's start with two extremely different sentences which we would expect to be very negative or very positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV3Oi6ViFYfD"
      },
      "outputs": [],
      "source": [
        "positive_sentence = 'That meal was excellent.'\n",
        "negative_sentence = 'That meal was awful.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMuZg1ByF-dp"
      },
      "source": [
        "Now let's write a function which will calculate the AFINN polarity for each word in the sentence, and then average that value. This should give us the average sentiment for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7g6_SxdG8lH"
      },
      "outputs": [],
      "source": [
        "# we'll need nltk for word_tokenize (which itself needs punkt)\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hKBJ_ETGJa4"
      },
      "outputs": [],
      "source": [
        "def afinn_average(sentence, afinn):\n",
        "  \"\"\"calculates average afinn sentiment for a given string\"\"\"\n",
        "\n",
        "  # tokenise a lower-cased version of our sentence\n",
        "  tokens = nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "  # initalise empty output to store sentiment ratings\n",
        "  output = []\n",
        "\n",
        "  for token in tokens:\n",
        "    # first check if the token is even in the dictionary\n",
        "    if token in afinn.keys():\n",
        "      #print(token) # for seeing which words are actually counting\n",
        "      # add the rating to the output if so\n",
        "      output.append(afinn[token])\n",
        "\n",
        "  # we calculate the average sentiment of the text from all the values in our output\n",
        "  if output:\n",
        "    avg_sentiment = sum(output)/len(output)\n",
        "  # let's inform the user how many words from the sentence were actually used in the calculation\n",
        "  # this is a measure of \"coverage\"\n",
        "    print(f'sentiment is {avg_sentiment},\\ncalculation used {round(len(output)/len(tokens)*100, 2)}% of words,\\nwhich is {len(output)} words total')\n",
        "  else:\n",
        "    print('Nothing in this text was in the sentiment dictionary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eANRBPHSGy3Q"
      },
      "outputs": [],
      "source": [
        "# calculate the average of the positive sentence\n",
        "afinn_average(positive_sentence, afinn_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lolMZjSZHVT7"
      },
      "outputs": [],
      "source": [
        "# caluclate the average of the negative sentence\n",
        "afinn_average(negative_sentence, afinn_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok1Qd_VcHb3S"
      },
      "source": [
        "Seems to be working, however, you probably have realised that these \"average\" values are actually only taking sentiment from one word in each sentence. The function skips the words `that meal was` because none of those words were in the `afinn_dict`. This is an issue related to **coverage** – the performance of a parser or lexical resource is largely dependent upon how much of the text can actually be analysed. In the two examples above, our resource only provided coverage over 20% of each text.\n",
        "\n",
        "Keep in mind, punctuation is still counted in those tokens, so our length variable will also be slightly different depending upon how we will deal with punctuation.\n",
        "\n",
        "Let's see what happens if we combine our sentences. If we join our sentences  (with a space in between), we see that the calculations are still based on the same 2 words, this is still 20% of the text, but our text's sentiment is now 0. This is because the -3 rating of `awful` and the +3 rating of `excellent` are summing to zero.\n",
        "\n",
        "So, in this sense, a \"neutral\" text is simply a text which does not have a strong tendency towards positive or negative, but we can clearly see that there exists both positive and negative sentiment in the text!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMOqjL6ILj_e"
      },
      "outputs": [],
      "source": [
        "# the positive and negative words cancel each other out\n",
        "afinn_average(positive_sentence + ' ' +  negative_sentence, afinn_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWQu8ZjkLfZK"
      },
      "source": [
        "Let's look at a larger text to see if adding words helps here.\n",
        "\n",
        "This text is a satirical story from *The Civilian*, a New Zealand satirical newspaper which seems to no longer be around :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK2Lpp9RHbGL"
      },
      "outputs": [],
      "source": [
        "xmas_threat = \"\"\"While Auckland prepares to move to Alert Level 3 on Wednesday,\n",
        "New Zealanders in the fun part of the country have been wondering when they might see a drop down to Level 1,\n",
        "given the persistent lack of Covid cases since the outbreak.\n",
        "But the Government isn't currently considering any such plans,\n",
        "and Prime Minister Jacinda Ardern poured cold water on the idea at today's post-Cabinet press conference,\n",
        "reiterating that Level 2 may last for “some time”\n",
        "and that a move to Level 1 won't be considered for at least as long as there remains\n",
        "a threat that organisers decide to go ahead with annual kids' concert Christmas in the Park.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXocB2HkKV1-"
      },
      "outputs": [],
      "source": [
        "afinn_average(xmas_threat,afinn_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR7DjaZqKol6"
      },
      "source": [
        "Ok, so our results are saying that, on average, this text is more negative than positive (because a neutral text would have a rating of 0). At the same time, the entire sentiment of the text is based on only 5 words — we can check which words those are:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmB3atJ7bXjx"
      },
      "outputs": [],
      "source": [
        "# let's scrutinize the exact words being used for the calculations here.\n",
        "for word in nltk.word_tokenize(xmas_threat):\n",
        "  if word.lower() in afinn_dict.keys(): # need to lower to check properly\n",
        "    print(word, afinn_dict[word.lower()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ttt4GJocANv"
      },
      "source": [
        "### Identifying the limitations of a lexicon-based approach\n",
        "\n",
        "Ok, so we see that the overall sentiment in that text is dictated by the average sentiment of `alert, fun, drop, lack, threat`. We see the same problem here that occured above — the sentiment of the text is being driven by a very small number of words.\n",
        "\n",
        "While it does make sense that specific words, namely **content** words such as adjectives, adverbs, verbs, and nouns are doing the heavy lifting to colour the overall sentiment of a text, it's a bit frustrating to see so few words being included in the lookup lexicon. This is again a matter of coverage and a limitation of any lexicon-based approach.\n",
        "\n",
        "These results also identify a second problem with a lexicon-based approach. Each word is being treated in isolation, without consideration of the words that come before or after. As we have seen with POS tagging and WordNet, a single word can take on possible different meanings and senses, and knowing which meaning/sense/part of speech is intended relies on knowing how the word is *used* - how it patterns with *other* words. Our sentiment lexicon does not capture this.\n",
        "\n",
        "For example, look at that sentence which contains the word `lack`:\n",
        "\n",
        "`given the persistent lack of Covid cases since the outbreak.`\n",
        "\n",
        "Is this sentence negative? One would think not, a *lack* of COVID cases is probably an overall *good* thing (unless someone doubts the authenticity of such a report, but let's just keep it simple for now). So, while the decontextualized meaning of `lack` is more or less negative (although even that is debatable), we see that it is referring to the lack of a bad thing – which we might want to reasonably argue as a good thing. This, hopefully, is starting to show you that word-based lexicons must be carefully used and interpreted, because words do not create meaning on their own...their meaning is also dependent upon the ways they are used with other words.\n",
        "\n",
        "I hope that this shows you the limitations of using a lookup approach like this. That does not mean that this is a terrible approach to use, but rather we must be careful with such an approach. Certain texts will be more suitable for this type of analysis, and parsers with better rules will also help.\n",
        "\n",
        "To finalise this point a bit more, also consider the topic of negation in English. Saying that something is *not* good means that it is bad. And saying something is \"not bad\" usually means that it is at least good, right? The AFINN dictionary cannot take this iniformation into account:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p32p9o-0dbjI"
      },
      "outputs": [],
      "source": [
        "# something that is not terrible should be good?\n",
        "afinn_average('This is not terrible', afinn_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ceybRU-dhm0"
      },
      "outputs": [],
      "source": [
        "# something that is not supberb should be bad?\n",
        "afinn_average(\"this is not superb\", afinn_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUtK86lqAjnb"
      },
      "source": [
        "How could we resolve this? We could write a new set of rules which looks for negation words before our sentiment words. This sort of appoach and the limitations of existing sentiment libraries such as AFINN and many others is what prompted CJ Hutto to make a better rule-based sentiment library named VADER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmSEbLFdgt7L"
      },
      "source": [
        "# Sentiment Analysis with VADER\n",
        "\n",
        "VADER (Valence Aware Dictionary for sEntiment Reasoning) was created to address the shortcomings with some of the lexicon approaches to sentiment analysis. If you read or even skimmed the paper and other links associated with VADER, you can see the author provides a rationale for why VADER is \"better,\" even though VADER is still rule-based!\n",
        "\n",
        "The author of VADER incorporated information from a variety of prior sentiment dictionaries, and then also thought about features such as capitalization, emoticons, non-standard words, negation, and a variety of other ways that people actually *use* language.  \n",
        "\n",
        "And, VADER is relatively simple to use in NLTK :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6-GQilrMzlV"
      },
      "source": [
        "Let's get VADER into our notebook, first we import `nltk` and then download the resource, per usual. The resource we need is `vader_lexicon`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls3_nS9dM3an"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "import nltk\n",
        "# download the vader lexicon\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqJNw9ieRxmL"
      },
      "source": [
        "Now we want to create a sentiment analyser function, to do so, we import the anlayzer from vader and save it to a variable name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jFnonM8M_Bf"
      },
      "outputs": [],
      "source": [
        "# Import the vader sentiment analyzer and save to the variable `sid`\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq1TKKMRSGzJ"
      },
      "source": [
        "We are interested primarily in two things from the `sid` object:'\n",
        "\n",
        "- `lexicon`\n",
        "- `polarity_scores`\n",
        "\n",
        "Let's peek at the `lexicon` part of this. It's another dictionary with valence ratings for individual words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoM9yOu0SqWc"
      },
      "outputs": [],
      "source": [
        "# the lexicon is a dictionary!\n",
        "type(sid.lexicon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbo5tAiM7xZc"
      },
      "outputs": [],
      "source": [
        "# words have valence measures just like the AFINN dictionary.\n",
        "\n",
        "sid.lexicon['irony']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8QRGK0FTeun"
      },
      "source": [
        "So, we could simply use this lexicon in the same way that we did for AFINN above. In fact, when I made `afinn_average`, I added a second argument to allow any dictionary to be used. Let's redo the function now but make it clear it can take any dictionary. This is identical to the program above except I have replaced `afinn` with `sentiment_dict` and changed the function name to `sentiment_lookup`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPmWuwaWTuBf"
      },
      "outputs": [],
      "source": [
        "def sentiment_lookup(sentence, sentiment_dict):\n",
        "  \"\"\"calculates average sentiment for a given sentence\"\"\"\n",
        "\n",
        "  # tokenise a lower-cased version of our sentence\n",
        "  tokens = nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "  # initalise empty output to store sentiment ratings\n",
        "  output = []\n",
        "\n",
        "  for token in tokens:\n",
        "    # first check if the token is even in the dictionary\n",
        "    if token in sentiment_dict.keys():\n",
        "      # add the rating to the output\n",
        "      output.append(sentiment_dict[token])\n",
        "\n",
        " # we calculate the average sentiment of the text from all the values in our output\n",
        "  if output:\n",
        "    avg_sentiment = sum(output)/len(output)\n",
        "  # let's inform the user how many words from the sentence were actually used in the calculation\n",
        "    print(f'sentiment is {avg_sentiment},\\ncalculation used {round(len(output)/len(tokens)*100, 2)}% of words,\\nwhich is {len(output)} words total')\n",
        "  # we need to prevent a division error if there are no words in the dictionary.\n",
        "  else:\n",
        "    print('Nothing in this text was in the sentiment dictionary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_3TGGbQUCc1"
      },
      "source": [
        "Let's check out how well this VADER dictionary does on our xmas threat text, and compare it to AFINN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtJ9-SUpTkO-"
      },
      "outputs": [],
      "source": [
        "# reminder of what the text is\n",
        "xmas_threat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw6_DhYXUIKv"
      },
      "outputs": [],
      "source": [
        "# using AFINN lexicon\n",
        "sentiment_lookup(xmas_threat, afinn_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SeSgaFGUK58"
      },
      "outputs": [],
      "source": [
        "# using VADER lexicon\n",
        "sentiment_lookup(xmas_threat, sid.lexicon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10DMzEg-USy1"
      },
      "source": [
        "Interesting! Both dictionaries only used five words, but we get different scores (although both are negative). Let's again scutinize what words are contributing here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE5TsDmzUYGQ"
      },
      "outputs": [],
      "source": [
        "# afinn\n",
        "for word in nltk.word_tokenize(xmas_threat):\n",
        "  if word.lower() in afinn_dict.keys():\n",
        "    print(word, afinn_dict[word.lower()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8-ySTQzUZs9"
      },
      "outputs": [],
      "source": [
        "# vader\n",
        "for word in nltk.word_tokenize(xmas_threat):\n",
        "  if word.lower() in sid.lexicon.keys():\n",
        "    print(word, sid.lexicon[word.lower()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgFgg4jTUgPZ"
      },
      "source": [
        "Welp, looks like they are *the exact same words*, but the values are different. This is because VADER was initially created by looking up the previous sentiment dictionaries and tweaking the values based on an assessment of prior dictionaries and lexical resources as well as human annotation. So the hard-coded sentiments in VADER are \"better\" in some regard. But that's not the only improvement VADER has made. There are a number of additional rules that VADER considers in order to consider the ways words are being used to calculate their sentiment. For instance, VADER can better account for negated uses of words:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiK8oTZOWe5t"
      },
      "source": [
        "## VADER is more than a lookup dictionary\n",
        "\n",
        "Above I used VADER as a lookup dictionary the exact same way as AFINN. But, VADER is more than a lookup dictionary. VADER also has a bunch of other rules which are part of its scoring algorithm that take into account the contexts words appear in. You can scan the [rules here](https://www.nltk.org/_modules/nltk/sentiment/vader.html). You'll see certain rules, such as declaring certain word as intensifiers and also taking negation into account.\n",
        "\n",
        "To get the *real* polarity scores, you use the main VADER function, `polarity_scores`, to obtain sentiment scores that have taken these additional rules into account. The function will return a set of values: `neg`, `neu`, `pos`, and `compound`.\n",
        "\n",
        "VADER takes into account various punctuation and capitalization features, which means we actually want to pass raw strings to VADER.  \n",
        "\n",
        "> `sid.polarity_scores('string input')`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbAt72UW7xZe"
      },
      "outputs": [],
      "source": [
        "sid.polarity_scores(\"You can't beat Wellington on a good day.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhK-lNr67xZe"
      },
      "source": [
        "Consider how two additions change the ratings (an intensifer and an exclamation point):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz99xUvI7xZe"
      },
      "outputs": [],
      "source": [
        "sid.polarity_scores(\"You can't friggen beat Wellington on a good day!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_vfY4P87xZe"
      },
      "source": [
        "## Understanding VADER output\n",
        "\n",
        "The output in the examples above have a \"negative\" score, a \"neutral\" score, and a \"positive\" score. For both texts, the negative score is 0 the neutral is the highest, and the positive score is about .3. This suggests both versions of the statement are not negative, but more neutral or positive. However, the value we *rerally* we care about is the \"compound\" score which takes into account a variety of extra rules the author of VADER has incorporated into the program. You can find the scoring system [here](https://github.com/cjhutto/vaderSentiment#about-the-scoring), which says that:\n",
        "\n",
        ">>> \"The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\"\n",
        "\n",
        "The following guidelines apply to the compound score:\n",
        "\n",
        "* positive sentiment: compound score >= 0.05\n",
        "* neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
        "* negative sentiment: compound score <= -0.05\n",
        "\n",
        "So, the above sentiment is best seen in the compoud scores for the sentences of .44/.49, suggesting they are both overly positive. Do you agree? :)\n",
        "\n",
        "Let's see what VADER says about our *Christmas in the Park* text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGFVWqYt7xZe"
      },
      "source": [
        "## VADER and negation\n",
        "\n",
        "We saw above that the inclusion of the word `friggen` and the use of an exclamation mark enhanced the positivity of the sentence. Another thing VADER takes into account is negation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRwOnY7yYEdU"
      },
      "outputs": [],
      "source": [
        "# happy is being negated, so it should be negative\n",
        "sid.polarity_scores('I am not happy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIUx9Gt-YIKl"
      },
      "outputs": [],
      "source": [
        "# no negation, so happy should be positive.\n",
        "sid.polarity_scores('I am happy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FcyVQ9WBpH3"
      },
      "source": [
        "Just to remind you, look how poorly the AFINN dictionary performs using basic look up and averaging. It says this sentence is *positive* because it is naively looking at the word `happy` and not considering the surrounding words. Clearly, VADER is providing a better understanding of the sentiment in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEs2vBO-BdiN"
      },
      "outputs": [],
      "source": [
        "# compare the \"negated\" sentence with AFINN\n",
        "sentiment_lookup('I am not happy', afinn_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdM8ZStT7xZf"
      },
      "source": [
        "## VADER and emoticons\n",
        "\n",
        "VADER also has some support for parsing the sentiment of emoticons. Look how the frowny face makes the word \"Thanks\" neutral, whereas the happy face makes \"Thanks\" more positive than just \"Thanks\" alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWNixyiN7xZf"
      },
      "outputs": [],
      "source": [
        "# explore how emoticons are parsed by VADER\n",
        "thanks = ['Thanks', 'Thanks :(',  'Thanks :)']\n",
        "\n",
        "for thank in thanks:\n",
        "    print(f'Score for {thank} is: {sid.polarity_scores(thank)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Xwr-VA7xZf"
      },
      "outputs": [],
      "source": [
        "# basically, the emoticons are just more words with their own valence\n",
        "emoticons = [':)', ':(', ':D']\n",
        "for emoticon in emoticons:\n",
        "  print(emoticon, sid.lexicon[emoticon])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnfbY0rW7xZf"
      },
      "source": [
        "## Adding new words to VADER\n",
        "\n",
        "Because VADER is a Python dictionary, we can update the dictionary if we want to add new words which don't exist in the VADER lexicon. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLEi6azaZfFq"
      },
      "outputs": [],
      "source": [
        "# find a word not in the vader lexicon\n",
        "'blarged' in sid.lexicon.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFJJMpUnZot-"
      },
      "outputs": [],
      "source": [
        "# there is effectively nothing to measure here, 'blarged' was probably identified as neutral because it is not in the dictionary.\n",
        "sid.polarity_scores('I really blarged that one up.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GkGXiQ5ZnHE"
      },
      "outputs": [],
      "source": [
        "# let's pretend blarg is negative\n",
        "sid.lexicon['blarged'] = -4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2cHD_tJZyoj"
      },
      "outputs": [],
      "source": [
        "# our new word is in there with a very neg rating :)\n",
        "sid.polarity_scores('I really blarged that one up.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6SIvX97xZg"
      },
      "source": [
        "You could even overwrite the sentiment values for existing words in the dictionary. Maybe we think the word `happy` is actually negative:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf0dEtC67xZg"
      },
      "outputs": [],
      "source": [
        "# happy is positive\n",
        "sid.polarity_scores('happy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FWYFNXZ7xZg"
      },
      "outputs": [],
      "source": [
        "# let's make it negative!\n",
        "sid.lexicon['happy'] = -2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8p_twDY7xZg"
      },
      "outputs": [],
      "source": [
        "# ultimate power over our words!!\n",
        "sid.polarity_scores('happy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-DDCUfeaHIC"
      },
      "source": [
        "# **The point**\n",
        "\n",
        "Sentiment anlaysis can be pretty bad when used naively. Averaging single word counts of valence/polarity is probably not a good method, but with larger texts some of that noise might wash out. However, fortunately we have VADER which is a much smarter sentiment tool, even if it is rule-based.\n",
        "\n",
        "Importantly, VADER will perform its own tokenization, so we have the luxury of passing raw strings to VADER, rather than needing to preprocess text.\n",
        "\n",
        "You might want to load in some of your own texts now and check the sentiment. You could compare the afinn/VADER dictionaries, or think about looking up and comparing genres in brown, or anything else that seems interesting.\n",
        "\n",
        "This could also show you how using sentiment or any other look-up resource might work in your final project, should you choose to incorporate it!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A final word of caution**\n",
        "\n",
        "VADER was intended to be used on short texts, such as social media posts and other similar types of data. Running VADER on very long texts usually results in poor performance. What might be some ways to address this?"
      ],
      "metadata": {
        "id": "wDysFpd_Gqig"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "44a9cdcbdccbf05a880e90d2e6fe72470baab4d1b82472d890be0596ed887a6b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}