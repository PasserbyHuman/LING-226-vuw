{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3rKEYWIXSCkNgKJE+zQx5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/22_Regular_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjCnw_zkMoGL"
      },
      "source": [
        "## Regular Expressions\n",
        "\n",
        "Regular expressions are a method for finding patterns in text. They are not specific to Python, and can be used in a variety of programming languages as well as applications that allow for regular expressions. You might see them called `regex` for short, but also referred to as `grep` and other terms, which indicate the primary functions used for regular expressions.\n",
        "\n",
        "Learning regular expressions is an incredibly useful skill for programming in general and NLP specifically. Regular expressions allow you to specify extremely precise search patterns, and perform operations on the results of those patterns. There is, however, a bit of a learning curve.\n",
        "\n",
        "You can use regular expressions to search for certain filenames in a directory or certain morphemes in a word.\n",
        "\n",
        "In this notebook, we'll learn the basics of regex which will help us understand how NLTK has employed regex for linguistic analysis.\n",
        "\n",
        "To use regular expressions in Python, we need to `import re`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqIb8-4rYIPn"
      },
      "source": [
        "# import re (regular expressions module)\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uhEy1h_N934"
      },
      "source": [
        "## creating regex patterns and `re.search()`\n",
        "\n",
        "All regex searches require defining a search pattern — you need to tell Python what it is you're looking for!\n",
        "\n",
        "Search patterns are entered as strings, but can include special characters to allow for variability and options that otherwise cannot be included in just a string. You can search for:\n",
        "\n",
        "- Actual characters/strings (e.g., searching for specific words or parts of words).\n",
        "  - These characters mean you want Python to search for the literal version of what you typed (e.g., if you typed \"hello!\" you would want to search for that exact word, including the exclamation mark)\n",
        "  \n",
        "\n",
        "- meta characters which represent something beyond their literal version. We have already seen some of these characters, such as `\\n` for newlines and `\\t` for tab.\n",
        "\n",
        "- Quantifiers, which allow you to constraint the number of times something repeats (e.g., asking for any number of characters followed by a number).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Searching through NZ Birds of the Year\n",
        "\n",
        "Let's start with a small-scale example. Let's pretend we want to search for various linguistic properties of the names of all [NZ Bird of the Year Winners](https://en.wikipedia.org/wiki/Bird_of_the_Year):\n",
        "\n",
        "```\n",
        "2022\tPīwauwau\n",
        "2021\tPekapeka-tou-roa\n",
        "2020\tKākāpō\n",
        "2019\tHoiho\n",
        "2018\tKererū\n",
        "2017\tKea\n",
        "2016\tKōkako\n",
        "2015\tKuaka\n",
        "2014\tTara iti\n",
        "2013\tMōhua\n",
        "2012\tKārearea\n",
        "2011\tPūkeko\n",
        "2010\tKākāriki\n",
        "2009\tKiwi\n",
        "2008\tKākāpō\n",
        "2007\tRiroriro\n",
        "2006\tPīwakawaka\n",
        "2005\tTūī\n",
        "```\n",
        "\n",
        "Let's first get those winners into a list!\n",
        "\n",
        "Note that 2023 is [bird of the century](https://www.birdoftheyear.org.nz/), with Kiwi a favourite to win!"
      ],
      "metadata": {
        "id": "_1sj_TaszkeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define list of BoTY winners:\n",
        "birbs = ['Pīwauwau', 'Pekapeka-tou-roa', 'Kākāpō', 'Hoiho', 'Kererū', 'Kea', 'Kōkako', 'Kuaka', 'Tara-iti',\n",
        "         'Mōhua', 'Kārearea', 'Pūkeko' ,'Kākāriki', 'Kiwi', 'Kākāpō', 'Riroriro', 'Pīwakawaka', 'Tūī']"
      ],
      "metadata": {
        "id": "AxWCnoULy2La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, now we can start using `re.search()` as a means to find if any of our bird names contain specific characters or patterns.\n",
        "\n",
        "`re.search()` is a function which results in two outcomes:\n",
        "\n",
        "1. If the pattern exists in the string being searched, `re.search()` will return a `match` object which contains details of the match\n",
        "2. If the pattern does not exist in the string being searched, `re.search()` will return nothing\n",
        "\n",
        "With this knowledge, we can use `re.search()` as a *conditional* test to return strings which do or do not have specific patterns of interest. More specifically, we can specify `if re.search():` which would only return `True` if there was a match in the string.\n",
        "\n",
        "The syntax of `re.search()` is:\n",
        "\n",
        "```\n",
        "re.search(pattern = '', string = '')\n",
        "```\n",
        "\n",
        "So we need to first supply the pattern we are looking for, and then supply the string we are seaching.\n",
        "\n",
        "\n",
        "Knowing this,  let's search for capital \"K\" in our list of birds and print any bird that includes a capital K:"
      ],
      "metadata": {
        "id": "sVA_y30O25_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's save our search pattern to a variable:\n",
        "pattern = 'K'\n",
        "\n",
        "\n",
        "# create a for loop through our list of birds\n",
        "for birb in birbs:\n",
        "  # if there is a resulting match object\n",
        "  if re.search(pattern , birb):\n",
        "  # print the bird\n",
        "    print(f'{birb} has a {pattern}!')\n",
        "  else:\n",
        "    print(f'{birb} does not have a {pattern}!')"
      ],
      "metadata": {
        "id": "2OdkFQb1280K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing you should notice about those results is that the search is quite literal - birds with only lowercased \"k\" such as Pūkeko did not meet the condition of the test - which makes sense. We could repeat the same search as above but with a lowercased 'k':"
      ],
      "metadata": {
        "id": "igBRviba87jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same as previous but w/ lowercase k\n",
        "pattern = 'k'\n",
        "\n",
        "# create a for loop through our list of birds\n",
        "for birb in birbs:\n",
        "  # if there is a resulting match object\n",
        "  if re.search(pattern , birb):\n",
        "  # print the bird\n",
        "    print(f'{birb} has a {pattern}!')\n",
        "  else:\n",
        "    print(f'{birb} does not have a {pattern}!')"
      ],
      "metadata": {
        "id": "wcyJ8Amf9G99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run into the converse situation here — birds with only uppcase \"K\" such as 'Kiwi' were not matched.\n",
        "\n",
        "Usually, we want to add a bit more flexibilty into our searches so that we can find strings that meet a variety of criteria. In this case, how could we create a pattern which would allow for birds including both upper and lowercased 'k' to be returned?\n",
        "\n",
        "Clearly, one option would be to convert the strings to lowercase before doing any searches! But, keep in mind that we are dealing with proper nouns here (i.e., names of things), and in English these are signalled in writing through capitalization of the initial letter. So in this case, applying a lowercase to everything might not be appropriate.\n",
        "\n",
        "\n",
        "So, let's use this opportunity to see how to allow for options in our search patterns. We can use square brackets to tell `re.search()` that we are looking for *any* candidate within the square brackets:\n",
        "\n",
        "```\n",
        "'[Kk]' = any of 'K' or 'k'\n",
        "```\n",
        "\n",
        "In this way, the square brackets are effectively a shorthand for using `or`. Let's demonstrate this, while also exploring how to use the `match` object to show us which part of the pattern was actually a hit. We can do so by using the `.group()` method from a resulting regex match.\n",
        "\n"
      ],
      "metadata": {
        "id": "VRhEsPzt9jKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define our search pattern\n",
        "pattern = '[Kk]'\n",
        "# create a for loop through our list of birds\n",
        "for birb in birbs:\n",
        "  # save the match to a variable\n",
        "  match = re.search(pattern, birb)\n",
        "  # if there is a match, print the bird and the results of the match\n",
        "  if match:\n",
        "  # print the bird\n",
        "    print(f'{birb} has a {match.group()}!')\n",
        "  else:\n",
        "    print(f'pattern {pattern} not found for {birb}!')"
      ],
      "metadata": {
        "id": "vB58iZgh_ZN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the output, we can see that the search was lazy and stopped once it found the first match. Crucially, the search begins from the start of the string, so all birds that start with `K` will be matched for uppercase K, but we still do not know if they contain a lower-cased `k`. In order to answer this relatively silly question of how many \"k\"s are in the bird names, we would have to take a different approach.\n",
        "\n",
        "But let's move on and instead refine a method to search specific parts of a string. Let's stick with our birds for just a little bit longer. First, let's revise our search pattern so that it searches for a `k` *within* the bird name, but *not* the first or last letter of the name. But, in order to do so, we need to understand the use of certain meta characters which designate the start/end of a string, as well as meta characters which stand for any character.\n",
        "\n",
        "The full stop `.` is a meta character which stands for *almost* anything (it excludes newlines, for example). You can think of the full stop as a wild card — it can represent anything you want.\n",
        "\n",
        "So, a regex pattern comprised of three full stops `'...'` would represent `three of any character in a row`. Such a pattern would match the first combination of three characters in a string, regardless of whether the string is longer than three characters. However, the pattern would *not* match a string of two characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ih-Gy2p1X3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wildcard: the full stop `.` in regex"
      ],
      "metadata": {
        "id": "KM-q0IPQ-pRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define our search pattern\n",
        "three_dots = '...'"
      ],
      "metadata": {
        "id": "RNI3qCrC47MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a match object\n",
        "re.search(three_dots, 'dog')"
      ],
      "metadata": {
        "id": "ZxLcPUoY49kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# also returns a match object (the first 3 characters of the string):\n",
        "re.search(three_dots, 'dogs and cats')"
      ],
      "metadata": {
        "id": "60CtcY6c9DUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns nothing because there is no sequence of three characters in this string\n",
        "re.search(three_dots,'ab')"
      ],
      "metadata": {
        "id": "odOIecM_5LEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns nothing because `.` cannot stand for newlines.\n",
        "re.search(three_dots, 'ab\\n')"
      ],
      "metadata": {
        "id": "pljQGD-39NnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### String anchors: `^` and `$`\n",
        "\n",
        "Remember the challenges associated with defining a word to make tokens? Regex has special characters which sort of help with this, in that they represent the *start* and the *end* of a string (which, of course, is not always the same as a word!).\n",
        "\n",
        "These characters are also called `anchors`:\n",
        "\n",
        "```\n",
        "^ - start of a string\n",
        "$ - end of a string\n",
        "```\n",
        "\n",
        "So we can use these strings to better constrain our search patterns. If instead of finding any three-character sequence, we wanted to find any three-character *word*, we could revise our three dots pattern as such:"
      ],
      "metadata": {
        "id": "ryJmc19Y2-fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# regex search pattern which looks for any three characters between a start/stop of a string\n",
        "three_character_word = '^...$'"
      ],
      "metadata": {
        "id": "eK43la8v84FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it will find dog...\n",
        "re.search(three_character_word, 'dog')"
      ],
      "metadata": {
        "id": "7UXSzy9q8_5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and cat...\n",
        "re.search(three_character_word, 'cat')"
      ],
      "metadata": {
        "id": "MlDL8VQ8-FEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but not bird. (because there are four characters between start/stop)\n",
        "re.search(three_character_word, 'bird')"
      ],
      "metadata": {
        "id": "8BHN_-ii-H_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantifiers\n",
        "\n",
        "So regex patterns allowed for us to scope the nature of our search to words of certain lengths, as well as to avoid characters that are at the start and end of a word.\n",
        "\n",
        "\n",
        "Let's extend this knowledge to return to our birds and find if we can define a search pattern which locates lowercase 'k' in bird names, even if they start with an uppercase 'k'. We need to learn *one more thing* though, and that's how to attach quantifier flags to characters. Quantifers allow us to specify how many times the *preceding* character can occur in the search. One of the most useful quantifiers is `*` which stands for `zero or more`, a rather liberal constraint on a character.\n",
        "\n",
        "So, if we wanted to search for \"zero to infinite instances of any character\", we could type `.*`, for example:"
      ],
      "metadata": {
        "id": "QW5sZJsj-LZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .* basically says \"find me anything\"\n",
        "re.search('.*', 'these pretzels are making me thirsty!')"
      ],
      "metadata": {
        "id": "hXy5QRMhA-I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# even nonsense\n",
        "re.search('.*', 'asdfasdlfkjasdl;k#fjasd;lk@fj)))')"
      ],
      "metadata": {
        "id": "XV4MM-EvA3Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally back to the birds!\n",
        "\n",
        "So, using `.*` is quite dangerous unless we constrain the search a bit more. Let's apply this to our birds problem and define a search pattern which finds a `k` occuring inside the name of our birds. Here is the pattern:\n",
        "\n",
        "```\n",
        "`'^..*k.*.$'`\n",
        "\n",
        "```\n",
        "\n",
        "This pattern says search for:\n",
        "\n",
        "`^` (start of a string), `.` (followed by one wild card - the first letter of the name), `.*` (followed by zero or more wildcards), `k` followed by one \"k\", `.*` (followed by zero or more wildcards), `.` (followed by one wild card - the last letter of the name), `$` (followed by the end of the string).\n",
        "\n",
        "This pattern lets us find a lowercase k within a string of any length."
      ],
      "metadata": {
        "id": "6mm6b9LUBKUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mid_k = '^..*k.*.$'"
      ],
      "metadata": {
        "id": "aptPnS4-_m9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = mid_k\n",
        "# create a for loop through our list of birds\n",
        "for birb in birbs:\n",
        "  # save the match to a variable\n",
        "  match = re.search(pattern, birb)\n",
        "  # if there is a match, print the bird and the results of the match\n",
        "  if match:\n",
        "  # print the bird\n",
        "    print(f'{birb} has a lower-cased k!')\n",
        "  else:\n",
        "    print(f'{birb} does not have a lower case k!')"
      ],
      "metadata": {
        "id": "oo536zeVDSB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular expressions are challenging\n",
        "\n",
        "It goes without saying that learning regular expressions is challenging. It usually takes a good bit of trial and error in order to figure out the exact search pattern you need for your purposes.\n",
        "\n",
        "The rest of this notebook covers some of the material from the NLTK book, and ideally this digression helps to digest the NLTK material. A really useful website for Regex is [Regex101](https://regex101.com/) (make sure you select Python), and there are a number of other resources out there to help you learn regex, such as [Geeks for Geeks](https://www.geeksforgeeks.org/python-re-search-vs-re-match/?ref=gcse).\n",
        "\n",
        "The next section will become a bit repetitive, but I think it is important for something like regex to be repeated.\n",
        "\n"
      ],
      "metadata": {
        "id": "D9s2Ia2DDm8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex and NLTK\n",
        "\n",
        "There are a number of other functions from `re`, and NLTK has also modified or uses regex in some of their custom functions. We will load in NLTK and download some resources.\n",
        "\n",
        "One of the built-in resources from NLTK is literally just a list of English words. In the cell below, we use a `list comprehension` to return a lowercase version of every word in the English list of words and save it to a variable named `wordlist`"
      ],
      "metadata": {
        "id": "P-rKy_4GFD7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('book')"
      ],
      "metadata": {
        "id": "5OB9CgrWFrGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK has wordlists built in. In the next cell, a `list comprehension` is used to return a lower-cased version of each word from the English set of words, which as saved to a variable named `wordlist`."
      ],
      "metadata": {
        "id": "d-dcbWqxF2HP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOQDcW5Nyou"
      },
      "source": [
        "# following NLTK, get a list of lower case words to use as examples\n",
        "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there are a lot of words in here!\n",
        "len(wordlist)"
      ],
      "metadata": {
        "id": "weNjRYvSwusF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample a random word\n",
        "wordlist[123456]"
      ],
      "metadata": {
        "id": "oDFf0SE0wodD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `re.search()` to find `ed` words.\n",
        "\n",
        "The first regex function NLTK shows you is `re.search()`, which we have already explored above. In the example below, the pattern looks for English words which end in `ed`. Remember, the `$` represents the end of a string. Because we are working with a word list, we know each string represents a single word.\n",
        "\n",
        "Curious, do you know why `ed$` might be an interesting pattern to search for? That's because most verbs in English which end in \"ed\" are marking past-tense, and \"ed\" is therefore a productive suffix performing a mophological task."
      ],
      "metadata": {
        "id": "gW8VbxRnyraD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFPQ21OGN41j"
      },
      "source": [
        "# this example loops through each word in wordlist and checks if it ends in ed\n",
        "[w for w in wordlist if re.search('ed$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYlQHt0MOPw1"
      },
      "source": [
        "As we already know, regular expressions allow you to search for literal sequences of letters, such a `ed` in the above example. Here is another example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW-_zcscOY9w"
      },
      "source": [
        "# create a smaller example to search through\n",
        "vuw = [\"Victoria\", \"University\", \"of\", \"Wellington\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZKqWpihOrbc"
      },
      "source": [
        "# let's search for the word \"Victoria\" -\n",
        "# Remember that this is a very weak use of regular expressions because we are only searching for one specific string.\n",
        "for word in vuw:\n",
        "  if re.search('Victoria', word):\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5NMIS4IO3bW"
      },
      "source": [
        "# The search can be for substrings as well, or even single letters.\n",
        "\n",
        "# here we will return anything that contains the pattern 'o'\n",
        "for word in vuw:\n",
        "  if re.search('o', word):\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsZd0KNTPBsY"
      },
      "source": [
        "# Remember that regex is case sensitive - why doesn't this pattern return a word?\n",
        "for word in vuw:\n",
        "  if re.search('wellington', word):\n",
        "    print(word)\n",
        "  else:\n",
        "    print('found nothing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDTZvXjxPRho"
      },
      "source": [
        "## End of words\n",
        "\n",
        "The NLTK example uses the metacharacter `$` so that only the ends of words are matched - otherwise any word containing `ed` as part of the string *anywhere in the string* would be matched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQU6YgRmQQc8"
      },
      "source": [
        "# what does the $ mean?\n",
        "welly = ['Wellington', 'wellington', 'Wellington!']\n",
        "\n",
        "# because the third 'wellington' has a '!' at the end, it will not be returned\n",
        "for word in welly:\n",
        "  if re.search('ton$', word):\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRzxOM9YGMz"
      },
      "source": [
        "## Wildcard again!\n",
        "\n",
        "Remember, the `'.'` (full stop) character means *match anything, one time* — it represents (almost) any possibility. In the code below, I save the results of each `re.search()` to a variable, then print that variable using the `.group()` method, which returns the actual string result. As I loop through increasing numbers of subsequent full stops, the search results grow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7PQltDFYSXC"
      },
      "source": [
        "# what does the '.' mean?\n",
        "s = 'soda'\n",
        "\n",
        "# a set of patterns with increasing wildcards\n",
        "more_dots = ['.','..', '...', '....']\n",
        "\n",
        "for dot in more_dots:\n",
        "  # each time this loops, it uses increasing number of dots\n",
        "  match = re.search(dot, s)\n",
        "  print(match.group())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ooy3R5yjks9"
      },
      "source": [
        "## Start of words\n",
        "\n",
        "\n",
        "The `^` character is the opposite of the `$` in that it indicates the start of a string. So you can look for things at the start/end of words using these patterns, and also specify the entire lenght of a word you are looking for"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRxEhsOJjQdQ"
      },
      "source": [
        "# the NLTK example shows how to use wildcards to make slots for words\n",
        "\n",
        "# look for all words which start with an 's', end with an 's', and have two characters in between\n",
        "[w for w in wordlist if re.search('^s..a$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRKqJJ_kj_hh"
      },
      "source": [
        "## **Groups and options**\n",
        "\n",
        "Did you ever text on a mobile phone before smart phones? Each number on the dial pad represented three possible letters, so you would choose the number for the letters, then press 1, 2, or 3 to choose the letter. For example, the letters. (abc) were on the 1 key, so to type 'a', you would push 1, 1, to type 'b', you would push, 1, 2, and so on. It was quite cumbersome.\n",
        "\n",
        "There is a scene in the movie *The Departed* where Matt Damon sends a text using this method while holding his phone in his pocket without looking at it - seems difficult!\n",
        "\n",
        "The NLTK book uses the example of texting this way to extend our knowledge of regular expressions and the **optional sequences** indicated by square brackets `[]`. These provide a list of options to the search pattern, where the pattern looks for *any* but not all of the things inside the brackets.\n",
        "\n",
        "Here's another example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qof4PmF_kue6"
      },
      "source": [
        "# using brackets to find patterns\n",
        "us_spies = ['fbi', 'cia', 'nsa']\n",
        "\n",
        "for spy in us_spies:\n",
        "  if re.search(\"[fcn][bis][ia]\", spy):\n",
        "    print(spy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CprM4QO1lmSd"
      },
      "source": [
        "# the NLTK example shows you how to find \"textonyms\"\n",
        "# why are only four words found? Can you find other words using other patterns?\n",
        "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96OMAHcYl8XW"
      },
      "source": [
        "# the '+' sign matches \"one or more\" - so the results range from one letter to 9\n",
        "[w for w in wordlist if re.search('^[ghijklmno]+$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3rcIK6Gnrlg"
      },
      "source": [
        "# the chat corpus is funny to see repeated letters\n",
        "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
        "\n",
        "[w for w in chat_words if re.search('^[ha]+$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFfRI1h9n7dt"
      },
      "source": [
        "# how many variations of LOL are there?\n",
        "[w for w in chat_words if re.search('^[lL][oO][lL]$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn9syWTvoXeR"
      },
      "source": [
        "## Finding regular and irregular past tense.\n",
        "\n",
        "Regular expressions become more complex as you add additional options to them. It is quite rare that you will want to search for just a word string, usually you will want to search for a variety of candidates that meet a certain condition - what if we wanted to write a regex that found all regular and irregular past tense in English?\n",
        "\n",
        "The following pattern looks for words that end in either `ed` or `en`. How else can we write the pattern? I've put three versions below that all use the meta characters in different ways. Of course, this also matches word with are *not* past tense but yet still end in these patterns, and there are also even more irregular past tense that are not captured (such as `sank`). Hopefully this gives you some understanding into how powerful regex patterns can be, but also that they will require some trial and error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRqRdRMTooZp"
      },
      "source": [
        "# using brackets\n",
        "[w for w in wordlist if re.search('e[dn]$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z7SWppHo92M"
      },
      "source": [
        "# using |\n",
        "[w for w in wordlist if re.search('ed$|en$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX2MAtb2pHlT"
      },
      "source": [
        "# using () and |\n",
        "[w for w in wordlist if re.search('(ed|en)$', w)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrkh7OFeqQ4N"
      },
      "source": [
        "As I said above, it is sometimes easy to get frustrated with regular expressions. The NLTK book uses a sort of \"try it and figure it out\" approach, which can work for some. You might also find resources such as [RegexOne](https://regexone.com/) to be useful in order to fully master regular expressions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ` re.findall()` - Extracting and Manipulating patterns\n",
        "\n",
        "Besides searching, you might also need to extract some information from words and perform some operation on that information. NLTK introduces the `re.findall()` function. When we used the `re.search()` function, we returned a match object that required some additional processing to get the actual string, so `re.search()` is useful for testing whether strings meet certain conditions, whereas `re.findall()` helps return the actual results.\n",
        "\n",
        "In the next example, the pattern searches for any of the vowels inside the square brackets, and using `.findall()` means that every single part of the string which meets that pattern will be returned.\n",
        "\n",
        "You can see that different words have a different number of vowels, and the entire set of matches is returned as a list."
      ],
      "metadata": {
        "id": "FVrxxrhYKz-f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rtABWbJq2An"
      },
      "source": [
        "# find all the vowels in VUW\n",
        "vuw = ['victoria', 'university', 'of', 'wellington']\n",
        "\n",
        "\n",
        "for word in vuw:\n",
        "  # findall does what it sounds like it does.\n",
        "  print(re.findall('[aeiou]', word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeyXjiSqrXza"
      },
      "source": [
        "This means that we can start to *count* the occurance of patterns, using functions such as `nltk.FreqDist()`.\n",
        "\n",
        "In the example below, the quantifer `{3}` is used. The squiggle brackets allow you to specify the a precise number of occurences for your pattern, either as a fixed number `{3}` = three times, or as a range: `{3,4}` = three to four times.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaByLtWcrp1C"
      },
      "source": [
        "# let's find all stretches of three letters which only include a certain subset of letters\n",
        "vuw_triplets = nltk.FreqDist(vowel for word in vuw for vowel in re.findall(r'[vuinc]{3}', word))\n",
        "\n",
        "vuw_triplets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5epPl3DtCHs"
      },
      "source": [
        "# what happens when we reverse sort the list?\n",
        "sorted(vuw_triplets, reverse = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWRJX4Srtawj"
      },
      "source": [
        "The **Your Turn** from NLTK is a good challenge. It prompts you to consider yet another meta character, the `\\d` which stands for \"digit\" (i.e., numbers).\n",
        "\n",
        "> *Your Turn: In the W3C Date Time Format, dates are represented like this: 2009-12-31. Replace the ? in the following Python code with a regular expression, in order to convert the string '2009-12-31' to a list of integers [2009, 12, 31]:*\n",
        "\n",
        "> `[int(n) for n in re.findall(?, '2009-12-31')]`\n",
        "\n",
        "\n",
        "The meta character `\\d` matches digits, which is one way to go about this. Can you figure out other ways?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKYIH71WtnFv"
      },
      "source": [
        "[int(n) for n in re.findall('\\d+', '2009-12-31')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvPtBvS7dUBt"
      },
      "source": [
        "## Using `.findall()` to delete all of the vowels in a word\n",
        "\n",
        "As you get more familiar with regular expressions, or search for more help online, you'll see that a lot of the time, the regular expression patterns are saved to a variable and then used in regex functions. That's what the NLTK authors do below, when showing how to remove all of the vowels from a word.\n",
        "\n",
        "You'll also see that an \"r\" is placed at the start of the regex pattern. This means the string is treated as a \"raw\" string and has implications for \"escaping\" characters that we will look at later on.\n",
        "\n",
        "Return to the NLTK example of how to remove all vowels from a word. The frustrating part of this example is the dual purpose of the `^`. When the `^` is *outside* of square brackets, it stands for the start of a string, as we've already explored. When the `^` is *inside* square brackets, it is actually *negating* the pattern, so while `'[AE]'` means either anything that is \"A\" or \"E\", `'[^AE]'` means anything that is not either \"A\" or \"E\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQN8IsIfdV6G"
      },
      "source": [
        "# this one is tricky to wrap your head around\n",
        "# because the carrot (^) inside the third set of vowels is negating the match...\n",
        "# so it's anything BUT a vowel - might be hard to notice at first\n",
        "\n",
        "# save the regex pattern\n",
        "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
        "\n",
        "def compress(word):\n",
        "  pieces = re.findall(regexp, word)\n",
        "\n",
        "  # do you remember how ''.join works?\n",
        "  return ''.join(pieces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCM08Jzddzqk"
      },
      "source": [
        "compress('victoria university of wellington')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLMKr2pAAmXo"
      },
      "source": [
        "compress('regular expressions are hard!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsY_jMSifh9U"
      },
      "source": [
        "## Distribution of sounds in a word\n",
        "The NLTK authors load in words from the [Rotokas language](https://en.wikipedia.org/wiki/Rotokas_language) and search for CV pairs which stand for consonant-vowel pairs. Consonants and vowels tend to pattern together in systematic ways in languages.\n",
        "\n",
        "\n",
        "A conditional frequency distribution is  used to show this distribution. Compare the `s` and `t` rows - `s` can be followed by `i` but is only rarely followed by any other vowels, whereas `t` can be followed be `a` and `o` but not `i`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I1WToAgfobi"
      },
      "source": [
        "# using cfd to find minimal pairs.\n",
        "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
        "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
        "cfd = nltk.ConditionalFreqDist(cvs)\n",
        "cfd.tabulate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3dM23swgr7G"
      },
      "source": [
        "The next section shows you more about using brackets to scope your regex properly, and also how to built a simple stemmer (i.e., a program which separates the roots of a word from the suffix.) The book asks you whether you can spot the problems with the stemmer as presented, the main problem being that many false positives will be returned through using a strict rule-based approach. Why is this? Because English morphology has many rules but also many exceptions to those rules.\n",
        "\n",
        "NLTK has a neat function, `re_show` which will show you the matches a regex search will make (on a string input)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEK4G6RIhH0C"
      },
      "source": [
        "# nltk re_show will print the regex matches for your pattern\n",
        "nltk.re_show(r'vuw', 'vuw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5sCzYFhWB5"
      },
      "source": [
        "nltk.re_show('\\d', '2009-12-31')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-TUhdvehdBd"
      },
      "source": [
        "us_spies = 'fbiciansa'\n",
        "\n",
        "nltk.re_show(\"[fcn][bis][ia]\", us_spies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iw7ItSrBLKm"
      },
      "source": [
        "# NLTK pattern searches\n",
        "\n",
        "One of the benefits of using NLTK is that the authors have provided their own versions of functions and processes for text analysis. To that end, they introduce their own version of `findall` which uses slightly different syntax to search. This is in many ways is an easier and perhaps more readable way to use regular expressions. Each set of crocodile brackets `<>` defines the boundaries of a match — which could be a word, a single letter, or whatever you please, based on using meta characters. Importantly, the NLTK `.findall()` is different from the regex `.findall()`.\n",
        "\n",
        "The NLTK version is a method that is used on an `nltk.Text` object which has been tokenized. The regex version is used on strings. Hopefully these examples make the difference clear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffLmuSHYC0J1"
      },
      "source": [
        "# the re.findall is what we have been using above - it requires a string as input\n",
        "re.findall(r\"these\", 'these pretzels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXQ64Kb2DCZX"
      },
      "source": [
        "# you can use a list, but you must loop the list\n",
        "for word in ['these', 'pretzels']:\n",
        "  print(re.findall(r'these', word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_fJh85iB6Ja"
      },
      "source": [
        "# can we use the NLTK .findall on a list? no - because it is not an nltk.Text object\n",
        "# (please note the error here tells us exactly what the problem is.)\n",
        "['these', 'pretzels'].findall(r\"<these>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dItTWBK0Dedl"
      },
      "source": [
        "# so we must remember to convert the strings into an nltk.Text object in order to use the NLTK specific method for searching tokens.\n",
        "nltk.Text(['these', 'pretzels']).findall(r\"<these>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXOgQfgkEvYN"
      },
      "source": [
        "Ok, now that we've covered the differences between the two versions of `findall`, let's follow NLTK's lead and do some more interesting stuff using their regex patterns. We need to combine the basic regex syntax (using meta characters and strings) with the angle bracket notation for tokens used by the NLTK version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wNrYn5ah_kI"
      },
      "source": [
        "# Let's convert the brown corpus to an nltk.Text object\n",
        "from nltk.corpus import brown\n",
        "brown_words = nltk.Text(brown.words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9YRcRPBiLx1"
      },
      "source": [
        "# what kind of stuff will we find with this pattern?\n",
        "# this pattern means as WORD as WORD\n",
        "brown_words.findall(r\"<as> <\\w*> <as> <\\w*>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80yKmh1XjyZ4"
      },
      "source": [
        "# and this one?\n",
        "brown_words.findall(r\"<a> <\\w*> <as> <\\w*>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4MYqemIkiZM"
      },
      "source": [
        "Why would one want to use the NLTK `findall` version instead of regular regex?\n",
        "\n",
        "For one thing, it's probably a bit easier to read and understand, but it also lets us think about larger patterns. You'll also see later on that this format can be extended to other types of searches.\n",
        "\n",
        "However, the requirement that the text be an `nltk.Text` object is something you will need to remember."
      ]
    }
  ]
}