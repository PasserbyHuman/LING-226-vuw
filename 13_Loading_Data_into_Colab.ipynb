{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/13_Loading_Data_into_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X3OUdi30S5Q"
      },
      "source": [
        "# Using your own data\n",
        "\n",
        "As useful and interesting as the NLTK data is, you will eventually want to load in your own data. This notebook shows you a few options for doing so\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf4AphsizytP"
      },
      "source": [
        "# Option 1: Mounting your Google Drive\n",
        "\n",
        "One way to do so involves connecting Google Colab with your Google Drive.\n",
        "\n",
        "The process of connecting Colab to your Google Drive is known as mounting your drive. To do so, you click on the folder icon on the left side of the Colab page:\n",
        "\n",
        "<img src = https://i.imgur.com/82Wedue.png>\n",
        "\n",
        "Then you click the \"mount drive\" icon in the next menu:\n",
        "\n",
        "<img src = https://i.imgur.com/d8DxFIu.png>\n",
        "\n",
        "Colab should then automatically add a code cell like this:\n",
        "\n",
        "<img src = https://i.imgur.com/ttfUkwi.png>\n",
        "\n",
        "Run the cell to mount your Google Drive. You will most likely see several permissions prompts asking you if its okay to make this connection with the associated Google account. It's fine to do this with notebooks you make or the ones I give you, but be wary of other notebooks that might try to ask for your account permissions. There is likely no big risk but I feel obligated to tell you that you should not blindly trust any other Colab notebooks you might come across.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDANcdwPIlZP"
      },
      "source": [
        "## Accessing files in your Google Drive\n",
        "\n",
        "Now that your Drive is connected, you can directly access files in your Google Drive account. This is very handy. (You might need to click the refresh button (the folder with the circle arrow) to see the new folder).\n",
        "\n",
        "You should see a new folder on the left side menu (after clicking on the folder icon) called `drive`. Clicking that folder should then reveal a subfolder called `MyDrive`. The `MyDrive` folder is the root folder for your Google Drive.\n",
        "\n",
        "<img src = https://i.imgur.com/Av1mGtQ.png>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In order to access files on your drive, you will need to be able to give Python the full filepath to your files. No matter where your files are, the start of your filepath will always be `/content/drive/MyDrive/...`, where the `...` are any additional folders.\n",
        "\n",
        "So, for example, if you had a file called `mydata.txt` located in the base level of your Google Drive, the filepath location would be `/content/drive/MyDrive/mydata.txt`. If you had that same file located in a folder called `mydata`, the filepath would be `/content/drive/MyDrive/mydata/mydata.txt`, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJhGnTHeLnZM"
      },
      "source": [
        "### Practice uploading a file from your drive\n",
        "\n",
        "Go [here](https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/marine_biologist.txt), you should see a page of text. Manually copy and paste the text into a text editor program, such as notepad on windows or textedit on Mac (don't use Microsoft Word). Save the file as a `.txt` file to your Google Drive folder and name it `marine_biologist.txt`\n",
        "\n",
        "Once you've done that, you should be able to read the text into Colab using the following cell.\n",
        "\n",
        "The code uses the Python `open()` function, which, well, opens files! We need to use the `.read()` method at the end of open to return the contents of the file, which in this case is a string of the raw text in the `.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEkhC6riMx-y"
      },
      "outputs": [],
      "source": [
        "marine_biologist = open('/content/drive/MyDrive/marine_biologist.txt').read()\n",
        "\n",
        "# a random quote from this text\n",
        "marine_biologist[15041:15135]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b3YjK0HNoFn"
      },
      "source": [
        "If you are having trouble with this step, make sure you are saving the file to your base level folder in Google Drive. Also make sure your drive is mounted, and that you have saved the file using the same filename I used above. If all else fails, please reach out for some help, because being able to access texts on your Google drive will be an important step for a lot of you in order to read in text data. Of course, if you're comfortable using Jupyter on your local machine, you're under no requirement/obligation to use Google drive to store your files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1qH-GgxQlx-"
      },
      "source": [
        "### Working with a read file.\n",
        "\n",
        "Once you've loaded the file in, you can perform all of the same operations on it as we have been doing on strings we've typed as well as the built-in data included with NLTK. You should be familiar with the following code at this point â€” are you able to leave comments explaining what each line is doing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzYbXW2sQvZ7"
      },
      "outputs": [],
      "source": [
        "marine_tokens = nltk.word_tokenize(marine_biologist.lower())\n",
        "\n",
        "marine_fdist = nltk.FreqDist(marine_tokens)\n",
        "\n",
        "marine_fdist.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmmobRTP4s2H"
      },
      "source": [
        "# Option 2: Using `!wget` or the `requests` library\n",
        "\n",
        "Using Google Drive is a solid bet for integrating with Colab, but you might not like mounting drives each time you run a notebook, or working with files in your drive.\n",
        "\n",
        "There are other options which involve reading files directly from the internet, using other functions such as `!wget` or Python libraries for requesting data from URLs, such as the `requests` library. There are various places in these materials which show how to do either method.\n",
        "\n",
        "However, using these methods requires that the data already exists on the internet somewhere, and also exists at a URL you can access (and ideally control). Therefore, I only recommend using this method if you able to control the place where you data lives - and it might just be easier for you to use Google Drive if you don't want to go that route. But, using GitHub is a solid choice, and one which is used in some of these notebooks (as well as the example below).\n",
        "\n",
        "The main benefit of using `!wget` is that the data is loaded directly into the notebook environment, so you would not need to muck around with sifting through files on the Google Drive. This method is also a bit easier to share resources with, since someone else would not need to have the same data on their Drive.\n",
        "\n",
        "Below is an example of using `!wget` to access a text file saved on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1MQjXQK_VlE"
      },
      "outputs": [],
      "source": [
        "# using !wget to load a file into the notebook environment\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/tmoom.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9CsM6Hl_kBW"
      },
      "source": [
        "Instead of pointing at `/content/drive/MyDrive/...`, you instead just point at `/content/...`\n",
        "\n",
        "You need to use the appropriate method to open the file, such as using `open()` to open a text file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x5bFgBN_x9j"
      },
      "outputs": [],
      "source": [
        "# read in the text\n",
        "tmoom = open('tmoom.txt').read()\n",
        "\n",
        "# split into tokens\n",
        "tmoom_tokens = nltk.word_tokenize(tmoom)\n",
        "\n",
        "# look at the first ten tokens!\n",
        "[token for token in tmoom_tokens][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uub01nj30Gwq"
      },
      "source": [
        "## `requests`\n",
        "\n",
        "You can also read in data directly from the url using Python libraries such as `requests` or `urllib`. This method still requires that you know where you can access a text file from, but unlike `!wget` will load the file directly into Python, rather than through the notebook environment first. You typically need to point a function towards a url and then use some additional methods to open the data. This works best for raw `.txt` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EqbD09Q0H6v"
      },
      "outputs": [],
      "source": [
        "# import the library\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU1xAczj0knu"
      },
      "outputs": [],
      "source": [
        "# save URL to a variable\n",
        "URL = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/tmoom.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTpZ6lFk0odh"
      },
      "outputs": [],
      "source": [
        "# use .get() to retrieve file at the URL\n",
        "data = requests.get(URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmMdBW800y84"
      },
      "source": [
        "You can see that the information is saved in the variable in a format specific to the requests library. On its own, we can't see that text in the data object.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF0MffZY0yaL"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4xWsY8d1BI9"
      },
      "source": [
        "This variable has a variety of attributes, one of which is the `text` attribute, which includes the text of the URL. In this case, it is a `.txt` file. You can access the text using `.text` - note that you do not need to use the brackets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6R8uuJz1Etm"
      },
      "outputs": [],
      "source": [
        "data.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15IkZ0D41a17"
      },
      "source": [
        "We can of course chain these functions together in order to read in the text and convert it into tokens or some other format in one single line. In the cell below, I split the URL results on newlines inside of a list comprehension. The result is requesting the file and receiving a list of all the sentences in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "estIp3c91gUm"
      },
      "outputs": [],
      "source": [
        "[line for line in requests.get(URL).text.split('\\n')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MoOmo5AyCbv"
      },
      "source": [
        "# Option 3: Uploading files manually\n",
        "\n",
        "There is also a way to upload files directly to the notebook environment. This involves using a function from the colab library. First, import the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiM7RmF0yR36"
      },
      "outputs": [],
      "source": [
        "# import the files function from colab\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWgQ9fnLyWv5"
      },
      "source": [
        "After importing the function, you now have access to a few functions, one of which allows you to upload files into your notebook. You do this with the `files.upload()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmZ0GeO0yTVc"
      },
      "outputs": [],
      "source": [
        "# run a cell with this command to prompt the user to upload files.\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXWkUrDYyw5_"
      },
      "source": [
        "You can then choose a file from anywhere on your computer and upload it to the notebook environment. The file can then be accessed using the same methods used with `!wget`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvJtd0u4RMSJ"
      },
      "source": [
        "# Creating your own NLTK corpus\n",
        "\n",
        "Regardless of how you get your data into Colab, you can use the NLTK library to make your own version of the NLTK corpora.\n",
        "\n",
        "There are two ways to do this, one is to read in a bunch of texts as one single corpus. To do this, we use the `PlaintextCorpusReader` class from NLTK.\n",
        "\n",
        "In order to use it, we need three things:\n",
        "\n",
        "1. some files,\n",
        "2. a filepath which leads to files, and\n",
        "3. the names of the files.\n",
        "\n",
        "Again, please follow along. Please go [here](https://github.com/scskalicky/LING-226-vuw/blob/main/other-data/seinfeld.zip) and click the \"download\" button to download a compressed file containing several scripts from an American television show *Seinfeld*.\n",
        "\n",
        "Download the file, unzip it, and save the folder to your base Google Drive folder. Your files should be located in `/content/drive/MyDrive/seinfeld`. This will be the filepath we feed to the NLTK corpus reader. Let's go ahead and save that to a variable so we only need to type it once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G2MvWvdrNk2s"
      },
      "outputs": [],
      "source": [
        "corpus_root = './the-current'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZaiWKvhTCm2"
      },
      "source": [
        "Next, we'll load in the corpus reader from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cIK6K6ZLU8mn"
      },
      "outputs": [],
      "source": [
        "# import the module to read in plain text\n",
        "from nltk.corpus import PlaintextCorpusReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJewgL9b3PzM"
      },
      "source": [
        "As well as some other required NLTK resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "35KrMkoA3RbM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package state_union to\n",
            "[nltk_data]     C:\\Users\\Ming\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import the NLTK library\n",
        "import nltk\n",
        "\n",
        "# download resources\n",
        "nltk_resources = ['gutenberg', 'punkt', 'brown', 'state_union']\n",
        "\n",
        "nltk.download(nltk_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ockSVnpbTIEa"
      },
      "source": [
        "Now, we need to create a new variable from the `PaintextCorpusReader`. We need to put the path to the files as the first agument, followed by a list of the files names we want to be included in the corpus. The files in the folder are:\n",
        "\n",
        "```\n",
        "THE BOYFRIEND PT 1_cleaned.txt\n",
        "THE BOYFRIEND PT 2_cleaned.txt\n",
        "THE CHINESE RESTAURANT_cleaned.txt\n",
        "THE DEALERSHIP_cleaned.txt\n",
        "THE DOODLE_cleaned.txt\n",
        "THE ENGLISH PATIENT_cleaned.txt\n",
        "THE FACE PAINTER_cleaned.txt\n",
        "THE GOOD SAMARITAN_cleaned.txt\n",
        "THE JUNIOR MINT_cleaned.txt\n",
        "THE LITTLE KICKS_cleaned.txt\n",
        "THE MARINE BIOLOGIST_cleaned.txt\n",
        "THE PARKING GARAGE_cleaned.txt\n",
        "THE PARKING SPACE_cleaned.txt\n",
        "THE PEZ DISPENSER_cleaned.txt\n",
        "```\n",
        "\n",
        "Let's try it out on a single file to start. Hey look, the marine biologist episode is in here, so we can try that again.\n",
        "\n",
        "(You may need to mount your Google Drive, or be comfortable using other methods to get these texts into Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gr-m9X4CNt38"
      },
      "outputs": [],
      "source": [
        "# read in my text (i've passed the name in a list, so I could include more than one text if I need to later)\n",
        "marine_biologist_corpus = PlaintextCorpusReader(corpus_root, ['tp001.txt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN5IxJtRUrB1"
      },
      "source": [
        "Now that we've loaded a corpus (even if it is just one text), we can use the built-in NLTK corpus functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GgXPlelEUoyY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'eed to work hard to make it happen\\r\\n0\\t3d is better than other bands in the whole country\\r\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The raw version should be just the string\n",
        "# note we get the exact same output here as when we read the text in manually, above.\n",
        "marine_biologist_corpus.raw()[10:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVJBVsyXVCJO"
      },
      "outputs": [],
      "source": [
        "# we can also get sentences\n",
        "marine_biologist_corpus.sents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk2nchqGV4dr"
      },
      "source": [
        "If you remember from the first part of NLTK, they were using functions like `.concordance()` on the built-in data. We can do the same with our data, but we need to wrap the tokenized words in an nltk function called `Text()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKAHoHt-WCE1"
      },
      "outputs": [],
      "source": [
        "# Create a special Text version of the corpus\n",
        "from nltk.text import Text\n",
        "mb_txt = Text(marine_biologist_corpus.words())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnAXtzDOWv5H"
      },
      "outputs": [],
      "source": [
        "# now we can look for concordance lines\n",
        "mb_txt.concordance('GEORGE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5OrtsAGXISG"
      },
      "outputs": [],
      "source": [
        "mb_txt.concordance('whale')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAX-0zaZXmbl"
      },
      "source": [
        "### Loading in multiple texts to make a corpus\n",
        "\n",
        "A corpus of a single text is not very interesting. Let's update our `PlaintextCorpusReader` to include all of the texts in our Seinfeld folder. But, it sure would be annoying having to type all of the filenames one-by-one. Fortunately, there's a way around this.\n",
        "\n",
        "We can use the [`glob` library](https://docs.python.org/3/library/glob.html) to grab all of the filenames in a directory. The `glob` function makes it easy to save all of the filenames from a directory into a variable.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkyB672wYomd"
      },
      "outputs": [],
      "source": [
        "# import the function which is the same name as the module\n",
        "from glob import glob\n",
        "\n",
        "# the * indicates you want everything from the folder.\n",
        "# we can use more intelligent ways to select only certain files, we'll see this later with regex\n",
        "filenames = glob('./the-current/*')\n",
        "\n",
        "filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR_hVkmvZ6hm"
      },
      "source": [
        "Doing this gives us the entire filepath which doesn't really hurt us but also is kind of annoying. We could easily remove this using slicing. Because the part that we want to remove is always the same (i.e., the `/content/drive/MyDrive/seinfeld/'` part), we could just slice that part off from each filename. All we need to know is where to start the slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LFBJvu8ZA6L"
      },
      "outputs": [],
      "source": [
        "# starting at 32 gives us the episode name only.\n",
        "filenames[1][14:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPazBRCOaQt9"
      },
      "outputs": [],
      "source": [
        "# let's write a list comprehension which removes the start of each filename\n",
        "filenames_short = [name[14:] for name in filenames]\n",
        "\n",
        "# voila!\n",
        "filenames_short"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ejP2OUaewQ"
      },
      "source": [
        "Now we can just pass `filenames_short` to the `PlaintextCorpusReader` function and make a larger corpus. I tested it and it will also work without cleaning the filepath we get from `glob`, but this is nice because we remove the clutter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mHOeHTwZmuG"
      },
      "outputs": [],
      "source": [
        "# make our seinfeld corpus\n",
        "seinfeld_corpus = PlaintextCorpusReader(root = corpus_root, fileids = filenames_short)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-HcLKA_ZzA-"
      },
      "outputs": [],
      "source": [
        "# we can use the fileids function to see the texts in here\n",
        "seinfeld_corpus.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDVeMgpJayXO"
      },
      "outputs": [],
      "source": [
        "# what are the ten most common words in our corpus?\n",
        "from nltk import FreqDist\n",
        "FreqDist(seinfeld_corpus.words()).most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIM7mCSUbk9Y"
      },
      "outputs": [],
      "source": [
        "# and I can search for concordances, neat!\n",
        "Text(seinfeld_corpus.words()).concordance('red')    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGdblR3iZrfb"
      },
      "source": [
        "# **Your Turn**\n",
        "\n",
        "You now have the ability to load in any text you like and use the existing NLTK corpus functions to explore the text.\n",
        "\n",
        "Spend some time repeating the steps above for a different set of text data to make your own corpus. You might want to create a specific folder on your Google Drive which has your data for this course."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMXHX3EDIFATK3gkQW8Q3Nb",
      "include_colab_link": true,
      "mount_file_id": "1RI6fg0azgrRs9h-5-VIjsMVKQA0NVZSH",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
