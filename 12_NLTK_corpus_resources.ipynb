{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/12_NLTK_corpus_resources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5cCWAdFYPGL"
      },
      "source": [
        "# NLTK Text Corpora\n",
        "\n",
        "The first section of Chapter 2 in NLTK teaches you how to explore the built-in corpora provided by NLTK. It is important to note that some of the examples used in Chapter 01 of NLTK were pedagogical – the authors have provided us with copora and texts that have already been pre-processed in various ways, or otherwise simplified. While a corpus *is* a large collection of language and documents, a corpus usually also contains metadata telling the user about different categories *within* a corpus. Categories can be anything - genre, speaker, task, etc.\n",
        "\n",
        "In this notebook, we will explore some of the other corpus resources available through the NLTK.\n",
        "\n",
        "We will also look at how you can load your own data into Colab to create your own corpus using NLTK function.\n",
        "\n",
        "Run the following cell to load in the required resources for this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-j_cbaUYFFt"
      },
      "source": [
        "# import the NLTK library\n",
        "import nltk\n",
        "\n",
        "# download resources for this notebook (takes a bit)\n",
        "nltk_resources = ['gutenberg', 'punkt', 'brown', 'state_union']\n",
        "\n",
        "nltk.download(nltk_resources)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj-__SQwtjqt"
      },
      "source": [
        "## The Gutenberg Corpus\n",
        "\n",
        "A good example of a corpus with different categories is the Gutenberg corpus used by NLTK, which is a collection of different public domain books. [Project Gutenberg](https://www.gutenberg.org/) is a website containing thousands of free eBooks, and is named after [Johannes Gutenberg](https://en.wikipedia.org/wiki/Johannes_Gutenberg), associated with the development of the printing press.\n",
        "\n",
        "<img src = https://i.imgur.com/skJBrKl.png height = '200'>\n",
        "\n",
        "\n",
        "The Gutenberg corpus is part of the `nltk.corpus` module, which provides several built-in methods for working with text data. You can see a complete list of the methods here: [NLTK corpus package](https://www.nltk.org/api/nltk.corpus.html). You can also see a breakdown in Table 1.3 in Chapter 02 of the NLTK book.\n",
        "\n",
        "NLTK includes just a small set of books from Project Gutenberg.  We access the gutenberg data using `nltk.corpus.gutenberg` followed by different NLTK functions and methods.\n",
        "\n",
        "To see the list of all of the files, we use the `.fileids()` method. The filenames are in the form of `author-title.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKGFnvwvZtmB"
      },
      "source": [
        "# inspect the different files in the gutenberg corpus - have you read any of these books?\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsfdJHsjtSgO"
      },
      "source": [
        "Note that the `.fileids()` represent some meta data — the file id contains both the author and the name of the text. So, in this corpus, there are different texts grouped by different authors. As such, authors represent the categories of this corpus.\n",
        "\n",
        "\n",
        "Chapter 02 introduces you to the methods somewhat implicitly, but it is good to look through all the possibilities. Perhaps the most basic possibility is to use the `.raw()` method to obtain a view of the raw text file.\n",
        "\n",
        "Note that we need to input the fileid of the text we are interested in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLVd9N5iZ6Wn"
      },
      "source": [
        "# we can select a single book using the book's name and the format we want, such as words or sentences.\n",
        "# We use `raw` to get the raw text file (as a string)\n",
        "macbeth = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
        "\n",
        "# look at the entire text file.\n",
        "macbeth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3QMh4pur91"
      },
      "source": [
        "There are different methods for sorting the corpus into words and sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QBdQfNEuvrY"
      },
      "source": [
        "# the .words method returns words as tokens\n",
        "macbeth_words = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
        "macbeth_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# could we do this on our own?\n",
        "# why do we get different results when we manually split the text?\n",
        "# do you think the .words is using .split(), or nltk.word_tokenize()?\n",
        "nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt').split()[:10]"
      ],
      "metadata": {
        "id": "4odD816HIFaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCTV8JfGu7aY"
      },
      "source": [
        "# use .sents to get the sentences\n",
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BAFCFBWbhhT"
      },
      "source": [
        "Chapter 02 also provides a discussion about how to import Python modules into a shorthand, which saves typing. This helps provide some clarity into why many Python scripts and examples start with lines such as `from x import y as z` — this just means import something and give it a shorter name.\n",
        "\n",
        "For the current example, we can import gutenburg directly, and thus can avoid typing the `nltk.corpus` bit before it. And, you can do this same thing with other functions from other packages / modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG9c7xQdb1Ti"
      },
      "source": [
        "# import the gutenberg package directly\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# now you can access `gutenberg` without needing to type `nltk.corpus`\n",
        "gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcGmArcYZ5ZO"
      },
      "source": [
        "## Looping through Gutenberg\n",
        "\n",
        "Chapter 02 includes a demonstration of the different methods NLTK can provide for a raw text by asking you to think about how the following function works.\n",
        "\n",
        "First, look at how the loop works. The loop is over the `fileids()` in `gutenberg.fileids()`, which is just a list of the different text files names. In the body of the loop, the fileid is used to access the specific text - this is why the iterator variable `fileid` is placed inside the brackets for `gutenberg.raw()` and all the other functions (`.words`, `.sents`).\n",
        "\n",
        "Examine the print statement and the output - do you understand how they have controlled the output using this `print()` statement? You might find it useful to include some comments in the code cell below, explaining what each line is doing.\n",
        "\n",
        "The book also claims there are some patterns related to average word length, average sentence length, and lexical diversity for different authors. Do you see these patterns? What uses could this information provide?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3fbTi_pcXO1"
      },
      "source": [
        "# Can you add comments to this code explaining what each line is doing?\n",
        "for fileid in gutenberg.fileids():\n",
        "  num_chars = len(gutenberg.raw(fileid))\n",
        "  num_words = len(gutenberg.words(fileid))\n",
        "  num_sents = len(gutenberg.sents(fileid))\n",
        "  num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "  print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Iwz18ssvmNX"
      },
      "source": [
        "## The Brown Corpus\n",
        "\n",
        "You should read through the explanation of various corpora in Chapter 02. One of the more well-known corpora is the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), which has been used in numerous studies of language either for checking word use and collocations, or for compiling frequency statistics of words in spoken and written English. As Chapter 02 explains, the Brown Corpus is an ideal example for how one can use different categories in a corpus to test questions about differences in language use.\n",
        "\n",
        "We load in Brown in a similar manner to Gutenberg. The Brown corpus has a function that Gutenberg did not have: `.categories()`. This function shows different genres or classifications of texts in the Brown corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4MOopTmwFFM"
      },
      "source": [
        "# load in Brown and look at the categories\n",
        "from nltk.corpus import brown\n",
        "\n",
        "brown.categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x2Bsh2HxniD"
      },
      "source": [
        "You can select a specific subsection of the brown corpus using `categories = ` when accessing the brown text as raw, words, sentences, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkj6RNXAxunP"
      },
      "source": [
        "# look at the a sentence from one of the the texts labelled as \"humour\" in the Brown corpus\n",
        "brown.sents(categories = 'humor')[80]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u275-4cpxTu2"
      },
      "source": [
        "### Comparing language among different genres\n",
        "\n",
        "The different genres or categories in Brown allows for a means to compare different writing styles. The NLTK book provide an analysis of [modal verbs](https://en.wikipedia.org/wiki/Modal_verb) as an example.\n",
        "\n",
        "Modal verbs are auxiliary verbs which provide a level of certainty, possibility, or urgency upon a main verb. Words such as *must*, *will*, *could*, etc.\n",
        "\n",
        "In the following example, the authors of NLTK wrote a function to create a frequency distribution of modal verbs in the `news` category of Brown using the `nltk.FreqDist()` function.\n",
        "\n",
        "Note how they do this - first they define a list of modal words — so they can provide the program with the targets it is trying to find. Then they save the words of the Brown corpus to a new variable `news_text`. Then they create a frequency distribution from a list comprehension which first lowercases the word (i.e., pre-processes it) and then only includes words if they are in the list of modals. Any word *not* in that list of modals will in turn not be included in the resulting Frequency Distribution.\n",
        "\n",
        "Run the cell and then ask yourself, what do you think about the frequency of modal verbs in the `news` category - does it make sense that `will` is the most frequent modal verb for news?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHtHBpbxXAH"
      },
      "source": [
        "# create a frequency distribution for specific modal verbs\n",
        "\n",
        "# define list of modal verbs\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "\n",
        "# create an object of words which occur in the news category of brown\n",
        "news_text = brown.words(categories = 'news')\n",
        "\n",
        "# create a frequency distribution - does it make sense for there to be a .lower() here?\n",
        "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
        "\n",
        "# loop through each modal and print the fdist\n",
        "for m in modals:\n",
        "  print(m + ':', fdist[m], end = ' ') # the end argument replaces the default newline which comes at the end of a print statement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Have a play with the Brown corpus, explore the different categories and make sure you are comfortable loading them into Python."
      ],
      "metadata": {
        "id": "z6VoYiYiz4fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State of the Union Corpus\n",
        "\n",
        "Another corpus which provides us some interesting data to use for various comparisons is a set of speeches given by US presidents over the years. Each year, the sitting president gives a \"state of the union\" speech which explains how everything they have done is good and how everything their opponents want to do is bad. Because most US Presidents will serve four or eight subequent years in office, this provides a neat way to compare the speech of different US Presidents over the years.\n",
        "\n",
        "We load in the corpus just like Gutenberg and Brown\n",
        "\n"
      ],
      "metadata": {
        "id": "JdNAP2HW04u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the state_union corpus\n",
        "from nltk.corpus import state_union\n",
        "\n",
        "# fileids shows us the different files\n",
        "state_union.fileids()"
      ],
      "metadata": {
        "id": "ALxYzJ5K1HaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much like Gutenberg, we can access various properties of specific speeches by using the `.raw()`, `.words()`, or `.sents()` methods with a specific fileid in the brackets:"
      ],
      "metadata": {
        "id": "BoumsZq21wgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw words of 1945 speech\n",
        "state_union.raw('1945-Truman.txt')"
      ],
      "metadata": {
        "id": "P-NH6Eq-12PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized version (truncated output)\n",
        "state_union.words('1945-Truman.txt')"
      ],
      "metadata": {
        "id": "AICFNDtu2Nig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences (truncated output)\n",
        "state_union.sents('1945-Truman.txt')"
      ],
      "metadata": {
        "id": "u32PM-Qu2TTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could ask a variety of questions about the nature of State of the Union addresses as they have occurred over time.\n",
        "\n",
        "- Have they changed in length?\n",
        "- What words are similar among all presidents?\n",
        "- What words are unique to different presidents?\n",
        "- Which president is the most lexically diverse?\n",
        "- and so on...\n",
        "\n",
        "Do you have an idea about how to do this? It probably involves some sort of looping over the fileids and then performing a function. For example, I'll write a for loop which reports the most frequent word from each speech. Run the function and then inspect the results. What could you do to get more interesting results? And, what does this function say about the distribution of words in the English language?\n"
      ],
      "metadata": {
        "id": "UGHG_Fsq2oPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fileid in state_union.fileids():\n",
        "  most_frequent_word = nltk.FreqDist(state_union.words(fileid)).most_common(1)\n",
        "  print(f'{fileid} most frequent word is: \\t {most_frequent_word}')"
      ],
      "metadata": {
        "id": "5m2qaqWc3WRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other NLTK Corpora\n",
        "\n",
        "There are a number of other NLTK corpora explained in Chapter 2 of the NLTK book. I encourage you to read them on your own and have a think about what they might be used for, and we will look at some of them in more depth later on."
      ],
      "metadata": {
        "id": "obJZSaOO3_Kp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X3OUdi30S5Q"
      },
      "source": [
        "# Using your own data - mounting Google Drive\n",
        "As useful and interesting as the NLTK data is, you will eventually want to load in your own data. One way to do so involves connecting Google Colab with your Google Drive.\n",
        "\n",
        "The process of connecting Colab to your Google Drive is known as mounting your drive. To do so, you click on the folder icon on the left side of the Colab page:\n",
        "\n",
        "<img src = https://i.imgur.com/82Wedue.png>\n",
        "\n",
        "Then you click the \"mount drive\" icon in the next menu:\n",
        "\n",
        "<img src = https://i.imgur.com/d8DxFIu.png>\n",
        "\n",
        "Colab should then automatically add a code cell like this:\n",
        "\n",
        "<img src = https://i.imgur.com/ttfUkwi.png>\n",
        "\n",
        "Run the cell to mount your Google Drive. You will most likely see several permissions prompts asking you if its okay to make this connection with the associated Google account. It's fine to do this with notebooks you make or the ones I give you, but be wary of other notebooks that might try to ask for your account permissions. There is likely no big risk but I feel obligated to tell you that you should not blindly trust any other Colab notebooks you might come across.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing files in your Google Drive\n",
        "\n",
        "Now that your Drive is connected, you can directly access files in your Google Drive account. This is very handy. (You might need to click the refresh button (the folder with the circle arrow) to see the new folder).\n",
        "\n",
        "You should see a new folder on the left side menu (after clicking on the folder icon) called `drive`. Clicking that folder should then reveal a subfolder called `MyDrive`. The `MyDrive` folder is the root folder for your Google Drive.\n",
        "\n",
        "<img src = https://i.imgur.com/Av1mGtQ.png>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In order to access files on your drive, you will need to be able to give Python the full filepath to your files. No matter where your files are, the start of your filepath will always be `/content/drive/MyDrive/...`, where the `...` are any additional folders.\n",
        "\n",
        "So, for example, if you had a file called `mydata.txt` located in the base level of your Google Drive, the filepath location would be `/content/drive/MyDrive/mydata.txt`. If you had that same file located in a folder called `mydata`, the filepath would be `/content/drive/MyDrive/mydata/mydata.txt`, and so on."
      ],
      "metadata": {
        "id": "KDANcdwPIlZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice uploading a file from your drive\n",
        "\n",
        "Go [here](https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/marine_biologist.txt), you should see a page of text. Manually copy and paste the text into a text editor program, such as notepad on windows or textedit on Mac (don't use Microsoft Word). Save the file as a `.txt` file to your Google Drive folder and name it `marine_biologist.txt`\n",
        "\n",
        "Once you've done that, you should be able to read the text into Colab using the following cell.\n",
        "\n",
        "The code uses the Python `open()` function, which, well, opens files! We need to use the `.read()` method at the end of open to return the contents of the file, which in this case is a string of the raw text in the `.txt` file."
      ],
      "metadata": {
        "id": "uJhGnTHeLnZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marine_biologist = open('/content/drive/MyDrive/marine_biologist.txt').read()\n",
        "\n",
        "# a random quote from this text\n",
        "marine_biologist[15041:15135]"
      ],
      "metadata": {
        "id": "jEkhC6riMx-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are having trouble with this step, make sure you are saving the file to your base level folder in Google Drive. Also make sure your drive is mounted, and that you have saved the file using the same filename I used above. If all else fails, please reach out for some help, because being able to access texts on your Google drive will be an important step for a lot of you in order to read in text data. Of course, if you're comfortable using Jupyter on your local machine, you're under no requirement/obligation to use Google drive to store your files."
      ],
      "metadata": {
        "id": "_b3YjK0HNoFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with a read file.\n",
        "\n",
        "Once you've loaded the file in, you can perform all of the same operations on it as we have been doing on strings we've typed as well as the built-in data included with NLTK. You should be familiar with the following code at this point — are you able to leave comments explaining what each line is doing?"
      ],
      "metadata": {
        "id": "u1qH-GgxQlx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marine_tokens = nltk.word_tokenize(marine_biologist.lower())\n",
        "\n",
        "marine_fdist = nltk.FreqDist(marine_tokens)\n",
        "\n",
        "marine_fdist.most_common(10)"
      ],
      "metadata": {
        "id": "PzYbXW2sQvZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `!wget` and other methods\n",
        "\n",
        "Using Google Drive is a solid bet for integrating with Colab, but you might not like mounting drives each time you run a notebook, or working with files in your drive.\n",
        "\n",
        "There are other options which involve reading files directly from the internet, using other functions such as `!wget` or Python libraries for requesting data from URls, such as the `requests` library.\n",
        "\n",
        "Using these methods requires that the data already exists on the internet somewhere, and also exists at a URL you can access. GitHub is a nice place to store data, as are plenty of other public links. Therefore, I only recommend using this method if you able to control the place where you data lives - and it might just be easier for you to use Google Drive if you don't want to go that route.\n",
        "\n",
        "The main benefit of using `!wget` is that the data is loaded directly into the notebook environment, so you would not need to much around with sifting through files on the Google Drive."
      ],
      "metadata": {
        "id": "EmmobRTP4s2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using !wget to load a file into the notebook environment\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/tmoom.txt'"
      ],
      "metadata": {
        "id": "f1MQjXQK_VlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of pointing at `/content/drive/MyDrive/...`, you instead just point at `/content/...`\n",
        "\n",
        "You need to use the appropriate method to open the file, such as using `open()` to open a text file:"
      ],
      "metadata": {
        "id": "V9CsM6Hl_kBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the text\n",
        "tmoom = open('tmoom.txt').read()\n",
        "\n",
        "# split into tokens\n",
        "tmoom_tokens = nltk.word_tokenize(tmoom)\n",
        "\n",
        "# look at the first ten tokens!\n",
        "[token for token in tmoom_tokens][:10]"
      ],
      "metadata": {
        "id": "6x5bFgBN_x9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating your own NLTK corpus\n",
        "\n",
        "Even when you use your own data, you can still use the NLTK functions to create the NLTK corpora. There are two ways we can do this, one is to read in a bunch of texts as one single corpus. To do this, we use the `PlaintextCorpusReader` class from NLTK.\n",
        "\n",
        "In order to use it, we need three things: 1. some files, 2. a filepath which leads to files, and 3. the names of the files.\n",
        "\n",
        "Again, please follow along. Please go [here](https://github.com/scskalicky/LING-226-vuw/blob/main/other-data/seinfeld.zip) and click the \"download\" button to download a compressed file containing several scripts from Seinfeld. Download the file, unzip it, and save the folder to your base Google Drive folder. Your files should be located in `/content/drive/MyDrive/seinfeld`. This will be the filepath we feed to the NLTK corpus reader. Let's go ahead and save that to a variable so we only need to type it once:"
      ],
      "metadata": {
        "id": "OvJtd0u4RMSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root = '/content/drive/MyDrive/seinfeld'"
      ],
      "metadata": {
        "id": "G2MvWvdrNk2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load in the corpus reader from NLTK"
      ],
      "metadata": {
        "id": "yZaiWKvhTCm2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIK6K6ZLU8mn"
      },
      "source": [
        "# import the module to read in plain text\n",
        "from nltk.corpus import PlaintextCorpusReader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to create a new variable from the `PaintextCorpusReader`. We need to put the path to the files as the first agument, followed by a list of the files names we want to be included in the corpus. The files in the folder are:\n",
        "\n",
        "```\n",
        "THE BOYFRIEND PT 1_cleaned.txt\n",
        "THE BOYFRIEND PT 2_cleaned.txt\n",
        "THE CHINESE RESTAURANT_cleaned.txt\n",
        "THE DEALERSHIP_cleaned.txt\n",
        "THE DOODLE_cleaned.txt\n",
        "THE ENGLISH PATIENT_cleaned.txt\n",
        "THE FACE PAINTER_cleaned.txt\n",
        "THE GOOD SAMARITAN_cleaned.txt\n",
        "THE JUNIOR MINT_cleaned.txt\n",
        "THE LITTLE KICKS_cleaned.txt\n",
        "THE MARINE BIOLOGIST_cleaned.txt\n",
        "THE PARKING GARAGE_cleaned.txt\n",
        "THE PARKING SPACE_cleaned.txt\n",
        "THE PEZ DISPENSER_cleaned.txt\n",
        "```\n",
        "\n",
        "Let's try it out on a single file to start. Hey look, the marine biologist episode is in here, so we can try that again."
      ],
      "metadata": {
        "id": "ockSVnpbTIEa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr-m9X4CNt38"
      },
      "source": [
        "# read in my text (i've passed the name in a list, so I could include more than one text if I need to later)\n",
        "marine_biologist_corpus = PlaintextCorpusReader(corpus_root, ['THE MARINE BIOLOGIST_cleaned.txt'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've loaded a corpus (even if it is just one text), we can use the built-in NLTK corpus functions."
      ],
      "metadata": {
        "id": "nN5IxJtRUrB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The raw version should be just the string\n",
        "# note we get the exact same output here as when we read the text in manually, above.\n",
        "marine_biologist_corpus.raw()[15041:15135]"
      ],
      "metadata": {
        "id": "GgXPlelEUoyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also get sentences\n",
        "marine_biologist_corpus.sents()"
      ],
      "metadata": {
        "id": "GVJBVsyXVCJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you remember from the first part of NLTK, they were using functions like `.concordance()` on the built-in data. We can do the same with our data, but we need to wrap the tokenized words in an nltk function called `Text()`."
      ],
      "metadata": {
        "id": "tk2nchqGV4dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a special Text version of the corpus\n",
        "from nltk.text import Text\n",
        "mb_txt = Text(marine_biologist_corpus.words())"
      ],
      "metadata": {
        "id": "CKAHoHt-WCE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can look for concordance lines\n",
        "mb_txt.concordance('GEORGE')"
      ],
      "metadata": {
        "id": "mnAXtzDOWv5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb_txt.concordance('whale')"
      ],
      "metadata": {
        "id": "b5OrtsAGXISG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in multiple texts to make a corpus\n",
        "\n",
        "A corpus of a single text is not very interesting. Let's update our `PlaintextCorpusReader` to include all of the texts in our Seinfeld folder. But, it sure would be annoying having to type all of the filenames one-by-one. Fortunately, there's a way around this.\n",
        "\n",
        "We can use the [`glob` library](https://docs.python.org/3/library/glob.html) to grab all of the filenames in a directory. The `glob` function makes it easy to save all of the filenames from a directory into a variable.  "
      ],
      "metadata": {
        "id": "FAX-0zaZXmbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the function which is the same name as the module\n",
        "from glob import glob\n",
        "\n",
        "# the * indicates you want everything from the folder.\n",
        "# we can use more intelligent ways to select only certain files, we'll see this later with regex\n",
        "filenames = glob('/content/drive/MyDrive/seinfeld/*')\n",
        "\n",
        "filenames"
      ],
      "metadata": {
        "id": "rkyB672wYomd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing this gives us the entire filepath which doesn't really hurt us but also is kind of annoying. We could easily remove this using slicing. Because the part that we want to remove is always the same, we could just slice that part off from each filename. All we need to know is where to start the slice"
      ],
      "metadata": {
        "id": "MR_hVkmvZ6hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting at 32 gives us the episode name only.\n",
        "filenames[1][32:]"
      ],
      "metadata": {
        "id": "5LFBJvu8ZA6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's write a list comprehension which removes the start of each filename\n",
        "#filenames_short = [name[32:] for name in filenames]\n",
        "filenames_short = [name[38:] for name in filenames]\n",
        "\n",
        "\n",
        "# voila!\n",
        "filenames_short"
      ],
      "metadata": {
        "id": "qPazBRCOaQt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can just pass `filenames_short` to the `PlaintextCorpusReader` function and make a larger corpus. I tested it and it will also work without cleaning the filepath we get from `glob`, but this is nice because we remove the clutter."
      ],
      "metadata": {
        "id": "P3ejP2OUaewQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make our seinfeld corpus\n",
        "seinfeld_corpus = PlaintextCorpusReader(root = corpus_root, fileids = filenames_short)"
      ],
      "metadata": {
        "id": "_mHOeHTwZmuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use the fileids function to see the texts in here\n",
        "seinfeld_corpus.fileids()"
      ],
      "metadata": {
        "id": "L-HcLKA_ZzA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the ten most common words in our corpus?\n",
        "from nltk import FreqDist\n",
        "FreqDist(seinfeld_corpus.words()).most_common(10)"
      ],
      "metadata": {
        "id": "dDVeMgpJayXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and I can search for concordances, neat!\n",
        "Text(seinfeld_corpus.words()).concordance('apartment')"
      ],
      "metadata": {
        "id": "yIM7mCSUbk9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Spend some time repeating the steps above for a different set of text data to make your own corpus. You might want to create a specific folder on your Google Drive which has your data for this course."
      ],
      "metadata": {
        "id": "oGdblR3iZrfb"
      }
    }
  ]
}