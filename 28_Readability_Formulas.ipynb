{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4LzozYAml7OUNnTONN5pU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/28_Readability_Formulas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text readability**\n",
        "\n",
        "What makes a text more or less difficult to read? The answer to this question has important pedagogical implications. Knowing the difficulty level of a text means that teachers and educators can select texts and readings most appropriate for a particular reading population. These considerations can be made based on grade level (e.g., primary vs. high school), but also language knowledge (e.g., first language vs.second language speakers).\n",
        "\n",
        "How can we computationally measure the difficulty of a text? Using what we know so far, the easiest method involves some sort of calculation based on the words in a text. For several decades now, researchers and scientists have been developing **readability formulas** for the automatic assessment of text readability. The most well-known readability formulas are probably the Flesh-Kincaid formulas. In fact, if you have ever used Microsoft Word, the software includes this formula as a way to assess the overall difficulty of a text. There are a number of other formulas, such as Dale-Chall and the Gunning Fog Index. [If you are interested, you can read all about the different formulas on *Wikipedia*.](https://en.wikipedia.org/wiki/Readability)\n",
        "\n",
        "\n",
        "These formulas have been used in many applications, and are generally accepted without criticism to be valid measures of text readability. However, these formluas have also come under criticism because they do not capture cognitive aspects of the reading process. Some of my own research has been working with teams to develop new readability formulas that might better model the reading process. [You can read about this topic in detail in one of our first articles on this topic.](https://www.tandfonline.com/doi/full/10.1080/0163853X.2017.1296264)However, some of out other research suggests that sometimes the [older readability formulas can function just as well to predict text readability.](https://onlinelibrary.wiley.com/doi/full/10.1111/1467-9817.12283)\n",
        "\n",
        "While work continues to develop in this area, we can take this opportunity to calculate some of these classic readability metrics in Python. We will start with Flesch-Kincaid readability formulas.\n"
      ],
      "metadata": {
        "id": "1Sp2lNFtUhM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Flesch-Kincaid**\n",
        "\n",
        "There are two [Flesh-Kincaid formulas](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests): reading ease, and grade level. What do the formulas calculate? *Wikipedia* gives us these formulas:\n",
        "\n",
        "\n",
        "**Flesch Reading Ease**\n",
        "<br>\n",
        "> <img src = 'https://wikimedia.org/api/rest_v1/media/math/render/svg/bd4916e193d2f96fa3b74ee258aaa6fe242e110e'>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Flesh Grade Level**\n",
        "<br>\n",
        "><img src = 'https://wikimedia.org/api/rest_v1/media/math/render/svg/8e68f5fc959d052d1123b85758065afecc4150c3'>\n",
        "\n",
        "\n",
        "We can see that these measures are really not that complicated. The main things we need to calculate are the total number of words and sentences in a text, plus the total number of syllables. We already know that getting the words and sentences in a text is quite easy, using `nltk.word_tokenize()` and `nltk.sents_tokenize()`, or we could even use `.split()` and `.split('\\n')`. The one new measure here that we need to grapple with is measuring the syllables in a word. First of all - let's make sure we understand what a syllable is. Here is the definition from Google dictionary:\n",
        "\n",
        "\n",
        "\n",
        " <img src = \"https://i.imgur.com/1djNlRb.png\">\n",
        "\n",
        "The crucial part of this definition is that a syllable is associated with a vowel *sound*. Well, most of the analyses we have been doing do not really take into account *sounds*, but we can at least identify vowels in a language. So, can we write a function to identify syllables in an English word?\n",
        "\n"
      ],
      "metadata": {
        "id": "PWA_tStrXDqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hunting syllables**\n",
        "\n",
        "Is finding the syllables in a word as simple as counting the number of vowels? Let's try some examples. First let's make a function which just equates the number of vowels with the number of syllables.\n"
      ],
      "metadata": {
        "id": "DRQ3ymMpbM50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable_v1(word):\n",
        "  vowels = 'aieouy'\n",
        "  vowel_count = 0\n",
        "  for char in word.lower():\n",
        "    if char in vowels:\n",
        "      vowel_count += 1\n",
        "  print(f'word has {vowel_count} syllables')"
      ],
      "metadata": {
        "id": "DY0UW4thbYMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this function out."
      ],
      "metadata": {
        "id": "R1THBs9FcEMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "syllable_v1('cat')"
      ],
      "metadata": {
        "id": "qb6qJaLncFpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syllable_v1('banana')"
      ],
      "metadata": {
        "id": "zaMtQQS8cJbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function seems to work fine so far, at least until we find words such as `ate`, which are one syllable but contain two vowels. Clearly, this equation between number of vowels and syllables doesn't work. We actually knew this from the start, because a syllable is associated with a vowel ***sound***, not a written vowel.\n",
        "\n",
        "If only there was an existing resource which measured the *sounds* of text input. And of course, there is, and it exist within NLTK - the CMU Pronunciation Dictionary! Let's load that resource in and recall what it does."
      ],
      "metadata": {
        "id": "5vWlhagEcq6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import CMU Pronunciation Dictionray\n",
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# create the dictionary\n",
        "cmu = cmudict.dict()"
      ],
      "metadata": {
        "id": "sxT-1rik5htW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the output for `ate`. We are given all of the sounds in the word, as well as numbers indicating vowel stress. [You might want to skim their website again.](http://www.speech.cs.cmu.edu/cgi-bin/cmudict). If a syllable is a part of a word associated with a vowel sound, then we can explore the properties of the different phones in the dictionary output to figure out what the vowel sounds are."
      ],
      "metadata": {
        "id": "6nm2ivn6eUNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmu['ate']"
      ],
      "metadata": {
        "id": "fV68Rc55eKsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmu['banana']"
      ],
      "metadata": {
        "id": "YrCRTHdLilMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmu['victoria']"
      ],
      "metadata": {
        "id": "de3GdmYXjwTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at the output, we can see that the stressed vowel sounds all have a number in their output. So, we could cound the number of times such phones occur in the entry for any one word, and in turn get a good approximation for the number of syllables in a word. Create a function to do so. Because words have more than one possible proununciation, each word has these pronunciations stored as a list of lists. So, we will simplify this problem by always choosing the first list from any entry using slicing `[0]`.\n",
        "\n",
        "There are various ways to count if a the sound has a number in it. The way that pops up when you search for this question is to slice the last part of each sound and check if that part is a digit using `.isdigit()`. I follow that approach in the function below. This function could be written in one line, but I like to break this one apart so we all understand what is going on."
      ],
      "metadata": {
        "id": "gqhb5prsjzH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "\n",
        "  if word in cmu.keys():\n",
        "    # get the entry from the dictionary, and slice the first set of sounds\n",
        "    phones = cmu[word][0]\n",
        "\n",
        "    # using a list comprehension, extract only the sounds that end in a digit\n",
        "    vowel_sounds= [sound for sound in phones if sound[-1].isdigit()]\n",
        "\n",
        "    # the number of syllabes is equal to the number of vowel_sounds,\n",
        "    syllables = len(vowel_sounds)\n",
        "\n",
        "    print(f'The word {word} has {syllables} syllables.')\n",
        "    return syllables\n",
        "  else:\n",
        "    print(f'sorry, {word} is not in the CMU Pronunciation Dictionary')\n",
        "    return 0"
      ],
      "metadata": {
        "id": "bJdp-L96j_mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test this function on our words:"
      ],
      "metadata": {
        "id": "HF3Jh4yvnZuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_syllables('ear')"
      ],
      "metadata": {
        "id": "6AfScmTMkmQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_syllables('syllable')"
      ],
      "metadata": {
        "id": "aBzYI4j1nbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_syllables('extraordinary')"
      ],
      "metadata": {
        "id": "rHRnUfsbnbc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_syllables('hairy')"
      ],
      "metadata": {
        "id": "en8_OqJvnqy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# darn, this word isn't in the dictionary.\n",
        "count_syllables('associational')"
      ],
      "metadata": {
        "id": "ShQCOr-0nyHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem, of course, is what can we do with words that are not in the CMU Dictionary? Again we refer back to some attempt at counting vowels in the words. This problem is discussed by others, because it turns out that computationally counting the sounds in a word based on the *spelling* in English is a difficult task. [Look at the discussion here](https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word) and you will see various attempts using regular expressions as well as the CMU dictionary. Our best friend [ChatGPT gives the same answer](https://chat.openai.com/share/95538d86-c30a-4187-8562-5a200a27d9ce). Note how the ChatGPT code has a few tricks for preprocessing, such as using `filter` to lowercase all characters and remove anything that is not alphanumeric.\n",
        "\n",
        "Line 10 includes a list comprehension which does the same thing as the `count_syllable` function above, in a less readable way. The one-liner also uses `max()` to get around the issue that words have more than on pronunciation.\n",
        "\n",
        "Line 11 includes the fallback algorithm in case the word is not in the CMU dictionary. Can you understand the code? It keeps track of each vowel instance, and also does not double count repeated vowels such as words like `heel`.\n",
        "\n",
        "How well doe the function work?"
      ],
      "metadata": {
        "id": "Dx-_H6JYntTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code from ChatGPT\n",
        "# Create a function to count syllables in a word\n",
        "def count_syllables_GPT(word):\n",
        "    # Remove any non-alphabetic characters and convert to lowercase\n",
        "    word = ''.join(filter(str.isalpha, word)).lower()\n",
        "\n",
        "    # Use the CMU Pronouncing Dictionary to count syllables\n",
        "    d = cmudict.dict()\n",
        "    if word in d:\n",
        "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word]])\n",
        "    else:\n",
        "        # If the word is not found in the dictionary, use a simple rule\n",
        "        # based on the number of vowel letters\n",
        "        vowels = \"aeiouy\"\n",
        "        count = 0\n",
        "        prev_char_is_vowel = False\n",
        "        for char in word:\n",
        "            if char in vowels:\n",
        "                if not prev_char_is_vowel:\n",
        "                    count += 1\n",
        "                prev_char_is_vowel = True\n",
        "            else:\n",
        "                prev_char_is_vowel = False\n",
        "        return count"
      ],
      "metadata": {
        "id": "bGoKcs0voYe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out the function - build a helper function to print the output from the chatGPT code."
      ],
      "metadata": {
        "id": "Ea9W1W2XqRHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syllables(word):\n",
        "  syllable_count = count_syllables_GPT(word)\n",
        "  print(f\"The word '{word}' has {syllable_count} syllables.\")"
      ],
      "metadata": {
        "id": "xpy0N5InpOIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syllables('ear')"
      ],
      "metadata": {
        "id": "TE1vmhHTplU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seems to perform okay on unseen words.\n",
        "syllables('associational')"
      ],
      "metadata": {
        "id": "F2Fr5p0IpVEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Flesh Kincaid\n",
        "\n",
        "Well, after sorting out the syllable issue, we can now start to calculate the Flesch Kincaid readability metrics for our text input.\n",
        "\n",
        "Let's start by building a function which will calculate the basic information we need:\n",
        "\n",
        "1. The total number of words in a text\n",
        "2. The total number of sentences in a text\n",
        "3. The total number of syllables in a text\n",
        "\n",
        "We need to download some nltk functions first and then build the program."
      ],
      "metadata": {
        "id": "qJvXdOtwqhEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "2bJpIcBXs0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also make a tweaked version of the chatGPT based syllable finder, and remove the creation of the cmu dict from insdie the function. The chatGPT version actually takes a very long time to run on texts."
      ],
      "metadata": {
        "id": "7CjMJtoFxLAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "\n",
        "# create the dictionary\n",
        "cmu = cmudict.dict()\n",
        "\n",
        "def count_syllables_v2(word):\n",
        "\n",
        "    if word in cmu.keys():\n",
        "      # get the entry from the dictionary, and slice the first set of sounds\n",
        "      phones = cmu[word][0]\n",
        "\n",
        "      # using a list comprehension, extract only the sounds that end in a digit\n",
        "      vowel_sounds= [sound for sound in phones if sound[-1].isdigit()]\n",
        "\n",
        "      # the number of syllabes is equal to the number of vowel_sounds,\n",
        "      syllables = len(vowel_sounds)\n",
        "\n",
        "    else:\n",
        "      # If the word is not found in the dictionary, use a simple rule\n",
        "      # based on the number of vowel letters\n",
        "      vowels = \"aeiouy\"\n",
        "      syllables = 0\n",
        "      prev_char_is_vowel = False\n",
        "      for char in word:\n",
        "          if char in vowels:\n",
        "              if not prev_char_is_vowel:\n",
        "                  syllables += 1\n",
        "              prev_char_is_vowel = True\n",
        "          else:\n",
        "              prev_char_is_vowel = False\n",
        "\n",
        "    #print(f'The word {word} has {syllables} syllables.')\n",
        "    return syllables"
      ],
      "metadata": {
        "id": "O0GFNT5sxNuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_info(text):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    text: a string\n",
        "  Returns:\n",
        "    ...\n",
        "  \"\"\"\n",
        "  # lowercase the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # extract tokens, removing any that are just punctuation\n",
        "  tokens = [token.lower() for token in nltk.word_tokenize(text) if token.isalpha()]\n",
        "\n",
        "  # extract sentences\n",
        "  sentences = [sentence for sentence in nltk.sent_tokenize(text)]\n",
        "\n",
        "  # extract syllables\n",
        "  syllables = 0\n",
        "\n",
        "  for token in tokens:\n",
        "    syllables += count_syllables_v2(token)\n",
        "  print(f'this text has {len(tokens)} words, {len(sentences)} sentences, and {syllables} syllables.')\n",
        "  return len(tokens), len(sentences), syllables"
      ],
      "metadata": {
        "id": "TN2c5gVGsrTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in a text to test out the function."
      ],
      "metadata": {
        "id": "PWC_PuvCuB7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/tmoom.txt'"
      ],
      "metadata": {
        "id": "oNvnTypVt_Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmoom = open('tmoom.txt').read()"
      ],
      "metadata": {
        "id": "eG6kN-65utFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_info(tmoom)"
      ],
      "metadata": {
        "id": "_LAECXAmuv2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Flesch Reading Ease Function**\n",
        "\n",
        "Now that we can extract the necessary information, we can develop functions to calculate the different readability metrics.\n"
      ],
      "metadata": {
        "id": "DwJcfjia2987"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flesch_reading_ease(text):\n",
        "  \"\"\"\n",
        "  calculate Flesch Reading Ease\n",
        "  206.835 - 1.015(total words / total sentences) - 84.6(total syllables / total words)\n",
        "  \"\"\"\n",
        "  words, sents, sylls = text_info(text)\n",
        "\n",
        "  word_sents = words/sents\n",
        "  syll_words = sylls/words\n",
        "\n",
        "  reading_ease_score = 206.835 - (1.015 * word_sents) - (84.6 * syll_words)\n",
        "\n",
        "  print(f'Flesch Reading Ease Score: {reading_ease_score}')"
      ],
      "metadata": {
        "id": "Q3Okgv6v3DE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this text scores an 86, which means it is highly readable.\n",
        "flesch_reading_ease(tmoom)"
      ],
      "metadata": {
        "id": "s-OEhdSn35jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try a more complicated text.\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/sample-texts/conversation_satire.txt'"
      ],
      "metadata": {
        "id": "HPbdj9434vG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = open('conversation_satire.txt').read()"
      ],
      "metadata": {
        "id": "D3LqAWAAIWMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A 47.7 means it is less readable.\n",
        "\n",
        "flesch_reading_ease(conversation)"
      ],
      "metadata": {
        "id": "E0qXX0odIb9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flesch Grade Level\n",
        "\n",
        "It should be relatively simple to now create the other formula to calculate Flesch Grade Level. [We can find at least one resource explaining the scales here.](https://readable.com/readability/flesch-reading-ease-flesch-kincaid-grade-level/)"
      ],
      "metadata": {
        "id": "B1id4PZEIygs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flesch_grade_level(text):\n",
        "  \"\"\"\n",
        "  calculate Flesch grade level\n",
        "  0.39 * (total words / total sentences) +  11.8 * (total syllables / total words) - 15.59\n",
        "  \"\"\"\n",
        "  words, sents, sylls = text_info(text)\n",
        "\n",
        "  word_sents = words/sents\n",
        "  syll_words = sylls/words\n",
        "\n",
        "  reading_grade_level = (0.39 * word_sents) + (11.8 * syll_words) -15.59\n",
        "\n",
        "  print(f'Flesch Grade Level: {reading_grade_level}')"
      ],
      "metadata": {
        "id": "NzO2n9TyJDCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.8 means this text is appropriate for basic readers\n",
        "flesch_grade_level(tmoom)"
      ],
      "metadata": {
        "id": "3-AnuL_EJdIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12.17 means this text is for average to skills readers\n",
        "flesch_grade_level(conversation)"
      ],
      "metadata": {
        "id": "UsHGzfy0JgZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gunning Fog Readability\n",
        "\n",
        "Another of the many formulas is Gunning Fog. I chose this because it is relatively simply to calculate:\n",
        "<br>\n",
        "> <img src = 'https://wikimedia.org/api/rest_v1/media/math/render/svg/84cd504cf61d43230ef59fbd0ecf201796e5e577'>\n",
        "\n",
        "In this formula, a complex word is a word that contains three or more syllables (and also does not count Names, compound words, and some other exceptions we will ignore for now).\n",
        "\n",
        "To create this formula, we only need to get a measure of how many words are \"complex\" in a text. We only need to make a small change to the text information function to record this data."
      ],
      "metadata": {
        "id": "fc10q3PuKV1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_info(text):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    text: a string\n",
        "  Returns:\n",
        "    ...\n",
        "  \"\"\"\n",
        "  # lowercase the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # extract tokens, removing any that are just punctuation\n",
        "  tokens = [token.lower() for token in nltk.word_tokenize(text) if token.isalpha()]\n",
        "\n",
        "  # extract sentences\n",
        "  sentences = [sentence for sentence in nltk.sent_tokenize(text)]\n",
        "\n",
        "  # extract syllables\n",
        "  syllables = 0\n",
        "  # add complex words counter\n",
        "  complex_words = 0\n",
        "\n",
        "  # adjust these calculations so that the new variable, complex words, is iterated\n",
        "  for token in tokens:\n",
        "    num_sylls = count_syllables_v2(token)\n",
        "    syllables += num_sylls\n",
        "    if num_sylls > 2:\n",
        "      complex_words += 1\n",
        "\n",
        "  print(f'this text has {len(tokens)} words, {len(sentences)} sentences, {syllables} syllables, and {complex_words} complex words.')\n",
        "  return len(tokens), len(sentences), syllables, complex_words"
      ],
      "metadata": {
        "id": "FDPLsLyLLd78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gunning_fog(text):\n",
        "  \"\"\"\n",
        "  calculate Gunning Fog\n",
        "  0.4 * ((word/sentences) + 100 *(complex words / word))\n",
        "  \"\"\"\n",
        "  words, sents, sylls, complex_words = text_info(text)\n",
        "\n",
        "  word_sents = words/sents\n",
        "  complexWords_words = complex_words/words\n",
        "\n",
        "  gunning_fog = 0.4 * ((word_sents) + (complexWords_words*100))\n",
        "\n",
        "  print(f'Gunning Fog Index: {gunning_fog}')"
      ],
      "metadata": {
        "id": "jBvz2dpxZHNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out the function:\n",
        "\n",
        "here is the Gunning Fog score interpretation. What do you think?\n",
        "\n",
        "Fog Index|Reading level by grade\n",
        ":-:|:-:\n",
        "17|\tCollege graduate\n",
        "16|\tCollege senior\n",
        "15|\tCollege junior\n",
        "14|\tCollege sophomore\n",
        "13|\tCollege freshman\n",
        "12|\tHigh school senior\n",
        "11|\tHigh school junior\n",
        "10|\tHigh school sophomore\n",
        "9|\tHigh school freshman\n",
        "8|\tEighth grade\n",
        "7|\tSeventh grade\n",
        "6|\tSixth grade"
      ],
      "metadata": {
        "id": "-eH_vdZ5ain4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gunning_fog(tmoom)"
      ],
      "metadata": {
        "id": "CBNYz3vuZopk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gunning_fog(conversation)"
      ],
      "metadata": {
        "id": "-1KIyvuGZqkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "I have shown you a number of formulas you can caclulate. There are even more, [which you can see here.](https://en.wikipedia.org/wiki/Readability#Popular_readability_formulas)\n",
        "\n",
        "Find some texts you think are easy, and some you think are difficult. Do the readability formulas accurately categorise these texts based on your expectations?\n",
        "\n",
        "Then, consider extending what is already here. Can you add more readability formulas? Can you also make a single function which calculates all of these formulas for a text? Can you develop a `regular expression` to find vowles better than the ChatGPT approach of counting non-sequential vowels in a word?"
      ],
      "metadata": {
        "id": "Gl01D3_Savny"
      }
    }
  ]
}